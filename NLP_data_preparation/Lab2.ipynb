{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-06T00:55:22.583880Z","iopub.execute_input":"2023-04-06T00:55:22.584323Z","iopub.status.idle":"2023-04-06T00:55:22.593464Z","shell.execute_reply.started":"2023-04-06T00:55:22.584281Z","shell.execute_reply":"2023-04-06T00:55:22.592531Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stdout","text":"/kaggle/input/2023-spring-nlp-lab2/sent_class.pred.csv\n/kaggle/input/2023-spring-nlp-lab2/sent_class.train.csv\n/kaggle/input/2023-spring-nlp-lab2/sent_class.test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_file = pd.read_csv(\"/kaggle/input/2023-spring-nlp-lab2/sent_class.train.csv\")\n\ntrain_file, type(train_file)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:22.595321Z","iopub.execute_input":"2023-04-06T00:55:22.596020Z","iopub.status.idle":"2023-04-06T00:55:22.620846Z","shell.execute_reply.started":"2023-04-06T00:55:22.595983Z","shell.execute_reply":"2023-04-06T00:55:22.619645Z"},"trusted":true},"execution_count":171,"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"(                                               sentence  label\n 0     ? december scorpion firewall is journalism 197...      5\n 1                  ? station son spumante the wine what      1\n 2               ? firewall srpska pointsettia find what      1\n 3       ? seasons virginia shores whip broken flag what      5\n 4                     is bible burma the lobster ? what      1\n ...                                                 ...    ...\n 4495  ? collection prostitute animal co-starred is h...      1\n 4496  ? starring enzymes marketed directorial most r...      1\n 4497  ? brothers 1978 is dc 0 washington o o fatalis...      3\n 4498  earth claws televised impossible another is de...      4\n 4499  ? the has rules word costner kevin into come l...      1\n \n [4500 rows x 2 columns],\n pandas.core.frame.DataFrame)"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt') # Download the required punkt tokenizer\nnltk.download('wordnet') # Download the required WordNet corpus\n\ntrain_file = pd.read_csv(\"/kaggle/input/2023-spring-nlp-lab2/sent_class.train.csv\")\n\n# Tokenize the sentences in the 'sentence' column of the train_file DataFrame\ntrain_file['tokenized_sentence'] = train_file['sentence'].apply(word_tokenize)\n\n# Print the first 5 rows of the DataFrame with tokenized sentences\ntrain_file","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:22.622768Z","iopub.execute_input":"2023-04-06T00:55:22.623113Z","iopub.status.idle":"2023-04-06T00:55:23.543009Z","shell.execute_reply.started":"2023-04-06T00:55:22.623080Z","shell.execute_reply":"2023-04-06T00:55:23.541748Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"                                               sentence  label  \\\n0     ? december scorpion firewall is journalism 197...      5   \n1                  ? station son spumante the wine what      1   \n2               ? firewall srpska pointsettia find what      1   \n3       ? seasons virginia shores whip broken flag what      5   \n4                     is bible burma the lobster ? what      1   \n...                                                 ...    ...   \n4495  ? collection prostitute animal co-starred is h...      1   \n4496  ? starring enzymes marketed directorial most r...      1   \n4497  ? brothers 1978 is dc 0 washington o o fatalis...      3   \n4498  earth claws televised impossible another is de...      4   \n4499  ? the has rules word costner kevin into come l...      1   \n\n                                     tokenized_sentence  \n0     [?, december, scorpion, firewall, is, journali...  \n1          [?, station, son, spumante, the, wine, what]  \n2        [?, firewall, srpska, pointsettia, find, what]  \n3     [?, seasons, virginia, shores, whip, broken, f...  \n4             [is, bible, burma, the, lobster, ?, what]  \n...                                                 ...  \n4495  [?, collection, prostitute, animal, co-starred...  \n4496  [?, starring, enzymes, marketed, directorial, ...  \n4497  [?, brothers, 1978, is, dc, 0, washington, o, ...  \n4498  [earth, claws, televised, impossible, another,...  \n4499  [?, the, has, rules, word, costner, kevin, int...  \n\n[4500 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>tokenized_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>? december scorpion firewall is journalism 197...</td>\n      <td>5</td>\n      <td>[?, december, scorpion, firewall, is, journali...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>? station son spumante the wine what</td>\n      <td>1</td>\n      <td>[?, station, son, spumante, the, wine, what]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>? firewall srpska pointsettia find what</td>\n      <td>1</td>\n      <td>[?, firewall, srpska, pointsettia, find, what]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>? seasons virginia shores whip broken flag what</td>\n      <td>5</td>\n      <td>[?, seasons, virginia, shores, whip, broken, f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>is bible burma the lobster ? what</td>\n      <td>1</td>\n      <td>[is, bible, burma, the, lobster, ?, what]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4495</th>\n      <td>? collection prostitute animal co-starred is h...</td>\n      <td>1</td>\n      <td>[?, collection, prostitute, animal, co-starred...</td>\n    </tr>\n    <tr>\n      <th>4496</th>\n      <td>? starring enzymes marketed directorial most r...</td>\n      <td>1</td>\n      <td>[?, starring, enzymes, marketed, directorial, ...</td>\n    </tr>\n    <tr>\n      <th>4497</th>\n      <td>? brothers 1978 is dc 0 washington o o fatalis...</td>\n      <td>3</td>\n      <td>[?, brothers, 1978, is, dc, 0, washington, o, ...</td>\n    </tr>\n    <tr>\n      <th>4498</th>\n      <td>earth claws televised impossible another is de...</td>\n      <td>4</td>\n      <td>[earth, claws, televised, impossible, another,...</td>\n    </tr>\n    <tr>\n      <th>4499</th>\n      <td>? the has rules word costner kevin into come l...</td>\n      <td>1</td>\n      <td>[?, the, has, rules, word, costner, kevin, int...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4500 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!unzip -o /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:23.544806Z","iopub.execute_input":"2023-04-06T00:55:23.547073Z","iopub.status.idle":"2023-04-06T00:55:25.080294Z","shell.execute_reply.started":"2023-04-06T00:55:23.547023Z","shell.execute_reply":"2023-04-06T00:55:25.078969Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\n# Lemmatize the tokenized sentences in the 'tokenized_sentence' column\ntrain_file['lemmatized_sentence'] = train_file['tokenized_sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\n# Print the first 5 rows of the DataFrame with lemmatized sentences\n\ntrain_file['lemmatized_sentence'].values","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.084439Z","iopub.execute_input":"2023-04-06T00:55:25.084840Z","iopub.status.idle":"2023-04-06T00:55:25.326819Z","shell.execute_reply.started":"2023-04-06T00:55:25.084801Z","shell.execute_reply":"2023-04-06T00:55:25.325588Z"},"trusted":true},"execution_count":174,"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"array([list(['?', 'december', 'scorpion', 'firewall', 'is', 'journalism', '1978', '17', 'is', 'became', '1978', 'keller', '12', 'commercially', '1978', 'what']),\n       list(['?', 'station', 'son', 'spumante', 'the', 'wine', 'what']),\n       list(['?', 'firewall', 'srpska', 'pointsettia', 'find', 'what']),\n       ...,\n       list(['?', 'brother', '1978', 'is', 'dc', '0', 'washington', 'o', 'o', 'fatalism', 'o', 'o', 'o', 'o', 'child']),\n       list(['earth', 'claw', 'televised', 'impossible', 'another', 'is', 'developed', 'named', 'meant', 'ear', 'meant', 'what']),\n       list(['?', 'the', 'ha', 'rule', 'word', 'costner', 'kevin', 'into', 'come', 'lmds', 'mayan', 'what'])],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"# length(sentence)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndata = train_file['lemmatized_sentence'].values # 데이터 배열\n\nmax_words = 20 # 최대 단어 개수     # 문장 길이\n\n# 배열의 각 요소에서 20개의 단어만 선택하는 함수\ndef select_words(arr):\n    return [word for word in arr[:max_words]]\n\n# 배열의 각 요소에서 20개의 단어만 선택하는 코드\nnew_data = np.array([select_words(arr) if len(arr) > max_words else arr for arr in data], dtype=object)\nnew_data\n\n\nmax_len = max(len(lst) for lst in new_data)      # 가장 긴 리스트의 길이\nnew_data, max_len","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.328456Z","iopub.execute_input":"2023-04-06T00:55:25.329032Z","iopub.status.idle":"2023-04-06T00:55:25.343094Z","shell.execute_reply.started":"2023-04-06T00:55:25.328986Z","shell.execute_reply":"2023-04-06T00:55:25.342193Z"},"trusted":true},"execution_count":175,"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"(array([list(['?', 'december', 'scorpion', 'firewall', 'is', 'journalism', '1978', '17', 'is', 'became', '1978', 'keller', '12', 'commercially', '1978', 'what']),\n        list(['?', 'station', 'son', 'spumante', 'the', 'wine', 'what']),\n        list(['?', 'firewall', 'srpska', 'pointsettia', 'find', 'what']),\n        ...,\n        list(['?', 'brother', '1978', 'is', 'dc', '0', 'washington', 'o', 'o', 'fatalism', 'o', 'o', 'o', 'o', 'child']),\n        list(['earth', 'claw', 'televised', 'impossible', 'another', 'is', 'developed', 'named', 'meant', 'ear', 'meant', 'what']),\n        list(['?', 'the', 'ha', 'rule', 'word', 'costner', 'kevin', 'into', 'come', 'lmds', 'mayan', 'what'])],\n       dtype=object),\n 20)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Length(word)","metadata":{}},{"cell_type":"code","source":"new_lst = []\nfor sub_lst in new_data:\n    new_sub_lst = []\n    for word in sub_lst:\n        new_sub_lst.append(word[:10])\n    new_lst.append(new_sub_lst)\nnew_lst[0:5], len(new_lst)  ","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.344405Z","iopub.execute_input":"2023-04-06T00:55:25.344737Z","iopub.status.idle":"2023-04-06T00:55:25.370697Z","shell.execute_reply.started":"2023-04-06T00:55:25.344707Z","shell.execute_reply":"2023-04-06T00:55:25.369460Z"},"trusted":true},"execution_count":176,"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"([['?',\n   'december',\n   'scorpion',\n   'firewall',\n   'is',\n   'journalism',\n   '1978',\n   '17',\n   'is',\n   'became',\n   '1978',\n   'keller',\n   '12',\n   'commercial',\n   '1978',\n   'what'],\n  ['?', 'station', 'son', 'spumante', 'the', 'wine', 'what'],\n  ['?', 'firewall', 'srpska', 'pointsetti', 'find', 'what'],\n  ['?', 'season', 'virginia', 'shore', 'whip', 'broken', 'flag', 'what'],\n  ['is', 'bible', 'burma', 'the', 'lobster', '?', 'what']],\n 4500)"},"metadata":{}}]},{"cell_type":"code","source":"vocab = sorted(list(set([char for sentence in train_file['lemmatized_sentence'] for word in sentence for char in word])))\n\n# Print the vocabulary\nprint(vocab)\n\n\n# 각 단어별 일련번호\nword_to_id = {\"[PAD]\": 0, \"[UNK]\": 1}\nfor w in vocab:\n    word_to_id[w] = len(word_to_id)\n\n# 각 번호별 단어\nid_to_word = {i: w for w, i in word_to_id.items()}\n\nword_to_id, len(word_to_id)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.372417Z","iopub.execute_input":"2023-04-06T00:55:25.372908Z","iopub.status.idle":"2023-04-06T00:55:25.411147Z","shell.execute_reply.started":"2023-04-06T00:55:25.372850Z","shell.execute_reply":"2023-04-06T00:55:25.409893Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"['!', '#', '$', '%', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"},{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"({'[PAD]': 0,\n  '[UNK]': 1,\n  '!': 2,\n  '#': 3,\n  '$': 4,\n  '%': 5,\n  '&': 6,\n  \"'\": 7,\n  ',': 8,\n  '-': 9,\n  '.': 10,\n  '/': 11,\n  '0': 12,\n  '1': 13,\n  '2': 14,\n  '3': 15,\n  '4': 16,\n  '5': 17,\n  '6': 18,\n  '7': 19,\n  '8': 20,\n  '9': 21,\n  ':': 22,\n  ';': 23,\n  '=': 24,\n  '?': 25,\n  '_': 26,\n  'a': 27,\n  'b': 28,\n  'c': 29,\n  'd': 30,\n  'e': 31,\n  'f': 32,\n  'g': 33,\n  'h': 34,\n  'i': 35,\n  'j': 36,\n  'k': 37,\n  'l': 38,\n  'm': 39,\n  'n': 40,\n  'o': 41,\n  'p': 42,\n  'q': 43,\n  'r': 44,\n  's': 45,\n  't': 46,\n  'u': 47,\n  'v': 48,\n  'w': 49,\n  'x': 50,\n  'y': 51,\n  'z': 52},\n 53)"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(word_to_id)\nEMBEDDING_SIZE = 100\nembeddings = np.random.randn(vocab_size, EMBEDDING_SIZE)\nembeddings, embeddings.shape ","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.412458Z","iopub.execute_input":"2023-04-06T00:55:25.412807Z","iopub.status.idle":"2023-04-06T00:55:25.422339Z","shell.execute_reply.started":"2023-04-06T00:55:25.412774Z","shell.execute_reply":"2023-04-06T00:55:25.420910Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"(array([[-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n         -1.25891419, -0.81495492],\n        [-2.02183828,  0.33583091, -0.41142069, ..., -0.55660492,\n         -0.26618721, -0.74677538],\n        [-0.19917248, -0.57359352,  1.516122  , ..., -0.13129445,\n         -0.14330212, -1.23502888],\n        ...,\n        [-2.47754635, -0.96547806, -0.18495325, ..., -0.9622177 ,\n         -2.19070392, -1.10706665],\n        [ 0.45003798, -0.66993945, -0.32635121, ...,  0.13493026,\n         -0.68607046, -0.27717663],\n        [-2.9908848 ,  1.53426262,  1.12566148, ..., -1.063002  ,\n         -0.03876942,  0.17151299]]),\n (53, 100))"},"metadata":{}}]},{"cell_type":"markdown","source":"# From Here","metadata":{}},{"cell_type":"code","source":"train_inputs = []\nfor i in range(len(new_lst)):\n    sub_inputs = []\n    for s in new_lst[i]:\n        row = []\n        for w in s:\n            row += [word_to_id[c] for c in w]\n        row += [0] * (10 - len(row))                         # max len word             \n        sub_inputs.append(row)\n    sub_inputs += [[0] * 10] * (20 - len(sub_inputs))       # max len sentence\n    train_inputs.append(sub_inputs)\ntrain_inputs = np.array(train_inputs, dtype=object)\n\ntrain_inputs, len(train_inputs), train_inputs.shape, type(train_inputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.424129Z","iopub.execute_input":"2023-04-06T00:55:25.424641Z","iopub.status.idle":"2023-04-06T00:55:25.672961Z","shell.execute_reply.started":"2023-04-06T00:55:25.424594Z","shell.execute_reply":"2023-04-06T00:55:25.671479Z"},"trusted":true},"execution_count":179,"outputs":[{"execution_count":179,"output_type":"execute_result","data":{"text/plain":"(array([[[25, 0, 0, ..., 0, 0, 0],\n         [30, 31, 29, ..., 44, 0, 0],\n         [45, 29, 41, ..., 40, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]],\n \n        [[25, 0, 0, ..., 0, 0, 0],\n         [45, 46, 27, ..., 0, 0, 0],\n         [45, 41, 40, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]],\n \n        [[25, 0, 0, ..., 0, 0, 0],\n         [32, 35, 44, ..., 38, 0, 0],\n         [45, 44, 42, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]],\n \n        ...,\n \n        [[25, 0, 0, ..., 0, 0, 0],\n         [28, 44, 41, ..., 0, 0, 0],\n         [13, 21, 19, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]],\n \n        [[31, 27, 44, ..., 0, 0, 0],\n         [29, 38, 27, ..., 0, 0, 0],\n         [46, 31, 38, ..., 31, 30, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]],\n \n        [[25, 0, 0, ..., 0, 0, 0],\n         [46, 34, 31, ..., 0, 0, 0],\n         [34, 27, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]]], dtype=object),\n 4500,\n (4500, 20, 10),\n numpy.ndarray)"},"metadata":{}}]},{"cell_type":"code","source":"data = []\nfor i in range(train_inputs.shape[0]):       # train_inputs.shape(0)\n    train = embeddings[train_inputs[i].astype(int)]\n    data.append(train)\ndata = np.array(data)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:25.674309Z","iopub.execute_input":"2023-04-06T00:55:25.674690Z","iopub.status.idle":"2023-04-06T00:55:26.505506Z","shell.execute_reply.started":"2023-04-06T00:55:25.674618Z","shell.execute_reply":"2023-04-06T00:55:26.504346Z"},"trusted":true},"execution_count":180,"outputs":[{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"(4500, 20, 10, 100)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Flatten","metadata":{}},{"cell_type":"code","source":"train = data.reshape(data.shape[0],data.shape[1],data.shape[2]*data.shape[3])\nX_train = train\n#X_train = np.concatenate((X_train, np.zeros((20-X_train.shape[0], 1000))), axis=0)\nX_train, X_train.shape # (sentence, words x emb)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:26.507351Z","iopub.execute_input":"2023-04-06T00:55:26.507811Z","iopub.status.idle":"2023-04-06T00:55:26.562165Z","shell.execute_reply.started":"2023-04-06T00:55:26.507765Z","shell.execute_reply":"2023-04-06T00:55:26.560767Z"},"trusted":true},"execution_count":181,"outputs":[{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"(array([[[-2.04940765, -0.02701208, -1.22597223, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-1.24643985, -0.39497078, -1.3254799 , ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 1.30369372,  1.857098  ,  0.79150668, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]],\n \n        [[-2.04940765, -0.02701208, -1.22597223, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 1.30369372,  1.857098  ,  0.79150668, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 1.30369372,  1.857098  ,  0.79150668, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]],\n \n        [[-2.04940765, -0.02701208, -1.22597223, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 1.60767136,  0.22863365,  0.04277177, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 1.30369372,  1.857098  ,  0.79150668, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]],\n \n        ...,\n \n        [[-2.04940765, -0.02701208, -1.22597223, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.03381715,  1.53952711, -1.01731232, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 2.26950185,  1.21023781, -1.00901464, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]],\n \n        [[-0.08761164,  0.83454507, -1.89238308, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-1.42960507,  1.58832939,  0.3395691 , ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 0.81500024,  1.44356224,  0.69857336, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]],\n \n        [[-2.04940765, -0.02701208, -1.22597223, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [ 0.81500024,  1.44356224,  0.69857336, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-1.18256336, -0.35732356, -0.41328392, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         ...,\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492],\n         [-0.71095027, -0.27833323,  0.12744716, ...,  1.04827281,\n          -1.25891419, -0.81495492]]]),\n (4500, 20, 1000))"},"metadata":{}}]},{"cell_type":"code","source":"X = X_train\ny = pd.get_dummies(train_file['label']).values\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:26.563548Z","iopub.execute_input":"2023-04-06T00:55:26.563946Z","iopub.status.idle":"2023-04-06T00:55:26.576854Z","shell.execute_reply.started":"2023-04-06T00:55:26.563892Z","shell.execute_reply":"2023-04-06T00:55:26.575399Z"},"trusted":true},"execution_count":182,"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"((4500, 20, 1000), (4500, 6))"},"metadata":{}}]},{"cell_type":"markdown","source":"# X_test","metadata":{}},{"cell_type":"code","source":"test_file = pd.read_csv(\"/kaggle/input/2023-spring-nlp-lab2/sent_class.test.csv\")\ntest_file['tokenized_sentence'] = test_file['sentence'].apply(word_tokenize)\ntest_file['lemmatized_sentence'] = test_file['tokenized_sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\ntest_file['lemmatized_sentence'].values\ndata = test_file['lemmatized_sentence'].values # 데이터 배열\nnew_data = np.array([select_words(arr) if len(arr) > max_words else arr for arr in data], dtype=object)\n\nnew_lst = []\nfor sub_lst in new_data:\n    new_sub_lst = []\n    for word in sub_lst:\n        new_sub_lst.append(word[:10])\n    new_lst.append(new_sub_lst)\nnew_lst[0:5], len(new_lst)  \n\ntrain_inputs = []\nfor i in range(len(new_lst)):\n    sub_inputs = []\n    for s in new_lst[i]:\n        row = []\n        for w in s:\n            #row += [word_to_id[c] for c in w]\n            row += [word_to_id[c] for c in w if c in word_to_id]\n        row += [0] * (10 - len(row))                         # max len word             \n        sub_inputs.append(row)\n    sub_inputs += [[0] * 10] * (20 - len(sub_inputs))       # max len sentence\n    train_inputs.append(sub_inputs)\ntrain_inputs = np.array(train_inputs, dtype=object)\n\ntrain_inputs, len(train_inputs), train_inputs.shape, type(train_inputs)\n\ndata = []\nfor i in range(train_inputs.shape[0]):       # train_inputs.shape(0)\n    train = embeddings[train_inputs[i].astype(int)]\n    data.append(train)\ndata = np.array(data)\n\ntrain = data.reshape(data.shape[0],data.shape[1],data.shape[2]*data.shape[3])\nX_test = train\n#X_train = np.concatenate((X_train, np.zeros((20-X_train.shape[0], 1000))), axis=0)\nX_test, X_test.shape # (sentence, words x emb)\n\nX_test = X_test\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:26.581859Z","iopub.execute_input":"2023-04-06T00:55:26.582525Z","iopub.status.idle":"2023-04-06T00:55:26.790294Z","shell.execute_reply.started":"2023-04-06T00:55:26.582489Z","shell.execute_reply":"2023-04-06T00:55:26.789020Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, BatchNormalization, concatenate, Input\nfrom tensorflow.keras.layers import Flatten, Softmax, ReLU\nfrom tensorflow.keras import Model\n\nfrom tensorflow.keras.layers import Activation, Conv1D, MaxPooling1D, LSTM \nfrom tensorflow.keras.layers import Layer\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, BatchNormalization, concatenate, Input\nfrom tensorflow.keras.layers import Flatten, Softmax, ReLU\nfrom tensorflow.keras import Model\nimport tensorflow as tf\n\nclass DenseLayer(Layer):\n    def __init__(self, units, activation='relu', kernel_initializer='glorot_uniform', **kwargs):\n        super(DenseLayer, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.kernel_initializer = kernel_initializer\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel', shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, trainable=True)\n        self.bias = self.add_weight(name='bias', shape=(self.units,), initializer='zeros', trainable=True)\n        super(DenseLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        outputs = tf.matmul(inputs, self.kernel) + self.bias\n        if self.activation:\n            activation_layer = Activation(self.activation)\n            outputs = activation_layer(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)\n\n\n\n\nfrom keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, concatenate, Flatten, Dense, Softmax\n\ndef DeepNN(input_shape, num_classes):\n    input_layer = Input(shape=input_shape)\n\n    # Conv 1\n    conv1 = Conv1D(filters=100, kernel_size=2, strides=1, activation=None)(input_layer)\n    batch1 = BatchNormalization()(conv1)\n    relu1 = ReLU()(batch1)\n    pool1 = MaxPooling1D(pool_size=2, strides=2)(relu1)\n\n    # Conv 2\n    conv2 = Conv1D(filters=100, kernel_size=3, strides=1, activation=None)(input_layer)\n    batch2 = BatchNormalization()(conv2)\n    relu2 = ReLU()(batch2)\n    pool2 = MaxPooling1D(pool_size=2, strides=2)(relu2)\n\n    # Conv 3\n    conv3 = Conv1D(filters=100, kernel_size=4, strides=1, activation=None)(input_layer)\n    batch3 = BatchNormalization()(conv3)\n    relu3 = ReLU()(batch3)\n    pool3 = MaxPooling1D(pool_size=2, strides=2)(relu3)\n\n    # Concatenate\n    concat = concatenate([pool1, pool2, pool3], axis=1)\n\n    # Flatten\n    flatten = Flatten()(concat)\n\n    # Dense 1\n    dense1 = DenseLayer(units=100, activation=ReLU())(flatten)\n\n    # Dense 2\n    dense2 = DenseLayer(units=num_classes, activation=Softmax())(dense1)\n\n    model = Model(inputs=input_layer, outputs=dense2)\n\n    return model\ninput_shape = (20, 1000)\nnum_classes = 6\n\nmodel = DeepNN(input_shape, num_classes)\n\n# 모델 컴파일\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\n\n# 모델 학습\nhist = model.fit(X_train, y, epochs=20, validation_split=0.1, batch_size=32) # X_train.shape : (4500,20,1000), y.shape : (4500,6) \n\n# 모델 예측\nprediction = model.predict(X_test)\n\n# 결과 출력\npredicted_labels = np.argmax(prediction, axis=1)\nprint(prediction, predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:55:26.794435Z","iopub.execute_input":"2023-04-06T00:55:26.794909Z","iopub.status.idle":"2023-04-06T00:57:52.146011Z","shell.execute_reply.started":"2023-04-06T00:55:26.794871Z","shell.execute_reply":"2023-04-06T00:57:52.144523Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stdout","text":"Model: \"model_20\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_8 (InputLayer)           [(None, 20, 1000)]   0           []                               \n                                                                                                  \n conv1d_104 (Conv1D)            (None, 19, 100)      200100      ['input_8[0][0]']                \n                                                                                                  \n conv1d_105 (Conv1D)            (None, 18, 100)      300100      ['input_8[0][0]']                \n                                                                                                  \n conv1d_106 (Conv1D)            (None, 17, 100)      400100      ['input_8[0][0]']                \n                                                                                                  \n batch_normalization_85 (BatchN  (None, 19, 100)     400         ['conv1d_104[0][0]']             \n ormalization)                                                                                    \n                                                                                                  \n batch_normalization_86 (BatchN  (None, 18, 100)     400         ['conv1d_105[0][0]']             \n ormalization)                                                                                    \n                                                                                                  \n batch_normalization_87 (BatchN  (None, 17, 100)     400         ['conv1d_106[0][0]']             \n ormalization)                                                                                    \n                                                                                                  \n re_lu_86 (ReLU)                (None, 19, 100)      0           ['batch_normalization_85[0][0]'] \n                                                                                                  \n re_lu_87 (ReLU)                (None, 18, 100)      0           ['batch_normalization_86[0][0]'] \n                                                                                                  \n re_lu_88 (ReLU)                (None, 17, 100)      0           ['batch_normalization_87[0][0]'] \n                                                                                                  \n max_pooling1d_96 (MaxPooling1D  (None, 9, 100)      0           ['re_lu_86[0][0]']               \n )                                                                                                \n                                                                                                  \n max_pooling1d_97 (MaxPooling1D  (None, 9, 100)      0           ['re_lu_87[0][0]']               \n )                                                                                                \n                                                                                                  \n max_pooling1d_98 (MaxPooling1D  (None, 8, 100)      0           ['re_lu_88[0][0]']               \n )                                                                                                \n                                                                                                  \n concatenate_24 (Concatenate)   (None, 26, 100)      0           ['max_pooling1d_96[0][0]',       \n                                                                  'max_pooling1d_97[0][0]',       \n                                                                  'max_pooling1d_98[0][0]']       \n                                                                                                  \n flatten_10 (Flatten)           (None, 2600)         0           ['concatenate_24[0][0]']         \n                                                                                                  \n dense_layer_10 (DenseLayer)    (None, 100)          260100      ['flatten_10[0][0]']             \n                                                                                                  \n dense_layer_11 (DenseLayer)    (None, 6)            606         ['dense_layer_10[0][0]']         \n                                                                                                  \n==================================================================================================\nTotal params: 1,162,206\nTrainable params: 1,161,606\nNon-trainable params: 600\n__________________________________________________________________________________________________\nEpoch 1/20\n127/127 [==============================] - 7s 41ms/step - loss: 1.2175 - accuracy: 0.5467 - val_loss: 1.0173 - val_accuracy: 0.6267\nEpoch 2/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.6052 - accuracy: 0.7751 - val_loss: 1.0668 - val_accuracy: 0.6267\nEpoch 3/20\n127/127 [==============================] - 5s 42ms/step - loss: 0.3700 - accuracy: 0.8736 - val_loss: 0.9763 - val_accuracy: 0.6844\nEpoch 4/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.1966 - accuracy: 0.9427 - val_loss: 1.0035 - val_accuracy: 0.7111\nEpoch 5/20\n127/127 [==============================] - 5s 38ms/step - loss: 0.1306 - accuracy: 0.9605 - val_loss: 0.9995 - val_accuracy: 0.7089\nEpoch 6/20\n127/127 [==============================] - 5s 38ms/step - loss: 0.0813 - accuracy: 0.9763 - val_loss: 1.1395 - val_accuracy: 0.7022\nEpoch 7/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.0524 - accuracy: 0.9859 - val_loss: 0.9998 - val_accuracy: 0.7267\nEpoch 8/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.0195 - accuracy: 0.9968 - val_loss: 0.9683 - val_accuracy: 0.7444\nEpoch 9/20\n127/127 [==============================] - 5s 42ms/step - loss: 0.0116 - accuracy: 0.9990 - val_loss: 1.0684 - val_accuracy: 0.7422\nEpoch 10/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 1.0266 - val_accuracy: 0.7556\nEpoch 11/20\n127/127 [==============================] - 5s 38ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 1.0571 - val_accuracy: 0.7444\nEpoch 12/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.0500 - val_accuracy: 0.7422\nEpoch 13/20\n127/127 [==============================] - 5s 39ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 1.0593 - val_accuracy: 0.7400\nEpoch 14/20\n127/127 [==============================] - 5s 38ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 1.0695 - val_accuracy: 0.7422\nEpoch 15/20\n127/127 [==============================] - 5s 40ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.0998 - val_accuracy: 0.7444\nEpoch 16/20\n127/127 [==============================] - 5s 41ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.1168 - val_accuracy: 0.7489\nEpoch 17/20\n127/127 [==============================] - 5s 39ms/step - loss: 6.8644e-04 - accuracy: 1.0000 - val_loss: 1.0958 - val_accuracy: 0.7533\nEpoch 18/20\n127/127 [==============================] - 5s 38ms/step - loss: 4.7650e-04 - accuracy: 1.0000 - val_loss: 1.1062 - val_accuracy: 0.7467\nEpoch 19/20\n127/127 [==============================] - 5s 39ms/step - loss: 4.0139e-04 - accuracy: 1.0000 - val_loss: 1.1001 - val_accuracy: 0.7467\nEpoch 20/20\n127/127 [==============================] - 5s 38ms/step - loss: 3.5011e-04 - accuracy: 1.0000 - val_loss: 1.1174 - val_accuracy: 0.7467\n16/16 [==============================] - 0s 14ms/step\n[[1.65455649e-09 1.12410909e-07 2.35038031e-12 9.99999464e-01\n  2.18792366e-07 1.48641590e-07]\n [9.68060255e-01 6.19442435e-04 2.96224002e-02 1.33975758e-03\n  1.97656089e-04 1.60364434e-04]\n [7.08864927e-01 1.21341035e-01 3.33004646e-05 6.70140162e-02\n  6.05337471e-02 4.22129221e-02]\n ...\n [3.85310159e-06 9.99546945e-01 1.06129749e-09 4.47566563e-04\n  1.62775586e-06 3.12131014e-08]\n [9.94163334e-01 4.73823817e-03 3.24968511e-04 6.53385860e-06\n  5.94203539e-06 7.61023897e-04]\n [4.08774428e-03 3.59591127e-07 2.98653589e-07 1.28981139e-06\n  1.27351120e-06 9.95909095e-01]] [3 0 0 0 5 1 3 0 1 1 0 4 0 4 1 0 5 1 4 0 1 1 4 5 0 1 1 1 3 5 0 1 3 4 4 0 0\n 1 3 3 4 3 0 1 1 3 0 1 4 3 3 1 3 1 2 1 5 4 1 4 4 1 1 4 5 5 0 0 1 3 4 1 5 1\n 1 4 4 0 0 2 0 0 1 1 0 4 4 1 3 3 0 5 1 3 0 3 3 1 3 5 1 1 3 0 1 5 1 3 1 3 5\n 1 0 5 1 3 5 0 5 1 0 0 3 1 3 3 1 1 3 0 4 5 4 4 1 1 3 1 5 3 3 0 3 3 0 1 3 1\n 2 4 5 4 1 4 1 1 1 0 5 3 0 1 0 0 0 0 3 5 0 3 4 0 1 4 0 4 1 1 3 1 0 0 1 5 3\n 0 1 0 3 0 0 0 5 0 5 0 4 0 3 0 1 0 4 3 4 1 1 0 1 5 4 3 3 5 5 5 1 4 3 0 1 4\n 4 3 0 5 3 1 5 0 4 1 1 5 5 5 0 0 1 1 3 3 0 1 1 3 0 5 0 0 5 1 3 1 1 1 1 0 5\n 3 0 5 3 4 4 4 0 0 1 3 5 1 1 0 1 4 5 1 5 0 3 3 3 5 3 0 3 4 5 1 3 3 1 0 1 2\n 1 4 5 4 0 0 5 1 0 0 0 3 3 3 4 3 3 0 5 0 3 5 1 4 0 0 0 3 4 1 3 4 3 3 1 3 3\n 3 4 1 0 3 5 1 3 4 0 1 3 0 1 0 4 3 0 4 2 0 0 0 0 4 4 0 1 4 1 3 5 1 3 3 5 4\n 0 4 1 1 1 3 0 0 1 0 0 0 3 4 1 3 3 0 0 1 1 3 1 0 3 5 4 5 3 5 0 3 4 4 3 1 3\n 1 2 1 0 1 4 0 5 4 1 3 1 3 0 5 1 1 0 1 3 0 5 1 3 1 5 0 0 4 4 1 3 1 0 4 0 0\n 3 0 5 5 4 4 3 5 3 0 3 1 3 0 4 1 3 3 3 3 3 5 3 2 1 3 5 0 1 5 3 3 1 0 3 1 4\n 4 3 3 5 1 1 5 0 3 3 3 3 0 3 3 1 1 0 5]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Here","metadata":{}},{"cell_type":"code","source":"index_list = []\nfor i in range(1,len(predicted_labels)+1):\n    index_list.append(f\"S{i:03}\")\n    \nprediction = pd.DataFrame(columns=['id', 'pred'])\n\nprediction[\"id\"] = index_list\nprediction[\"pred\"] = predicted_labels\n\nprediction = prediction.reset_index(drop=True)\n\nprediction.to_csv('20221119_하준서_sent_class.pred.csv', index = False)\n\n#index_list\nprediction\n","metadata":{"execution":{"iopub.status.busy":"2023-04-06T00:57:52.149635Z","iopub.execute_input":"2023-04-06T00:57:52.150418Z","iopub.status.idle":"2023-04-06T00:57:52.174623Z","shell.execute_reply.started":"2023-04-06T00:57:52.150383Z","shell.execute_reply":"2023-04-06T00:57:52.173361Z"},"trusted":true},"execution_count":185,"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"       id  pred\n0    S001     3\n1    S002     0\n2    S003     0\n3    S004     0\n4    S005     5\n..    ...   ...\n495  S496     3\n496  S497     1\n497  S498     1\n498  S499     0\n499  S500     5\n\n[500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S001</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>S002</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>S003</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>S004</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>S005</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>S496</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>S497</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>S498</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>S499</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>S500</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}