{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebd1b4c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:11.830874Z",
     "iopub.status.busy": "2023-03-24T10:46:11.829818Z",
     "iopub.status.idle": "2023-03-24T10:46:11.848382Z",
     "shell.execute_reply": "2023-03-24T10:46:11.847172Z"
    },
    "papermill": {
     "duration": 0.0314,
     "end_time": "2023-03-24T10:46:11.852428",
     "exception": false,
     "start_time": "2023-03-24T10:46:11.821028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2023-nlp-lab1-problem2/submission_example.csv\n",
      "/kaggle/input/2023-nlp-lab1-problem2/simple_seq.train.csv\n",
      "/kaggle/input/2023-nlp-lab1-problem2/simple_seq.test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585e3562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:11.867798Z",
     "iopub.status.busy": "2023-03-24T10:46:11.866625Z",
     "iopub.status.idle": "2023-03-24T10:46:22.029871Z",
     "shell.execute_reply": "2023-03-24T10:46:22.028202Z"
    },
    "papermill": {
     "duration": 10.1741,
     "end_time": "2023-03-24T10:46:22.033300",
     "exception": false,
     "start_time": "2023-03-24T10:46:11.859200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W25 W26 W27 W19 W28 W29 W30 W31 W32 W33 W34 W35 W36 W37 W38 W39 W24 W40 D11',\n",
       " 'W41 W4 W42 W43 W44 W45 W46 W47 W48 W49 W50 W51 W52 W53 W17 W54 W24 D1',\n",
       " 'W55 W19 W46 W32 W32 W56 W57 W58 W59 W19 W13 W60 W19 W13 W61 W62 D3',\n",
       " 'W13 W83 W32 W32 W56 W57 W13 W84 W19 W28 W85 W86 W24 D20',\n",
       " 'W87 W88 W89 W90 W32 W91 W13 W92 W93 W90 W94 W95 W24 D20',\n",
       " 'W13 W52 W32 W53 W17 W13 W96 W97 W10 W2 W98 W99 W19 W13 W100 W24 D20',\n",
       " 'W122 W123 W110 W124 W125 W19 W13 W126 W127 W128 W32 W129 W130 W36 W13 W131 W132 W17 W133 W24 D20',\n",
       " 'W1 W2 W3 W4 W134 W7 W28 W78 W19 W13 W135 W136 W137 W138 W139 W90 W17 W140 W24 D1',\n",
       " 'W154 W155 W134 W47 W139 W28 W156 W157 W97 W90 W158 W159 W46 W32 W154 W46 W160 W161 W24 D20',\n",
       " 'W17 W162 W12 W163 W144 W12 W55 W164 W165 W32 W166 W167 W46 W168 W32 W169 W24 D20']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, CuDNNGRU, Activation, CuDNNLSTM \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.metrics import MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "#from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout\n",
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/2023-nlp-lab1-problem2/simple_seq.train.csv\") as f:\n",
    "    lines = f.read() ##Assume the sample file has 3 lines\n",
    "    lines = \"\".join([s for s in lines.strip().splitlines(True) if s.strip()])\n",
    "    first = lines.split('\\n', -1)\n",
    "    train_list = []\n",
    "    for i in range(len(first)):\n",
    "        strings = first[i].replace(\" \",\"\")\n",
    "        strings = first[i].split(',', -1)\n",
    "        strings = [v for v in strings if v]         # 빈요소 \"\" 제거 \n",
    "        strings = \" \".join(strings)\n",
    "        train_list.append(strings)\n",
    "        \n",
    "        \n",
    "train_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba9da36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.055143Z",
     "iopub.status.busy": "2023-03-24T10:46:22.052802Z",
     "iopub.status.idle": "2023-03-24T10:46:22.068527Z",
     "shell.execute_reply": "2023-03-24T10:46:22.066917Z"
    },
    "papermill": {
     "duration": 0.032738,
     "end_time": "2023-03-24T10:46:22.072336",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.039598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['W25',\n",
       "  'W26',\n",
       "  'W27',\n",
       "  'W19',\n",
       "  'W28',\n",
       "  'W29',\n",
       "  'W30',\n",
       "  'W31',\n",
       "  'W32',\n",
       "  'W33',\n",
       "  'W34',\n",
       "  'W35',\n",
       "  'W36',\n",
       "  'W37',\n",
       "  'W38',\n",
       "  'W39',\n",
       "  'W24',\n",
       "  'W40',\n",
       "  'D11',\n",
       "  'W41',\n",
       "  'W4',\n",
       "  'W42',\n",
       "  'W43',\n",
       "  'W44',\n",
       "  'W45',\n",
       "  'W46',\n",
       "  'W47',\n",
       "  'W48',\n",
       "  'W49',\n",
       "  'W50',\n",
       "  'W51',\n",
       "  'W52',\n",
       "  'W53',\n",
       "  'W17',\n",
       "  'W54',\n",
       "  'W24',\n",
       "  'D1',\n",
       "  'W55',\n",
       "  'W19',\n",
       "  'W46'],\n",
       " 12335)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []       # train_words\n",
    "for s in train_list:     # 문장에서 단어 단위로 쪼개기 \n",
    "    words.extend(s.split())\n",
    "words[0:40], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67bece",
   "metadata": {
    "papermill": {
     "duration": 0.0056,
     "end_time": "2023-03-24T10:46:22.084659",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.079059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ed20e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.104668Z",
     "iopub.status.busy": "2023-03-24T10:46:22.103089Z",
     "iopub.status.idle": "2023-03-24T10:46:22.148713Z",
     "shell.execute_reply": "2023-03-24T10:46:22.147200Z"
    },
    "papermill": {
     "duration": 0.060849,
     "end_time": "2023-03-24T10:46:22.151875",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.091026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[PAD]': 0,\n",
       "  '[UNK]': 1,\n",
       "  'W25': 2,\n",
       "  'W26': 3,\n",
       "  'W27': 4,\n",
       "  'W19': 5,\n",
       "  'W28': 6,\n",
       "  'W29': 7,\n",
       "  'W30': 8,\n",
       "  'W31': 9,\n",
       "  'W32': 10,\n",
       "  'W33': 11,\n",
       "  'W34': 12,\n",
       "  'W35': 13,\n",
       "  'W36': 14,\n",
       "  'W37': 15,\n",
       "  'W38': 16,\n",
       "  'W39': 17,\n",
       "  'W24': 18,\n",
       "  'W40': 19,\n",
       "  'D11': 20,\n",
       "  'W41': 21,\n",
       "  'W4': 22,\n",
       "  'W42': 23,\n",
       "  'W43': 24,\n",
       "  'W44': 25,\n",
       "  'W45': 26,\n",
       "  'W46': 27,\n",
       "  'W47': 28,\n",
       "  'W48': 29,\n",
       "  'W49': 30,\n",
       "  'W50': 31,\n",
       "  'W51': 32,\n",
       "  'W52': 33,\n",
       "  'W53': 34,\n",
       "  'W17': 35,\n",
       "  'W54': 36,\n",
       "  'D1': 37,\n",
       "  'W55': 38,\n",
       "  'W56': 39,\n",
       "  'W57': 40,\n",
       "  'W58': 41,\n",
       "  'W59': 42,\n",
       "  'W13': 43,\n",
       "  'W60': 44,\n",
       "  'W61': 45,\n",
       "  'W62': 46,\n",
       "  'D3': 47,\n",
       "  'W83': 48,\n",
       "  'W84': 49,\n",
       "  'W85': 50,\n",
       "  'W86': 51,\n",
       "  'D20': 52,\n",
       "  'W87': 53,\n",
       "  'W88': 54,\n",
       "  'W89': 55,\n",
       "  'W90': 56,\n",
       "  'W91': 57,\n",
       "  'W92': 58,\n",
       "  'W93': 59,\n",
       "  'W94': 60,\n",
       "  'W95': 61,\n",
       "  'W96': 62,\n",
       "  'W97': 63,\n",
       "  'W10': 64,\n",
       "  'W2': 65,\n",
       "  'W98': 66,\n",
       "  'W99': 67,\n",
       "  'W100': 68,\n",
       "  'W122': 69,\n",
       "  'W123': 70,\n",
       "  'W110': 71,\n",
       "  'W124': 72,\n",
       "  'W125': 73,\n",
       "  'W126': 74,\n",
       "  'W127': 75,\n",
       "  'W128': 76,\n",
       "  'W129': 77,\n",
       "  'W130': 78,\n",
       "  'W131': 79,\n",
       "  'W132': 80,\n",
       "  'W133': 81,\n",
       "  'W1': 82,\n",
       "  'W3': 83,\n",
       "  'W134': 84,\n",
       "  'W7': 85,\n",
       "  'W78': 86,\n",
       "  'W135': 87,\n",
       "  'W136': 88,\n",
       "  'W137': 89,\n",
       "  'W138': 90,\n",
       "  'W139': 91,\n",
       "  'W140': 92,\n",
       "  'W154': 93,\n",
       "  'W155': 94,\n",
       "  'W156': 95,\n",
       "  'W157': 96,\n",
       "  'W158': 97,\n",
       "  'W159': 98,\n",
       "  'W160': 99,\n",
       "  'W161': 100,\n",
       "  'W162': 101,\n",
       "  'W12': 102,\n",
       "  'W163': 103,\n",
       "  'W144': 104,\n",
       "  'W164': 105,\n",
       "  'W165': 106,\n",
       "  'W166': 107,\n",
       "  'W167': 108,\n",
       "  'W168': 109,\n",
       "  'W169': 110,\n",
       "  'W201': 111,\n",
       "  'W202': 112,\n",
       "  'W203': 113,\n",
       "  'W204': 114,\n",
       "  'W205': 115,\n",
       "  'W206': 116,\n",
       "  'W8': 117,\n",
       "  'W207': 118,\n",
       "  'W208': 119,\n",
       "  'W209': 120,\n",
       "  'W210': 121,\n",
       "  'W211': 122,\n",
       "  'W212': 123,\n",
       "  'W213': 124,\n",
       "  'D12': 125,\n",
       "  'W214': 126,\n",
       "  'W215': 127,\n",
       "  'W216': 128,\n",
       "  'W217': 129,\n",
       "  'W218': 130,\n",
       "  'W219': 131,\n",
       "  'D15': 132,\n",
       "  'W21': 133,\n",
       "  'W220': 134,\n",
       "  'W221': 135,\n",
       "  'W222': 136,\n",
       "  'W223': 137,\n",
       "  'W224': 138,\n",
       "  'W225': 139,\n",
       "  'W226': 140,\n",
       "  'W227': 141,\n",
       "  'W228': 142,\n",
       "  'W229': 143,\n",
       "  'W230': 144,\n",
       "  'W142': 145,\n",
       "  'W231': 146,\n",
       "  'D4': 147,\n",
       "  'W279': 148,\n",
       "  'W280': 149,\n",
       "  'W281': 150,\n",
       "  'W282': 151,\n",
       "  'W271': 152,\n",
       "  'W283': 153,\n",
       "  'W284': 154,\n",
       "  'W285': 155,\n",
       "  'W286': 156,\n",
       "  'W287': 157,\n",
       "  'W288': 158,\n",
       "  'W289': 159,\n",
       "  'W290': 160,\n",
       "  'W291': 161,\n",
       "  'W292': 162,\n",
       "  'W293': 163,\n",
       "  'D5': 164,\n",
       "  'W296': 165,\n",
       "  'W303': 166,\n",
       "  'W304': 167,\n",
       "  'W305': 168,\n",
       "  'W64': 169,\n",
       "  'W306': 170,\n",
       "  'W307': 171,\n",
       "  'W308': 172,\n",
       "  'W309': 173,\n",
       "  'W310': 174,\n",
       "  'W311': 175,\n",
       "  'D16': 176,\n",
       "  'W328': 177,\n",
       "  'W261': 178,\n",
       "  'W262': 179,\n",
       "  'W329': 180,\n",
       "  'W255': 181,\n",
       "  'W330': 182,\n",
       "  'W331': 183,\n",
       "  'W274': 184,\n",
       "  'W332': 185,\n",
       "  'W333': 186,\n",
       "  'W334': 187,\n",
       "  'W335': 188,\n",
       "  'W336': 189,\n",
       "  'W337': 190,\n",
       "  'W338': 191,\n",
       "  'W339': 192,\n",
       "  'W340': 193,\n",
       "  'W341': 194,\n",
       "  'W342': 195,\n",
       "  'W343': 196,\n",
       "  'W344': 197,\n",
       "  'W345': 198,\n",
       "  'W346': 199,\n",
       "  'W112': 200,\n",
       "  'W113': 201,\n",
       "  'W347': 202,\n",
       "  'W348': 203,\n",
       "  'W349': 204,\n",
       "  'W350': 205,\n",
       "  'W351': 206,\n",
       "  'W183': 207,\n",
       "  'W364': 208,\n",
       "  'W365': 209,\n",
       "  'W366': 210,\n",
       "  'W367': 211,\n",
       "  'W368': 212,\n",
       "  'W363': 213,\n",
       "  'W357': 214,\n",
       "  'W369': 215,\n",
       "  'W370': 216,\n",
       "  'W371': 217,\n",
       "  'W372': 218,\n",
       "  'W373': 219,\n",
       "  'W374': 220,\n",
       "  'W117': 221,\n",
       "  'W375': 222,\n",
       "  'W376': 223,\n",
       "  'W377': 224,\n",
       "  'W378': 225,\n",
       "  'W379': 226,\n",
       "  'W380': 227,\n",
       "  'W149': 228,\n",
       "  'W391': 229,\n",
       "  'W392': 230,\n",
       "  'W393': 231,\n",
       "  'W389': 232,\n",
       "  'W394': 233,\n",
       "  'W403': 234,\n",
       "  'W404': 235,\n",
       "  'W405': 236,\n",
       "  'W406': 237,\n",
       "  'W407': 238,\n",
       "  'W408': 239,\n",
       "  'W409': 240,\n",
       "  'W462': 241,\n",
       "  'W463': 242,\n",
       "  'W246': 243,\n",
       "  'W464': 244,\n",
       "  'W465': 245,\n",
       "  'W466': 246,\n",
       "  'W467': 247,\n",
       "  'W398': 248,\n",
       "  'W534': 249,\n",
       "  'W503': 250,\n",
       "  'W535': 251,\n",
       "  'W536': 252,\n",
       "  'W115': 253,\n",
       "  'W401': 254,\n",
       "  'W582': 255,\n",
       "  'W354': 256,\n",
       "  'W583': 257,\n",
       "  'W584': 258,\n",
       "  'W585': 259,\n",
       "  'W586': 260,\n",
       "  'W587': 261,\n",
       "  'W443': 262,\n",
       "  'W588': 263,\n",
       "  'W589': 264,\n",
       "  'W433': 265,\n",
       "  'W638': 266,\n",
       "  'W642': 267,\n",
       "  'W657': 268,\n",
       "  'W658': 269,\n",
       "  'W659': 270,\n",
       "  'W660': 271,\n",
       "  'W434': 272,\n",
       "  'W661': 273,\n",
       "  'W662': 274,\n",
       "  'W663': 275,\n",
       "  'W664': 276,\n",
       "  'W495': 277,\n",
       "  'W665': 278,\n",
       "  'W475': 279,\n",
       "  'W528': 280,\n",
       "  'W457': 281,\n",
       "  'W666': 282,\n",
       "  'W654': 283,\n",
       "  'W667': 284,\n",
       "  'W692': 285,\n",
       "  'W698': 286,\n",
       "  'W411': 287,\n",
       "  'W755': 288,\n",
       "  'W756': 289,\n",
       "  'W795': 290,\n",
       "  'W787': 291,\n",
       "  'W796': 292,\n",
       "  'W797': 293,\n",
       "  'W798': 294,\n",
       "  'W799': 295,\n",
       "  'W626': 296,\n",
       "  'W800': 297,\n",
       "  'W801': 298,\n",
       "  'W802': 299,\n",
       "  'W803': 300,\n",
       "  'W694': 301,\n",
       "  'W804': 302,\n",
       "  'W805': 303,\n",
       "  'W806': 304,\n",
       "  'W807': 305,\n",
       "  'W300': 306,\n",
       "  'W808': 307,\n",
       "  'W809': 308,\n",
       "  'W647': 309,\n",
       "  'W498': 310,\n",
       "  'W69': 311,\n",
       "  'W835': 312,\n",
       "  'W555': 313,\n",
       "  'W836': 314,\n",
       "  'W114': 315,\n",
       "  'W468': 316,\n",
       "  'W837': 317,\n",
       "  'W851': 318,\n",
       "  'W321': 319,\n",
       "  'W852': 320,\n",
       "  'W853': 321,\n",
       "  'W854': 322,\n",
       "  'W855': 323,\n",
       "  'W680': 324,\n",
       "  'W856': 325,\n",
       "  'W461': 326,\n",
       "  'W844': 327,\n",
       "  'W111': 328,\n",
       "  'W863': 329,\n",
       "  'W516': 330,\n",
       "  'W864': 331,\n",
       "  'W170': 332,\n",
       "  'W865': 333,\n",
       "  'W188': 334,\n",
       "  'W866': 335,\n",
       "  'W867': 336,\n",
       "  'W550': 337,\n",
       "  'W640': 338,\n",
       "  'W879': 339,\n",
       "  'W533': 340,\n",
       "  'W893': 341,\n",
       "  'W894': 342,\n",
       "  'W636': 343,\n",
       "  'W895': 344,\n",
       "  'W896': 345,\n",
       "  'D28': 346,\n",
       "  'W918': 347,\n",
       "  'W919': 348,\n",
       "  'W920': 349,\n",
       "  'W925': 350,\n",
       "  'W934': 351,\n",
       "  'W935': 352,\n",
       "  'W936': 353,\n",
       "  'W937': 354,\n",
       "  'W152': 355,\n",
       "  'W597': 356,\n",
       "  'W938': 357,\n",
       "  'W970': 358,\n",
       "  'W971': 359,\n",
       "  'W940': 360,\n",
       "  'W746': 361,\n",
       "  'W972': 362,\n",
       "  'W430': 363,\n",
       "  'W646': 364,\n",
       "  'W967': 365,\n",
       "  'W973': 366,\n",
       "  'W974': 367,\n",
       "  'W975': 368,\n",
       "  'W68': 369,\n",
       "  'W574': 370,\n",
       "  'W245': 371,\n",
       "  'W976': 372,\n",
       "  'W977': 373,\n",
       "  'W963': 374,\n",
       "  'W978': 375,\n",
       "  'W66': 376,\n",
       "  'W790': 377,\n",
       "  'W979': 378,\n",
       "  'W980': 379,\n",
       "  'W981': 380,\n",
       "  'W926': 381,\n",
       "  'W119': 382,\n",
       "  'W908': 383,\n",
       "  'W1034': 384,\n",
       "  'W984': 385,\n",
       "  'W1014': 386,\n",
       "  'W1035': 387,\n",
       "  'W1036': 388,\n",
       "  'W1037': 389,\n",
       "  'W104': 390,\n",
       "  'W480': 391,\n",
       "  'W553': 392,\n",
       "  'W1072': 393,\n",
       "  'W1073': 394,\n",
       "  'W1074': 395,\n",
       "  'W1085': 396,\n",
       "  'W173': 397,\n",
       "  'W489': 398,\n",
       "  'W1086': 399,\n",
       "  'W252': 400,\n",
       "  'W1087': 401,\n",
       "  'W1088': 402,\n",
       "  'W1101': 403,\n",
       "  'W1102': 404,\n",
       "  'W1103': 405,\n",
       "  'W1104': 406,\n",
       "  'W1124': 407,\n",
       "  'W1125': 408,\n",
       "  'W1043': 409,\n",
       "  'W1126': 410,\n",
       "  'W1127': 411,\n",
       "  'W1128': 412,\n",
       "  'W1129': 413,\n",
       "  'W1010': 414,\n",
       "  'W1130': 415,\n",
       "  'W1131': 416,\n",
       "  'W928': 417,\n",
       "  'W1138': 418,\n",
       "  'W526': 419,\n",
       "  'W1139': 420,\n",
       "  'W1140': 421,\n",
       "  'W930': 422,\n",
       "  'W472': 423,\n",
       "  'W1141': 424,\n",
       "  'W539': 425,\n",
       "  'W1142': 426,\n",
       "  'W1143': 427,\n",
       "  'W1110': 428,\n",
       "  'W1146': 429,\n",
       "  'W15': 430,\n",
       "  'W1147': 431,\n",
       "  'W1148': 432,\n",
       "  'W1149': 433,\n",
       "  'W1150': 434,\n",
       "  'W1151': 435,\n",
       "  'W1152': 436,\n",
       "  'W1153': 437,\n",
       "  'W848': 438,\n",
       "  'W1177': 439,\n",
       "  'W1181': 440,\n",
       "  'W1160': 441,\n",
       "  'W1182': 442,\n",
       "  'W1183': 443,\n",
       "  'W1004': 444,\n",
       "  'W910': 445,\n",
       "  'W494': 446,\n",
       "  'W1041': 447,\n",
       "  'W1191': 448,\n",
       "  'W1192': 449,\n",
       "  'W923': 450,\n",
       "  'W1193': 451,\n",
       "  'W1194': 452,\n",
       "  'W187': 453,\n",
       "  'W950': 454,\n",
       "  'W1195': 455,\n",
       "  'W1196': 456,\n",
       "  'W1197': 457,\n",
       "  'W1198': 458,\n",
       "  'W1208': 459,\n",
       "  'W689': 460,\n",
       "  'W1209': 461,\n",
       "  'W235': 462,\n",
       "  'W1210': 463,\n",
       "  'W1211': 464,\n",
       "  'W324': 465,\n",
       "  'W929': 466,\n",
       "  'W1227': 467,\n",
       "  'W1165': 468,\n",
       "  'W1167': 469,\n",
       "  'W1228': 470,\n",
       "  'W904': 471,\n",
       "  'W1252': 472,\n",
       "  'W1253': 473,\n",
       "  'W1254': 474,\n",
       "  'W1255': 475,\n",
       "  'W1159': 476,\n",
       "  'W1256': 477,\n",
       "  'W70': 478,\n",
       "  'W1261': 479,\n",
       "  'W1262': 480,\n",
       "  'W1214': 481,\n",
       "  'W1263': 482,\n",
       "  'W993': 483,\n",
       "  'W1205': 484,\n",
       "  'W417': 485,\n",
       "  'W493': 486,\n",
       "  'W253': 487,\n",
       "  'W1271': 488,\n",
       "  'W184': 489,\n",
       "  'W1021': 490,\n",
       "  'W1272': 491,\n",
       "  'W951': 492,\n",
       "  'W1273': 493,\n",
       "  'W428': 494,\n",
       "  'W1274': 495,\n",
       "  'W580': 496,\n",
       "  'W1284': 497,\n",
       "  'W1204': 498,\n",
       "  'W1285': 499,\n",
       "  'W1200': 500,\n",
       "  'W387': 501,\n",
       "  'W1287': 502,\n",
       "  'W1174': 503,\n",
       "  'W627': 504,\n",
       "  'W385': 505,\n",
       "  'W1306': 506,\n",
       "  'W1307': 507,\n",
       "  'W1310': 508,\n",
       "  'W1311': 509,\n",
       "  'W1312': 510,\n",
       "  'W1075': 511,\n",
       "  'W1325': 512,\n",
       "  'W1305': 513,\n",
       "  'W1270': 514,\n",
       "  'W358': 515,\n",
       "  'W386': 516,\n",
       "  'W450': 517,\n",
       "  'W1371': 518,\n",
       "  'W1020': 519,\n",
       "  'W609': 520,\n",
       "  'W1118': 521,\n",
       "  'W1017': 522,\n",
       "  'W1032': 523,\n",
       "  'W1386': 524,\n",
       "  'W241': 525,\n",
       "  'W1387': 526,\n",
       "  'W5': 527,\n",
       "  'W1388': 528,\n",
       "  'W1389': 529,\n",
       "  'W1392': 530,\n",
       "  'W1393': 531,\n",
       "  'W1044': 532,\n",
       "  'W1096': 533,\n",
       "  'W1394': 534,\n",
       "  'W604': 535,\n",
       "  'W446': 536,\n",
       "  'W1395': 537,\n",
       "  'W1396': 538,\n",
       "  'W731': 539,\n",
       "  'W927': 540,\n",
       "  'W1397': 541,\n",
       "  'W1398': 542,\n",
       "  'W1399': 543,\n",
       "  'W1412': 544,\n",
       "  'W1413': 545,\n",
       "  'W1414': 546,\n",
       "  'W1003': 547,\n",
       "  'W577': 548,\n",
       "  'W1415': 549,\n",
       "  'W1162': 550,\n",
       "  'W1425': 551,\n",
       "  'W236': 552,\n",
       "  'W251': 553,\n",
       "  'W1426': 554,\n",
       "  'W540': 555,\n",
       "  'W1427': 556,\n",
       "  'W1029': 557,\n",
       "  'W429': 558,\n",
       "  'W1428': 559,\n",
       "  'W1439': 560,\n",
       "  'W1440': 561,\n",
       "  'W1338': 562,\n",
       "  'W1409': 563,\n",
       "  'W1460': 564,\n",
       "  'W1117': 565,\n",
       "  'W1461': 566,\n",
       "  'W1449': 567,\n",
       "  'W1462': 568,\n",
       "  'W1463': 569,\n",
       "  'W954': 570,\n",
       "  'W1467': 571,\n",
       "  'W1315': 572,\n",
       "  'W1473': 573,\n",
       "  'W1474': 574,\n",
       "  'W301': 575,\n",
       "  'W1475': 576,\n",
       "  'W1243': 577,\n",
       "  'W1487': 578,\n",
       "  'W1488': 579,\n",
       "  'W610': 580,\n",
       "  'W1506': 581,\n",
       "  'W1507': 582,\n",
       "  'W744': 583,\n",
       "  'W1508': 584,\n",
       "  'W1509': 585,\n",
       "  'W1510': 586,\n",
       "  'W1511': 587,\n",
       "  'W1538': 588,\n",
       "  'W1516': 589,\n",
       "  'W1517': 590,\n",
       "  'W1571': 591,\n",
       "  'W949': 592,\n",
       "  'W1586': 593,\n",
       "  'W1587': 594,\n",
       "  'W1573': 595,\n",
       "  'W512': 596,\n",
       "  'W1275': 597,\n",
       "  'W470': 598,\n",
       "  'W1627': 599,\n",
       "  'W426': 600,\n",
       "  'W1528': 601,\n",
       "  'W1512': 602,\n",
       "  'W1666': 603,\n",
       "  'W1667': 604,\n",
       "  'W1042': 605,\n",
       "  'W1563': 606,\n",
       "  'W1277': 607,\n",
       "  'W1011': 608,\n",
       "  'W983': 609,\n",
       "  'W1526': 610,\n",
       "  'W427': 611,\n",
       "  'W1677': 612,\n",
       "  'W1697': 613,\n",
       "  'W106': 614,\n",
       "  'W1698': 615,\n",
       "  'W1553': 616,\n",
       "  'W1796': 617,\n",
       "  'W312': 618,\n",
       "  'W532': 619,\n",
       "  'W1797': 620,\n",
       "  'W1798': 621,\n",
       "  'W314': 622,\n",
       "  'W932': 623,\n",
       "  'W1730': 624,\n",
       "  'W1731': 625,\n",
       "  'W1832': 626,\n",
       "  'W105': 627,\n",
       "  'W617': 628,\n",
       "  'W1264': 629,\n",
       "  'W1265': 630,\n",
       "  'W1810': 631,\n",
       "  'W1783': 632,\n",
       "  'W1784': 633,\n",
       "  'W641': 634,\n",
       "  'W1298': 635,\n",
       "  'W1960': 636,\n",
       "  'W595': 637,\n",
       "  'W901': 638,\n",
       "  'W1966': 639,\n",
       "  'W1058': 640,\n",
       "  'W1411': 641,\n",
       "  'W1982': 642,\n",
       "  'W1983': 643,\n",
       "  'W1984': 644,\n",
       "  'W1985': 645,\n",
       "  'W1986': 646,\n",
       "  'W1343': 647,\n",
       "  'W1987': 648,\n",
       "  'W1846': 649,\n",
       "  'W1988': 650,\n",
       "  'W1989': 651,\n",
       "  'W1990': 652,\n",
       "  'W1991': 653,\n",
       "  'W2004': 654,\n",
       "  'W748': 655,\n",
       "  'W2005': 656,\n",
       "  'W2006': 657,\n",
       "  'W2010': 658,\n",
       "  'W2009': 659,\n",
       "  'W1120': 660,\n",
       "  'W2011': 661,\n",
       "  'W2015': 662,\n",
       "  'W874': 663,\n",
       "  'W2016': 664,\n",
       "  'W242': 665,\n",
       "  'W1632': 666,\n",
       "  'W297': 667,\n",
       "  'W2024': 668,\n",
       "  'W2025': 669,\n",
       "  'W435': 670,\n",
       "  'W1976': 671,\n",
       "  'W323': 672,\n",
       "  'W2026': 673,\n",
       "  'W2027': 674,\n",
       "  'W2028': 675,\n",
       "  'W1757': 676,\n",
       "  'W2036': 677,\n",
       "  'W2037': 678,\n",
       "  'W1339': 679,\n",
       "  'W2038': 680,\n",
       "  'W889': 681,\n",
       "  'W2039': 682,\n",
       "  'W2040': 683,\n",
       "  'W778': 684,\n",
       "  'W2041': 685,\n",
       "  'W2042': 686,\n",
       "  'W397': 687,\n",
       "  'W2043': 688,\n",
       "  'W2044': 689,\n",
       "  'W146': 690,\n",
       "  'W931': 691,\n",
       "  'W946': 692,\n",
       "  'W2045': 693,\n",
       "  'W1115': 694,\n",
       "  'W2046': 695,\n",
       "  'W232': 696,\n",
       "  'W2047': 697,\n",
       "  'W2048': 698,\n",
       "  'W1372': 699,\n",
       "  'W1824': 700,\n",
       "  'W2049': 701,\n",
       "  'W2050': 702,\n",
       "  'W2051': 703,\n",
       "  'W1831': 704,\n",
       "  'W2052': 705,\n",
       "  'W2053': 706,\n",
       "  'W2054': 707,\n",
       "  'W2055': 708,\n",
       "  'W2059': 709,\n",
       "  'W2060': 710,\n",
       "  'W2066': 711,\n",
       "  'W1857': 712,\n",
       "  'W2067': 713,\n",
       "  'W2068': 714,\n",
       "  'W2077': 715,\n",
       "  'W2078': 716,\n",
       "  'W1296': 717,\n",
       "  'W2079': 718,\n",
       "  'W1965': 719,\n",
       "  'W575': 720,\n",
       "  'W2085': 721,\n",
       "  'W2086': 722,\n",
       "  'W2084': 723,\n",
       "  'D6': 724,\n",
       "  'W1503': 725,\n",
       "  'W2087': 726,\n",
       "  'W1248': 727,\n",
       "  'W2088': 728,\n",
       "  'W412': 729,\n",
       "  'W2089': 730,\n",
       "  'W1097': 731,\n",
       "  'W1090': 732,\n",
       "  'W840': 733,\n",
       "  'W264': 734,\n",
       "  'D17': 735,\n",
       "  'W875': 736,\n",
       "  'W76': 737,\n",
       "  'W2104': 738,\n",
       "  'W2105': 739,\n",
       "  'W2118': 740,\n",
       "  'W2130': 741,\n",
       "  'W2131': 742,\n",
       "  'W1083': 743,\n",
       "  'W1868': 744,\n",
       "  'W2132': 745,\n",
       "  'W2133': 746,\n",
       "  'W1144': 747,\n",
       "  'W2125': 748,\n",
       "  'W2135': 749,\n",
       "  'W2142': 750,\n",
       "  'W2143': 751,\n",
       "  'W505': 752,\n",
       "  'D19': 753,\n",
       "  'W2112': 754,\n",
       "  'W1776': 755,\n",
       "  'W2144': 756,\n",
       "  'W2145': 757,\n",
       "  'W1890': 758,\n",
       "  'W2146': 759,\n",
       "  'W2147': 760,\n",
       "  'W2148': 761,\n",
       "  'W2149': 762,\n",
       "  'W2150': 763,\n",
       "  'W1203': 764,\n",
       "  'W2151': 765,\n",
       "  'W515': 766,\n",
       "  'W259': 767,\n",
       "  'W2152': 768,\n",
       "  'W2153': 769,\n",
       "  'W1820': 770,\n",
       "  'W2154': 771,\n",
       "  'W2155': 772,\n",
       "  'W2156': 773,\n",
       "  'W1582': 774,\n",
       "  'W2157': 775,\n",
       "  'W2168': 776,\n",
       "  'W2167': 777,\n",
       "  'W1603': 778,\n",
       "  'W1383': 779,\n",
       "  'W180': 780,\n",
       "  'W2228': 781,\n",
       "  'W956': 782,\n",
       "  'W270': 783,\n",
       "  'W2229': 784,\n",
       "  'W2230': 785,\n",
       "  'W2218': 786,\n",
       "  'W559': 787,\n",
       "  'W2231': 788,\n",
       "  'W861': 789,\n",
       "  'W1781': 790,\n",
       "  'W507': 791,\n",
       "  'W1280': 792,\n",
       "  'W2242': 793,\n",
       "  'W2269': 794,\n",
       "  'W2270': 795,\n",
       "  'W1702': 796,\n",
       "  'W496': 797,\n",
       "  'W1368': 798,\n",
       "  'W383': 799,\n",
       "  'W437': 800,\n",
       "  'W2189': 801,\n",
       "  'W81': 802,\n",
       "  'W2271': 803,\n",
       "  'W764': 804,\n",
       "  'W244': 805,\n",
       "  'W2272': 806,\n",
       "  'W2273': 807,\n",
       "  'W1366': 808,\n",
       "  'W2303': 809,\n",
       "  'W2308': 810,\n",
       "  'W2192': 811,\n",
       "  'W2315': 812,\n",
       "  'W2320': 813,\n",
       "  'W1819': 814,\n",
       "  'W791': 815,\n",
       "  'W441': 816,\n",
       "  'W316': 817,\n",
       "  'W2309': 818,\n",
       "  'W2321': 819,\n",
       "  'W325': 820,\n",
       "  'W2328': 821,\n",
       "  'W1436': 822,\n",
       "  'W2336': 823,\n",
       "  'W1106': 824,\n",
       "  'W2337': 825,\n",
       "  'W707': 826,\n",
       "  'W1329': 827,\n",
       "  'W2300': 828,\n",
       "  'W2338': 829,\n",
       "  'W2339': 830,\n",
       "  'W317': 831,\n",
       "  'W438': 832,\n",
       "  'W1078': 833,\n",
       "  'W2340': 834,\n",
       "  'W2341': 835,\n",
       "  'W2345': 836,\n",
       "  'W2346': 837,\n",
       "  'W1362': 838,\n",
       "  'W23': 839,\n",
       "  'W2347': 840,\n",
       "  'W2348': 841,\n",
       "  'W2349': 842,\n",
       "  'W452': 843,\n",
       "  'W1206': 844,\n",
       "  'W909': 845,\n",
       "  'W2355': 846,\n",
       "  'W2356': 847,\n",
       "  'W788': 848,\n",
       "  'W1580': 849,\n",
       "  'W254': 850,\n",
       "  'W1848': 851,\n",
       "  'W2357': 852,\n",
       "  'W1482': 853,\n",
       "  'W2358': 854,\n",
       "  'W1282': 855,\n",
       "  'W1053': 856,\n",
       "  'W2359': 857,\n",
       "  'W1544': 858,\n",
       "  'W2360': 859,\n",
       "  'W2361': 860,\n",
       "  'W2362': 861,\n",
       "  'W982': 862,\n",
       "  'W2208': 863,\n",
       "  'W2363': 864,\n",
       "  'W2378': 865,\n",
       "  'W1775': 866,\n",
       "  'W2379': 867,\n",
       "  'W2372': 868,\n",
       "  'W2074': 869,\n",
       "  'W2385': 870,\n",
       "  'W2377': 871,\n",
       "  'D13': 872,\n",
       "  'W905': 873,\n",
       "  'W1105': 874,\n",
       "  'W2396': 875,\n",
       "  'W2172': 876,\n",
       "  'W2397': 877,\n",
       "  'W2398': 878,\n",
       "  'W145': 879,\n",
       "  'W181': 880,\n",
       "  'W2399': 881,\n",
       "  'W2402': 882,\n",
       "  'W1931': 883,\n",
       "  'W1795': 884,\n",
       "  'W608': 885,\n",
       "  'W2407': 886,\n",
       "  'W2408': 887,\n",
       "  'W2409': 888,\n",
       "  'W109': 889,\n",
       "  'W1119': 890,\n",
       "  'W2410': 891,\n",
       "  'W322': 892,\n",
       "  'W2422': 893,\n",
       "  'W2412': 894,\n",
       "  'W2423': 895,\n",
       "  'W765': 896,\n",
       "  'W2451': 897,\n",
       "  'W2474': 898,\n",
       "  'W2475': 899,\n",
       "  'W2476': 900,\n",
       "  'W2477': 901,\n",
       "  'W2505': 902,\n",
       "  'W2506': 903,\n",
       "  'W2507': 904,\n",
       "  'W2508': 905,\n",
       "  'W1283': 906,\n",
       "  'W2509': 907,\n",
       "  'W2510': 908,\n",
       "  'W2555': 909,\n",
       "  'W537': 910,\n",
       "  'W1294': 911,\n",
       "  'W2556': 912,\n",
       "  'W2563': 913,\n",
       "  'W1895': 914,\n",
       "  'W2564': 915,\n",
       "  'W2565': 916,\n",
       "  'W2567': 917,\n",
       "  'W2568': 918,\n",
       "  'W2569': 919,\n",
       "  'W616': 920,\n",
       "  'W2572': 921,\n",
       "  'W2220': 922,\n",
       "  'W2573': 923,\n",
       "  'W2261': 924,\n",
       "  'W2574': 925,\n",
       "  'W2575': 926,\n",
       "  'W2576': 927,\n",
       "  'W2577': 928,\n",
       "  'W2578': 929,\n",
       "  'W2579': 930,\n",
       "  'W2580': 931,\n",
       "  'W2581': 932,\n",
       "  'W644': 933,\n",
       "  'W1094': 934,\n",
       "  'W2582': 935,\n",
       "  'W2103': 936,\n",
       "  'W2583': 937,\n",
       "  'W2584': 938,\n",
       "  'W2585': 939,\n",
       "  'W2586': 940,\n",
       "  'W912': 941,\n",
       "  'W1379': 942,\n",
       "  'W1061': 943,\n",
       "  'W501': 944,\n",
       "  'W2587': 945,\n",
       "  'W299': 946,\n",
       "  'W774': 947,\n",
       "  'W2227': 948,\n",
       "  'W1065': 949,\n",
       "  'W2605': 950,\n",
       "  'W2606': 951,\n",
       "  'W2607': 952,\n",
       "  'W2608': 953,\n",
       "  'W2609': 954,\n",
       "  'W939': 955,\n",
       "  'W2610': 956,\n",
       "  'D21': 957,\n",
       "  'W2611': 958,\n",
       "  'W2612': 959,\n",
       "  'W1548': 960,\n",
       "  'W725': 961,\n",
       "  'W2645': 962,\n",
       "  'W2637': 963,\n",
       "  'W2649': 964,\n",
       "  'W1905': 965,\n",
       "  'W2650': 966,\n",
       "  'W1588': 967,\n",
       "  'W2429': 968,\n",
       "  'W302': 969,\n",
       "  'W2692': 970,\n",
       "  'W2654': 971,\n",
       "  'W2678': 972,\n",
       "  'W2697': 973,\n",
       "  'W2696': 974,\n",
       "  'W2698': 975,\n",
       "  'W1536': 976,\n",
       "  'W2740': 977,\n",
       "  'W2741': 978,\n",
       "  'W2742': 979,\n",
       "  'W2743': 980,\n",
       "  'W2744': 981,\n",
       "  'D27': 982,\n",
       "  'W2768': 983,\n",
       "  'W1524': 984,\n",
       "  'W2769': 985,\n",
       "  'W2253': 986,\n",
       "  'W1932': 987,\n",
       "  'W2775': 988,\n",
       "  'W2776': 989,\n",
       "  'W275': 990,\n",
       "  'W2653': 991,\n",
       "  'W2777': 992,\n",
       "  'W1109': 993,\n",
       "  'W247': 994,\n",
       "  'W150': 995,\n",
       "  'W2778': 996,\n",
       "  'W2779': 997,\n",
       "  'W2780': 998,\n",
       "  'W749': 999,\n",
       "  ...},\n",
       " 2567)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##print(\"words : \", words)\n",
    "\n",
    "# 중복단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 일련번호\n",
    "word_to_id = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "for w in words:\n",
    "    word_to_id[w] = len(word_to_id)\n",
    "\n",
    "# 각 번호별 단어\n",
    "id_to_word = {i: w for w, i in word_to_id.items()}\n",
    "\n",
    "word_to_id, len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f97c4",
   "metadata": {
    "papermill": {
     "duration": 0.0064,
     "end_time": "2023-03-24T10:46:22.165499",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.159099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Divide the Train_data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc910aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.181161Z",
     "iopub.status.busy": "2023-03-24T10:46:22.180642Z",
     "iopub.status.idle": "2023-03-24T10:46:22.195554Z",
     "shell.execute_reply": "2023-03-24T10:46:22.193785Z"
    },
    "papermill": {
     "duration": 0.02609,
     "end_time": "2023-03-24T10:46:22.198315",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.172225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:20] : \n",
      " ['W25 W26 W27 W19 W28 W29 W30 W31 W32 W33 W34 W35 W36 W37 W38 W39 W24 W40', 'W41 W4 W42 W43 W44 W45 W46 W47 W48 W49 W50 W51 W52 W53 W17 W54 W24', 'W55 W19 W46 W32 W32 W56 W57 W58 W59 W19 W13 W60 W19 W13 W61 W62', 'W13 W83 W32 W32 W56 W57 W13 W84 W19 W28 W85 W86 W24', 'W87 W88 W89 W90 W32 W91 W13 W92 W93 W90 W94 W95 W24', 'W13 W52 W32 W53 W17 W13 W96 W97 W10 W2 W98 W99 W19 W13 W100 W24', 'W122 W123 W110 W124 W125 W19 W13 W126 W127 W128 W32 W129 W130 W36 W13 W131 W132 W17 W133 W24', 'W1 W2 W3 W4 W134 W7 W28 W78 W19 W13 W135 W136 W137 W138 W139 W90 W17 W140 W24', 'W154 W155 W134 W47 W139 W28 W156 W157 W97 W90 W158 W159 W46 W32 W154 W46 W160 W161 W24', 'W17 W162 W12 W163 W144 W12 W55 W164 W165 W32 W166 W167 W46 W168 W32 W169 W24', 'W134 W7 W28 W201 W12 W202 W203 W12 W204 W205 W24', 'W46 W206 W7 W8 W207 W208 W12 W28 W209 W210 W19 W28 W211 W19 W13 W212 W213 W17 W205 W24', 'W90 W214 W32 W206 W7 W17 W13 W215 W24', 'W55 W131 W212 W216 W47 W32 W7 W17 W13 W217 W218 W97 W28 W219 W24', 'W134 W21 W220 W221 W28 W215 W45 W222 W223 W224 W12 W225 W226 W42 W227 W24', 'W228 W19 W46 W32 W204 W13 W229 W213 W97 W32 W230 W142 W19 W13 W220 W231 W24', 'W13 W279 W280 W122 W47 W281 W282 W271 W36 W283 W28 W284 W24', 'W26 W285 W32 W28 W286 W228 W97 W287 W288 W24', 'W289 W17 W290 W291 W122 W32 W292 W293 W24', 'W296 W47 W32 W303 W304 W305 W64 W12 W306 W12 W307 W90 W308 W19 W309 W93 W310 W97 W311 W24'] 900\n",
      "\n",
      "label[0:20] : \n",
      " ['D11', 'D1', 'D3', 'D20', 'D20', 'D20', 'D20', 'D1', 'D20', 'D20', 'D20', 'D12', 'D20', 'D15', 'D15', 'D4', 'D20', 'D20', 'D5', 'D16'] 900\n"
     ]
    }
   ],
   "source": [
    "#########################   TRAIN   ################################\n",
    "\n",
    "with open(\"/kaggle/input/2023-nlp-lab1-problem2/simple_seq.train.csv\") as f:\n",
    "    lines = f.read()\n",
    "    lines = \"\".join([s for s in lines.strip().splitlines(True) if s.strip()])\n",
    "    first = lines.split('\\n', -1)\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(first)):\n",
    "        strings = first[i].replace(\" \",\"\")\n",
    "        strings = first[i].split(',', -1)\n",
    "        strings = [v for v in strings if v]         # 빈요소 \"\" 제거 \n",
    "        label_list.append(strings.pop())\n",
    "        strings = \" \".join(strings)\n",
    "        data_list.append(strings)\n",
    "        \n",
    "print(\"data[0:20] : \\n\", data_list[0:20], len(data_list)) \n",
    "print(\"\\nlabel[0:20] : \\n\",label_list[0:20], len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c7302",
   "metadata": {
    "papermill": {
     "duration": 0.006613,
     "end_time": "2023-03-24T10:46:22.211943",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.205330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Integer encoding and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0dc57f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.228399Z",
     "iopub.status.busy": "2023-03-24T10:46:22.227539Z",
     "iopub.status.idle": "2023-03-24T10:46:22.244630Z",
     "shell.execute_reply": "2023-03-24T10:46:22.243195Z"
    },
    "papermill": {
     "duration": 0.028571,
     "end_time": "2023-03-24T10:46:22.247514",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.218943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:20] : \n",
      " [[   2    3    4 ...   19    0    0]\n",
      " [  21   22   23 ...    0    0    0]\n",
      " [  38    5   27 ...    0    0    0]\n",
      " ...\n",
      " [ 199  737 1415 ...    0    0    0]\n",
      " [ 199  328 2563 ...    0    0    0]\n",
      " [ 199   28   16 ...    0    0    0]] (900, 20)\n"
     ]
    }
   ],
   "source": [
    "# 일련번호 데이터\n",
    "train_inputs = []\n",
    "for s in data_list:                                 ##### 문장별 반복 \n",
    "    row = [word_to_id[w] for w in s.split()]\n",
    "##    print(row)                                    ##### 번호 나열\n",
    "    row += [0] * (20 - len(row))                    ##### padding, later we use pad id in dictionary\n",
    "    train_inputs.append(row)\n",
    "train_inputs = np.array(train_inputs)\n",
    "\n",
    "print(\"data[0:20] : \\n\", train_inputs, train_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcacc75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.264349Z",
     "iopub.status.busy": "2023-03-24T10:46:22.263787Z",
     "iopub.status.idle": "2023-03-24T10:46:22.273075Z",
     "shell.execute_reply": "2023-03-24T10:46:22.271848Z"
    },
    "papermill": {
     "duration": 0.021268,
     "end_time": "2023-03-24T10:46:22.276221",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.254953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label[0:20] : \n",
      " [[ 20]\n",
      " [ 37]\n",
      " [ 47]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [ 37]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [125]\n",
      " [ 52]\n",
      " [132]\n",
      " [132]\n",
      " [147]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [164]\n",
      " [176]] (900, 1)\n"
     ]
    }
   ],
   "source": [
    "label_inputs = []\n",
    "for s in label_list:\n",
    "    row = [word_to_id[w] for w in s.split()]\n",
    "    label_inputs.append(row)\n",
    "label_inputs = np.array(label_inputs)\n",
    "print(\"label[0:20] : \\n\", label_inputs[0:20], label_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abceb35e",
   "metadata": {
    "papermill": {
     "duration": 0.006674,
     "end_time": "2023-03-24T10:46:22.290177",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.283503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load_Test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d9b5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.307543Z",
     "iopub.status.busy": "2023-03-24T10:46:22.307096Z",
     "iopub.status.idle": "2023-03-24T10:46:22.321923Z",
     "shell.execute_reply": "2023-03-24T10:46:22.320931Z"
    },
    "papermill": {
     "duration": 0.026086,
     "end_time": "2023-03-24T10:46:22.324497",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.298411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W13 W81 W19 W346 W846 W1582 W70 W28 W5433 W19 W1163 W2261 W24',\n",
       " 'W5413 W111 W5414 W32 W68 W5415 W12 W2402 W19 W5438 W5439 W5440 W12 W346 W240 W5441 W5442 W24',\n",
       " 'W5413 W111 W5414 W32 W68 W5415 W12 W417 W346 W336 W17 W28 W5443 W12 W122 W47 W38 W335 W1248 W24',\n",
       " 'W5413 W111 W5414 W32 W68 W5415 W12 W346 W32 W2833 W93 W28 W5444 W5445 W17 W346 W5446 W24',\n",
       " 'W5413 W111 W5414 W32 W68 W5415 W12 W111 W346 W47 W336 W286 W5415 W552 W5447 W641 W346 W24',\n",
       " 'W5413 W111 W5414 W32 W68 W5415 W12 W346 W168 W2464 W5448 W24',\n",
       " 'W87 W31 W47 W38 W1196 W97 W627 W5449',\n",
       " 'W447 W28 W498 W204 W2590 W5451 W24',\n",
       " 'W475 W32 W246 W337 W1254 W1198 W461',\n",
       " 'W110 W904 W90 W110 W904 W87 W38 W498 W90 W81 W97 W15 W1065 W555 W475 W90 W47 W38 W280 W24']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################   TEST_SET   ######################################\n",
    "with open(\"/kaggle/input/2023-nlp-lab1-problem2/simple_seq.test.csv\") as f:\n",
    "    lines = f.read() ##Assume the sample file has 3 lines\n",
    "    lines = \"\".join([s for s in lines.strip().splitlines(True) if s.strip()])\n",
    "    first = lines.split('\\n', -1)\n",
    "    test_list = []\n",
    "    for i in range(len(first)):\n",
    "        strings = first[i].replace(\" \",\"\")\n",
    "        strings = first[i].split(',', -1)\n",
    "        strings = [v for v in strings if v]         # 빈요소 \"\" 제거 \n",
    "        strings = \" \".join(strings)\n",
    "        test_list.append(strings)\n",
    "        \n",
    "test_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd55b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.341435Z",
     "iopub.status.busy": "2023-03-24T10:46:22.340948Z",
     "iopub.status.idle": "2023-03-24T10:46:22.353335Z",
     "shell.execute_reply": "2023-03-24T10:46:22.352029Z"
    },
    "papermill": {
     "duration": 0.023737,
     "end_time": "2023-03-24T10:46:22.355763",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.332026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  43,  802,    5, ...,    0,    0,    0],\n",
       "        [2537,  328, 2538, ...,   18,    0,    0],\n",
       "        [2537,  328, 2538, ...,  188,  727,   18],\n",
       "        ...,\n",
       "        [ 205,  355,   59, ...,    0,    0,    0],\n",
       "        [  55,   53,  403, ...,    0,    0,    0],\n",
       "        [   1,  102,   71, ...,    0,    0,    0]]),\n",
       " (100, 20))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = []\n",
    "for s in test_list:\n",
    "    row = []\n",
    "    for w in s.split():\n",
    "        if w in word_to_id:\n",
    "            row.append(word_to_id[w])\n",
    "        else:\n",
    "            row.append(word_to_id['[UNK]'])  # 'W846' 대신 'UNK'를 사용\n",
    "    row += [0] * (20 - len(row))\n",
    "    test_inputs.append(row)\n",
    "test_inputs = np.array(test_inputs)\n",
    "\n",
    "test_inputs, test_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378bb2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.373167Z",
     "iopub.status.busy": "2023-03-24T10:46:22.372158Z",
     "iopub.status.idle": "2023-03-24T10:46:22.468762Z",
     "shell.execute_reply": "2023-03-24T10:46:22.467381Z"
    },
    "papermill": {
     "duration": 0.108306,
     "end_time": "2023-03-24T10:46:22.471562",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.363256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.08181538, -1.11618727,  0.3949702 , ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [-1.13637254, -0.37439796, -1.10473781, ..., -1.069971  ,\n",
       "         -2.0059688 ,  1.79621251],\n",
       "        [ 0.74986233,  0.13079743,  0.60353269, ..., -0.96900658,\n",
       "         -1.78592776, -1.92081582],\n",
       "        ...,\n",
       "        [-0.13741363, -0.76402701,  0.85756657, ..., -1.55858277,\n",
       "         -0.45858667,  0.37200918],\n",
       "        [ 2.01012851, -1.22775891, -0.44469232, ..., -0.25242779,\n",
       "         -0.09001727,  0.37168003],\n",
       "        [-1.07296078, -0.56730458, -0.81276934, ...,  0.75487275,\n",
       "         -0.9124351 , -0.22944681]]),\n",
       " (2567, 1024))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "EMBEDDING_SIZE = 1024\n",
    "embeddings = np.random.randn(vocab_size, EMBEDDING_SIZE)\n",
    "embeddings, embeddings.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a1dcbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.489111Z",
     "iopub.status.busy": "2023-03-24T10:46:22.488592Z",
     "iopub.status.idle": "2023-03-24T10:46:22.580423Z",
     "shell.execute_reply": "2023-03-24T10:46:22.579538Z"
    },
    "papermill": {
     "duration": 0.103393,
     "end_time": "2023-03-24T10:46:22.582735",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.479342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.74986233,  0.13079743,  0.60353269, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [ 0.60438267, -1.39879817,  0.85212724, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [ 2.41067104, -0.64734006,  0.46702607, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        ...,\n",
       "        [-0.81876007,  0.87634017,  1.23396578, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [-0.81876007,  0.87634017,  1.23396578, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [-0.81876007,  0.87634017,  1.23396578, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305]]),\n",
       " (900, 20480))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = embeddings[train_inputs]\n",
    "train = train.reshape(train.shape[0],train.shape[1]*train.shape[2])\n",
    "X_train = train\n",
    "X_train, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd3d10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.601119Z",
     "iopub.status.busy": "2023-03-24T10:46:22.599995Z",
     "iopub.status.idle": "2023-03-24T10:46:22.629575Z",
     "shell.execute_reply": "2023-03-24T10:46:22.628256Z"
    },
    "papermill": {
     "duration": 0.041976,
     "end_time": "2023-03-24T10:46:22.632618",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.590642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onehot_metrix = np.eye(len(word_to_id))\n",
    "onehot_metrix, onehot_metrix.shape\n",
    "label_onehots = onehot_metrix[label_inputs]\n",
    "flatten_label_onehots= label_onehots.reshape(label_onehots.shape[0],label_onehots.shape[2])\n",
    "label_onehots.shape, flatten_label_onehots, flatten_label_onehots.shape\n",
    "y_train = flatten_label_onehots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6783bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.650405Z",
     "iopub.status.busy": "2023-03-24T10:46:22.649951Z",
     "iopub.status.idle": "2023-03-24T10:46:22.668795Z",
     "shell.execute_reply": "2023-03-24T10:46:22.667577Z"
    },
    "papermill": {
     "duration": 0.03111,
     "end_time": "2023-03-24T10:46:22.671598",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.640488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.28961505, -1.33766027,  0.29409028, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [-1.68114221,  0.3385586 ,  0.0913771 , ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [-1.68114221,  0.3385586 ,  0.0913771 , ...,  0.14758096,\n",
       "          0.33133842, -0.82885428],\n",
       "        ...,\n",
       "        [-0.02815954,  0.60704294,  3.02949846, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [ 1.5922265 ,  0.65735952, -0.93833681, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305],\n",
       "        [ 2.89652332,  0.86793524,  0.08694349, ...,  0.07243768,\n",
       "          0.91730544,  0.21661305]]),\n",
       " (100, 20480))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = embeddings[test_inputs]\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "X_test[0:20], X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af50791b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:46:22.689658Z",
     "iopub.status.busy": "2023-03-24T10:46:22.688807Z",
     "iopub.status.idle": "2023-03-24T10:58:36.466119Z",
     "shell.execute_reply": "2023-03-24T10:58:36.463921Z"
    },
    "papermill": {
     "duration": 733.790321,
     "end_time": "2023-03-24T10:58:36.469846",
     "exception": false,
     "start_time": "2023-03-24T10:46:22.679525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 5s 306ms/step - loss: 7.8704 - accuracy: 0.0012 - val_loss: 7.8286 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 7.8443 - accuracy: 0.0012 - val_loss: 7.8248 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 7.8182 - accuracy: 0.0012 - val_loss: 7.8205 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 7.7891 - accuracy: 0.0025 - val_loss: 7.8155 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 4s 307ms/step - loss: 7.7637 - accuracy: 0.0074 - val_loss: 7.8098 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 7.7378 - accuracy: 0.0123 - val_loss: 7.8034 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 7.7085 - accuracy: 0.0198 - val_loss: 7.7962 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 7.6841 - accuracy: 0.0272 - val_loss: 7.7883 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 7.6566 - accuracy: 0.0420 - val_loss: 7.7788 - val_accuracy: 0.0333\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 7.6330 - accuracy: 0.0481 - val_loss: 7.7688 - val_accuracy: 0.0556\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 7.6012 - accuracy: 0.0617 - val_loss: 7.7576 - val_accuracy: 0.0889\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 7.5787 - accuracy: 0.0679 - val_loss: 7.7452 - val_accuracy: 0.1000\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 4s 284ms/step - loss: 7.5453 - accuracy: 0.0790 - val_loss: 7.7321 - val_accuracy: 0.1000\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 4s 296ms/step - loss: 7.5192 - accuracy: 0.0877 - val_loss: 7.7175 - val_accuracy: 0.1222\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 7.4889 - accuracy: 0.1123 - val_loss: 7.7020 - val_accuracy: 0.1444\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 7.4635 - accuracy: 0.1272 - val_loss: 7.6849 - val_accuracy: 0.1556\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 7.4291 - accuracy: 0.1370 - val_loss: 7.6662 - val_accuracy: 0.1778\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 7.4021 - accuracy: 0.1568 - val_loss: 7.6477 - val_accuracy: 0.1889\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 7.3665 - accuracy: 0.1778 - val_loss: 7.6275 - val_accuracy: 0.2111\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 7.3380 - accuracy: 0.2062 - val_loss: 7.6061 - val_accuracy: 0.2111\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 7.3051 - accuracy: 0.2210 - val_loss: 7.5835 - val_accuracy: 0.2222\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 4s 298ms/step - loss: 7.2651 - accuracy: 0.2444 - val_loss: 7.5605 - val_accuracy: 0.2222\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 7.2352 - accuracy: 0.2580 - val_loss: 7.5363 - val_accuracy: 0.2222\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 7.1898 - accuracy: 0.2840 - val_loss: 7.5118 - val_accuracy: 0.2222\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 7.1563 - accuracy: 0.3000 - val_loss: 7.4874 - val_accuracy: 0.2222\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 7.1214 - accuracy: 0.3160 - val_loss: 7.4599 - val_accuracy: 0.2556\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 7.0760 - accuracy: 0.3457 - val_loss: 7.4328 - val_accuracy: 0.2667\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 7.0413 - accuracy: 0.3531 - val_loss: 7.4051 - val_accuracy: 0.2667\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 6.9875 - accuracy: 0.3840 - val_loss: 7.3753 - val_accuracy: 0.2667\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 6.9409 - accuracy: 0.4198 - val_loss: 7.3486 - val_accuracy: 0.2889\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 6.8888 - accuracy: 0.4333 - val_loss: 7.3204 - val_accuracy: 0.2889\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 6.8427 - accuracy: 0.4469 - val_loss: 7.2913 - val_accuracy: 0.3222\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 6.7922 - accuracy: 0.4630 - val_loss: 7.2586 - val_accuracy: 0.3444\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 6.7336 - accuracy: 0.4901 - val_loss: 7.2252 - val_accuracy: 0.3778\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 6.6815 - accuracy: 0.5160 - val_loss: 7.1933 - val_accuracy: 0.3667\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 6.6224 - accuracy: 0.5173 - val_loss: 7.1614 - val_accuracy: 0.3667\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 6.5678 - accuracy: 0.5543 - val_loss: 7.1231 - val_accuracy: 0.4000\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 6.4986 - accuracy: 0.5654 - val_loss: 7.0876 - val_accuracy: 0.4000\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 6.4298 - accuracy: 0.5802 - val_loss: 7.0513 - val_accuracy: 0.4222\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 4s 300ms/step - loss: 6.3624 - accuracy: 0.5988 - val_loss: 7.0138 - val_accuracy: 0.4333\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 6.2916 - accuracy: 0.6210 - val_loss: 6.9718 - val_accuracy: 0.4333\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 6.2267 - accuracy: 0.6296 - val_loss: 6.9271 - val_accuracy: 0.4333\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 6.1527 - accuracy: 0.6568 - val_loss: 6.8804 - val_accuracy: 0.4444\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 6.0692 - accuracy: 0.6827 - val_loss: 6.8455 - val_accuracy: 0.4444\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 4s 284ms/step - loss: 5.9862 - accuracy: 0.7000 - val_loss: 6.8046 - val_accuracy: 0.4556\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 5.9118 - accuracy: 0.7062 - val_loss: 6.7628 - val_accuracy: 0.4778\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 5.8220 - accuracy: 0.7210 - val_loss: 6.7116 - val_accuracy: 0.4778\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 5.7271 - accuracy: 0.7136 - val_loss: 6.6688 - val_accuracy: 0.4667\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 5.6466 - accuracy: 0.7309 - val_loss: 6.6200 - val_accuracy: 0.4778\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 5.5447 - accuracy: 0.7506 - val_loss: 6.5767 - val_accuracy: 0.4778\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 5.4477 - accuracy: 0.7531 - val_loss: 6.5245 - val_accuracy: 0.4778\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 5.3570 - accuracy: 0.7667 - val_loss: 6.4618 - val_accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 5.2466 - accuracy: 0.7704 - val_loss: 6.4087 - val_accuracy: 0.4889\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 5.1460 - accuracy: 0.7753 - val_loss: 6.3545 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 5.0286 - accuracy: 0.7877 - val_loss: 6.3035 - val_accuracy: 0.5111\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 4.9231 - accuracy: 0.7914 - val_loss: 6.2445 - val_accuracy: 0.5111\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 4.8235 - accuracy: 0.7840 - val_loss: 6.2016 - val_accuracy: 0.5111\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 4.7104 - accuracy: 0.7877 - val_loss: 6.1489 - val_accuracy: 0.5222\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 4.5944 - accuracy: 0.8074 - val_loss: 6.0714 - val_accuracy: 0.5111\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 4.5100 - accuracy: 0.8062 - val_loss: 6.0308 - val_accuracy: 0.5222\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 4.3773 - accuracy: 0.8148 - val_loss: 5.9683 - val_accuracy: 0.5111\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 4.2703 - accuracy: 0.8111 - val_loss: 5.9056 - val_accuracy: 0.5111\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 4.1874 - accuracy: 0.8210 - val_loss: 5.8744 - val_accuracy: 0.5222\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 4.0635 - accuracy: 0.8321 - val_loss: 5.8206 - val_accuracy: 0.5111\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 3.9487 - accuracy: 0.8309 - val_loss: 5.7825 - val_accuracy: 0.5222\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 3.8358 - accuracy: 0.8309 - val_loss: 5.7347 - val_accuracy: 0.5222\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 3.7783 - accuracy: 0.8395 - val_loss: 5.7051 - val_accuracy: 0.5222\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 3.6750 - accuracy: 0.8407 - val_loss: 5.6462 - val_accuracy: 0.5222\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 3.5920 - accuracy: 0.8432 - val_loss: 5.6042 - val_accuracy: 0.5222\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 3.4594 - accuracy: 0.8506 - val_loss: 5.5735 - val_accuracy: 0.5444\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 3.3841 - accuracy: 0.8444 - val_loss: 5.5377 - val_accuracy: 0.5222\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 3.2899 - accuracy: 0.8531 - val_loss: 5.5102 - val_accuracy: 0.5444\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 3.2560 - accuracy: 0.8556 - val_loss: 5.4823 - val_accuracy: 0.5444\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 3.1149 - accuracy: 0.8556 - val_loss: 5.4521 - val_accuracy: 0.5000\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 4s 293ms/step - loss: 3.0438 - accuracy: 0.8642 - val_loss: 5.4395 - val_accuracy: 0.5556\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 2.9596 - accuracy: 0.8728 - val_loss: 5.4067 - val_accuracy: 0.5222\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 2.8551 - accuracy: 0.8667 - val_loss: 5.3874 - val_accuracy: 0.5444\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 2.7883 - accuracy: 0.8654 - val_loss: 5.3637 - val_accuracy: 0.5444\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 2.7355 - accuracy: 0.8642 - val_loss: 5.3422 - val_accuracy: 0.5111\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 2.6823 - accuracy: 0.8704 - val_loss: 5.3294 - val_accuracy: 0.5111\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 2.6417 - accuracy: 0.8815 - val_loss: 5.3115 - val_accuracy: 0.5111\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 2.5524 - accuracy: 0.8753 - val_loss: 5.2966 - val_accuracy: 0.5333\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 2.4768 - accuracy: 0.8864 - val_loss: 5.2777 - val_accuracy: 0.5111\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 2.4272 - accuracy: 0.8790 - val_loss: 5.2640 - val_accuracy: 0.5222\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 2.3536 - accuracy: 0.8815 - val_loss: 5.2468 - val_accuracy: 0.5333\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 2.2810 - accuracy: 0.8926 - val_loss: 5.2315 - val_accuracy: 0.5333\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 2.2327 - accuracy: 0.8951 - val_loss: 5.2121 - val_accuracy: 0.5111\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 2.1818 - accuracy: 0.8877 - val_loss: 5.2004 - val_accuracy: 0.5333\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 2.1387 - accuracy: 0.8963 - val_loss: 5.1898 - val_accuracy: 0.5444\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 2.0703 - accuracy: 0.9025 - val_loss: 5.1725 - val_accuracy: 0.5333\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 2.0666 - accuracy: 0.9049 - val_loss: 5.1667 - val_accuracy: 0.5444\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 1.9811 - accuracy: 0.8975 - val_loss: 5.1544 - val_accuracy: 0.5556\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 1.9320 - accuracy: 0.9062 - val_loss: 5.1314 - val_accuracy: 0.5667\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.8891 - accuracy: 0.8963 - val_loss: 5.1226 - val_accuracy: 0.5667\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.8464 - accuracy: 0.9099 - val_loss: 5.1099 - val_accuracy: 0.5667\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.8006 - accuracy: 0.9173 - val_loss: 5.0958 - val_accuracy: 0.5667\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 1.7680 - accuracy: 0.9148 - val_loss: 5.0820 - val_accuracy: 0.5667\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 1.7220 - accuracy: 0.9099 - val_loss: 5.0752 - val_accuracy: 0.5778\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 1.6838 - accuracy: 0.9210 - val_loss: 5.0572 - val_accuracy: 0.5667\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.7050 - accuracy: 0.9185 - val_loss: 5.0453 - val_accuracy: 0.5667\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 1.5905 - accuracy: 0.9210 - val_loss: 5.0326 - val_accuracy: 0.5667\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.5855 - accuracy: 0.9210 - val_loss: 5.0259 - val_accuracy: 0.5667\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 1.5632 - accuracy: 0.9210 - val_loss: 5.0156 - val_accuracy: 0.5667\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 1.5274 - accuracy: 0.9198 - val_loss: 5.0030 - val_accuracy: 0.5667\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.5028 - accuracy: 0.9198 - val_loss: 4.9925 - val_accuracy: 0.5667\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.4633 - accuracy: 0.9272 - val_loss: 4.9884 - val_accuracy: 0.5667\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.4508 - accuracy: 0.9235 - val_loss: 4.9770 - val_accuracy: 0.5667\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.3969 - accuracy: 0.9222 - val_loss: 4.9601 - val_accuracy: 0.5667\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 4s 294ms/step - loss: 1.4026 - accuracy: 0.9296 - val_loss: 4.9508 - val_accuracy: 0.5667\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.3604 - accuracy: 0.9247 - val_loss: 4.9432 - val_accuracy: 0.5667\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.3350 - accuracy: 0.9358 - val_loss: 4.9392 - val_accuracy: 0.5667\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.3158 - accuracy: 0.9296 - val_loss: 4.9334 - val_accuracy: 0.6000\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.2879 - accuracy: 0.9346 - val_loss: 4.9265 - val_accuracy: 0.6111\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 1.2695 - accuracy: 0.9309 - val_loss: 4.9163 - val_accuracy: 0.6000\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.2550 - accuracy: 0.9358 - val_loss: 4.9123 - val_accuracy: 0.6111\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.2291 - accuracy: 0.9321 - val_loss: 4.9005 - val_accuracy: 0.5889\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.2043 - accuracy: 0.9333 - val_loss: 4.8930 - val_accuracy: 0.6000\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 4s 300ms/step - loss: 1.1648 - accuracy: 0.9370 - val_loss: 4.8852 - val_accuracy: 0.6000\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 1.1503 - accuracy: 0.9370 - val_loss: 4.8786 - val_accuracy: 0.6111\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 1.1371 - accuracy: 0.9432 - val_loss: 4.8657 - val_accuracy: 0.6111\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.1222 - accuracy: 0.9444 - val_loss: 4.8600 - val_accuracy: 0.5778\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 1.1005 - accuracy: 0.9383 - val_loss: 4.8515 - val_accuracy: 0.5778\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 1.0866 - accuracy: 0.9457 - val_loss: 4.8448 - val_accuracy: 0.5778\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 1.0846 - accuracy: 0.9444 - val_loss: 4.8429 - val_accuracy: 0.6111\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.0433 - accuracy: 0.9407 - val_loss: 4.8340 - val_accuracy: 0.5778\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 4s 286ms/step - loss: 1.0267 - accuracy: 0.9481 - val_loss: 4.8267 - val_accuracy: 0.5889\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 1.0016 - accuracy: 0.9481 - val_loss: 4.8183 - val_accuracy: 0.5889\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 1.0048 - accuracy: 0.9444 - val_loss: 4.8169 - val_accuracy: 0.6111\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 0.9864 - accuracy: 0.9481 - val_loss: 4.8079 - val_accuracy: 0.5778\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.9899 - accuracy: 0.9457 - val_loss: 4.8066 - val_accuracy: 0.5667\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.9425 - accuracy: 0.9494 - val_loss: 4.7999 - val_accuracy: 0.5889\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 4s 285ms/step - loss: 0.9249 - accuracy: 0.9519 - val_loss: 4.7919 - val_accuracy: 0.5667\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.9240 - accuracy: 0.9469 - val_loss: 4.7847 - val_accuracy: 0.5778\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.8938 - accuracy: 0.9519 - val_loss: 4.7818 - val_accuracy: 0.5889\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 0.8829 - accuracy: 0.9506 - val_loss: 4.7788 - val_accuracy: 0.5889\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 4s 286ms/step - loss: 0.8640 - accuracy: 0.9494 - val_loss: 4.7738 - val_accuracy: 0.5889\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.8575 - accuracy: 0.9531 - val_loss: 4.7693 - val_accuracy: 0.5778\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.8358 - accuracy: 0.9543 - val_loss: 4.7706 - val_accuracy: 0.5889\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.8282 - accuracy: 0.9568 - val_loss: 4.7671 - val_accuracy: 0.5889\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 0.8173 - accuracy: 0.9568 - val_loss: 4.7639 - val_accuracy: 0.5889\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 4s 285ms/step - loss: 0.7879 - accuracy: 0.9531 - val_loss: 4.7565 - val_accuracy: 0.5889\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 0.7938 - accuracy: 0.9580 - val_loss: 4.7512 - val_accuracy: 0.5778\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.7843 - accuracy: 0.9568 - val_loss: 4.7487 - val_accuracy: 0.5778\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 4s 307ms/step - loss: 0.7638 - accuracy: 0.9580 - val_loss: 4.7424 - val_accuracy: 0.5889\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 0.7486 - accuracy: 0.9568 - val_loss: 4.7392 - val_accuracy: 0.5778\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.7442 - accuracy: 0.9593 - val_loss: 4.7350 - val_accuracy: 0.5889\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.7518 - accuracy: 0.9531 - val_loss: 4.7354 - val_accuracy: 0.5889\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.7135 - accuracy: 0.9593 - val_loss: 4.7283 - val_accuracy: 0.6000\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.7368 - accuracy: 0.9593 - val_loss: 4.7259 - val_accuracy: 0.5889\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.7128 - accuracy: 0.9605 - val_loss: 4.7212 - val_accuracy: 0.5889\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.6879 - accuracy: 0.9580 - val_loss: 4.7194 - val_accuracy: 0.5889\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.6806 - accuracy: 0.9580 - val_loss: 4.7175 - val_accuracy: 0.6000\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 4s 306ms/step - loss: 0.6856 - accuracy: 0.9568 - val_loss: 4.7103 - val_accuracy: 0.5889\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.6630 - accuracy: 0.9543 - val_loss: 4.7088 - val_accuracy: 0.5889\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.6589 - accuracy: 0.9654 - val_loss: 4.7056 - val_accuracy: 0.5889\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.6406 - accuracy: 0.9667 - val_loss: 4.7021 - val_accuracy: 0.5889\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 0.6385 - accuracy: 0.9679 - val_loss: 4.6989 - val_accuracy: 0.6000\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.6233 - accuracy: 0.9691 - val_loss: 4.6929 - val_accuracy: 0.6000\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 4s 284ms/step - loss: 0.6210 - accuracy: 0.9679 - val_loss: 4.6923 - val_accuracy: 0.6000\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.6189 - accuracy: 0.9704 - val_loss: 4.6906 - val_accuracy: 0.5889\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 4s 287ms/step - loss: 0.6101 - accuracy: 0.9593 - val_loss: 4.6881 - val_accuracy: 0.6000\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 4s 288ms/step - loss: 0.5926 - accuracy: 0.9642 - val_loss: 4.6799 - val_accuracy: 0.5889\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 0.5962 - accuracy: 0.9716 - val_loss: 4.6792 - val_accuracy: 0.5889\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.5881 - accuracy: 0.9704 - val_loss: 4.6781 - val_accuracy: 0.5889\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 0.5947 - accuracy: 0.9691 - val_loss: 4.6753 - val_accuracy: 0.5889\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.5728 - accuracy: 0.9716 - val_loss: 4.6713 - val_accuracy: 0.5889\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.5628 - accuracy: 0.9691 - val_loss: 4.6730 - val_accuracy: 0.5667\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.5628 - accuracy: 0.9679 - val_loss: 4.6674 - val_accuracy: 0.5778\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.5708 - accuracy: 0.9691 - val_loss: 4.6641 - val_accuracy: 0.5778\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 0.5631 - accuracy: 0.9704 - val_loss: 4.6560 - val_accuracy: 0.5778\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 0.5440 - accuracy: 0.9716 - val_loss: 4.6515 - val_accuracy: 0.5889\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.5420 - accuracy: 0.9778 - val_loss: 4.6558 - val_accuracy: 0.5889\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.5339 - accuracy: 0.9741 - val_loss: 4.6563 - val_accuracy: 0.5778\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 4s 287ms/step - loss: 0.5274 - accuracy: 0.9741 - val_loss: 4.6556 - val_accuracy: 0.5778\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.5385 - accuracy: 0.9704 - val_loss: 4.6490 - val_accuracy: 0.5778\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.5194 - accuracy: 0.9741 - val_loss: 4.6529 - val_accuracy: 0.5778\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.5319 - accuracy: 0.9716 - val_loss: 4.6519 - val_accuracy: 0.5667\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.5131 - accuracy: 0.9728 - val_loss: 4.6488 - val_accuracy: 0.5778\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 0.5050 - accuracy: 0.9778 - val_loss: 4.6455 - val_accuracy: 0.5667\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 0.5133 - accuracy: 0.9765 - val_loss: 4.6440 - val_accuracy: 0.5667\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4888 - accuracy: 0.9753 - val_loss: 4.6422 - val_accuracy: 0.5667\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 0.5002 - accuracy: 0.9753 - val_loss: 4.6440 - val_accuracy: 0.5556\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.4936 - accuracy: 0.9728 - val_loss: 4.6401 - val_accuracy: 0.5667\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4922 - accuracy: 0.9778 - val_loss: 4.6357 - val_accuracy: 0.5667\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4819 - accuracy: 0.9790 - val_loss: 4.6358 - val_accuracy: 0.5556\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4798 - accuracy: 0.9802 - val_loss: 4.6395 - val_accuracy: 0.5778\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 4s 284ms/step - loss: 0.4733 - accuracy: 0.9802 - val_loss: 4.6387 - val_accuracy: 0.5556\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 4s 287ms/step - loss: 0.4730 - accuracy: 0.9778 - val_loss: 4.6347 - val_accuracy: 0.5667\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 0.4866 - accuracy: 0.9778 - val_loss: 4.6353 - val_accuracy: 0.5556\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.4595 - accuracy: 0.9802 - val_loss: 4.6353 - val_accuracy: 0.5556\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 4s 283ms/step - loss: 0.4573 - accuracy: 0.9753 - val_loss: 4.6345 - val_accuracy: 0.5444\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4552 - accuracy: 0.9778 - val_loss: 4.6294 - val_accuracy: 0.5667\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.4548 - accuracy: 0.9852 - val_loss: 4.6251 - val_accuracy: 0.5556\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 4s 278ms/step - loss: 0.4486 - accuracy: 0.9802 - val_loss: 4.6248 - val_accuracy: 0.5556\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 4s 277ms/step - loss: 0.4485 - accuracy: 0.9741 - val_loss: 4.6228 - val_accuracy: 0.5556\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 0.4328 - accuracy: 0.9790 - val_loss: 4.6229 - val_accuracy: 0.5556\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.4380 - accuracy: 0.9753 - val_loss: 4.6213 - val_accuracy: 0.5444\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 4s 275ms/step - loss: 0.4466 - accuracy: 0.9778 - val_loss: 4.6227 - val_accuracy: 0.5556\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.4277 - accuracy: 0.9790 - val_loss: 4.6199 - val_accuracy: 0.5556\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.4261 - accuracy: 0.9778 - val_loss: 4.6205 - val_accuracy: 0.5333\n",
      "4/4 [==============================] - 0s 24ms/step\n",
      "[[0.00029286 0.00033722 0.00040623 ... 0.00037734 0.00031173 0.00038314]\n",
      " [0.00070445 0.00036858 0.00037236 ... 0.0002549  0.00026777 0.00015292]\n",
      " [0.00044893 0.00030898 0.00056801 ... 0.00031775 0.00020455 0.00036542]\n",
      " ...\n",
      " [0.00023846 0.00026125 0.0003208  ... 0.00035653 0.00030357 0.0003009 ]\n",
      " [0.00028718 0.00030028 0.00042451 ... 0.00025087 0.0004784  0.000372  ]\n",
      " [0.00053519 0.00037375 0.00021661 ... 0.0002576  0.00029009 0.00023628]] (100, 2567)\n",
      "D20\n",
      "D16\n",
      "D3\n",
      "D16\n",
      "D16\n",
      "D15\n",
      "D12\n",
      "D1\n",
      "D15\n",
      "D20\n",
      "D16\n",
      "D12\n",
      "D15\n",
      "D28\n",
      "D12\n",
      "D20\n",
      "D12\n",
      "D15\n",
      "D28\n",
      "D15\n",
      "D16\n",
      "D12\n",
      "D1\n",
      "D12\n",
      "D20\n",
      "D12\n",
      "D16\n",
      "D15\n",
      "D20\n",
      "D20\n",
      "D15\n",
      "D12\n",
      "D28\n",
      "D12\n",
      "D12\n",
      "D12\n",
      "D1\n",
      "D1\n",
      "D16\n",
      "D12\n",
      "D1\n",
      "D15\n",
      "D12\n",
      "D1\n",
      "D20\n",
      "D16\n",
      "D15\n",
      "D20\n",
      "D20\n",
      "D20\n",
      "D15\n",
      "D12\n",
      "D1\n",
      "D12\n",
      "D15\n",
      "D20\n",
      "D1\n",
      "D20\n",
      "D15\n",
      "D12\n",
      "D12\n",
      "D28\n",
      "D15\n",
      "D20\n",
      "D20\n",
      "D3\n",
      "D1\n",
      "D20\n",
      "D20\n",
      "D20\n",
      "D20\n",
      "D1\n",
      "D20\n",
      "D12\n",
      "D3\n",
      "D16\n",
      "D15\n",
      "D20\n",
      "D12\n",
      "D16\n",
      "D12\n",
      "D1\n",
      "D12\n",
      "D1\n",
      "D1\n",
      "D1\n",
      "D20\n",
      "D12\n",
      "D12\n",
      "D28\n",
      "D28\n",
      "D20\n",
      "D12\n",
      "D16\n",
      "D12\n",
      "D20\n",
      "D12\n",
      "D20\n",
      "D12\n",
      "D16\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, units, activation='sigmoid', kernel_initializer='glorot_uniform', **kwargs):\n",
    "        super(DenseLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', shape=(self.units,), initializer='zeros', trainable=True)\n",
    "        super(DenseLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = tf.matmul(inputs, self.kernel) + self.bias\n",
    "        if self.activation:\n",
    "            activation_layer = Activation(self.activation)\n",
    "            outputs = activation_layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)\n",
    "def DeepNN(X_train, y_train, X_test):# ,sc):\n",
    "    # create a model \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Embedding, Flatten, BatchNormalization\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    hidden_layer_1 = 1000\n",
    "    hidden_layer_2 = 100\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(DenseLayer(hidden_layer_1, activation='sigmoid', input_dim = X_train.shape[1], kernel_initializer=GlorotUniform()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DenseLayer(hidden_layer_2, activation='sigmoid', kernel_initializer=GlorotUniform()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DenseLayer(output_dim, activation='softmax'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=weight_decay, nesterov=True),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    hist_gru = model.fit(X_train, y_train, epochs=200, validation_split=0.1, batch_size=64)\n",
    "          \n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    \n",
    "    return model, prediction, hist_gru\n",
    "\n",
    "\n",
    "model, prediction, hist_gru = DeepNN(X_train, y_train, X_test)#, sc)\n",
    "\n",
    "\n",
    "#print(\"train_onehots.shape, test_onehots.shape : \", train_onehots.shape, test_onehots.shape)\n",
    "#print(\"flatten_train_onehots.shape, flatten_test_onehots.shape : \", flatten_train_onehots.shape, flatten_test_onehots.shape)\n",
    "\n",
    "predicted_labels = np.argmax(prediction, axis=1)\n",
    "\n",
    "print(prediction,prediction.shape)\n",
    "\n",
    "pred_label_list = []\n",
    "for label in predicted_labels:\n",
    "    pred_label_list.append(id_to_word[label])\n",
    "    print(id_to_word[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac984687",
   "metadata": {
    "papermill": {
     "duration": 0.197736,
     "end_time": "2023-03-24T10:58:36.868876",
     "exception": false,
     "start_time": "2023-03-24T10:58:36.671140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a521170e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T10:58:37.267632Z",
     "iopub.status.busy": "2023-03-24T10:58:37.267184Z",
     "iopub.status.idle": "2023-03-24T10:58:37.316719Z",
     "shell.execute_reply": "2023-03-24T10:58:37.315400Z"
    },
    "papermill": {
     "duration": 0.252263,
     "end_time": "2023-03-24T10:58:37.319467",
     "exception": false,
     "start_time": "2023-03-24T10:58:37.067204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>D20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>D16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>D16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S005</td>\n",
       "      <td>D16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>S096</td>\n",
       "      <td>D20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>S097</td>\n",
       "      <td>D12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>S098</td>\n",
       "      <td>D20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>S099</td>\n",
       "      <td>D12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>S100</td>\n",
       "      <td>D16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id pred\n",
       "0   S001  D20\n",
       "1   S002  D16\n",
       "2   S003   D3\n",
       "3   S004  D16\n",
       "4   S005  D16\n",
       "..   ...  ...\n",
       "95  S096  D20\n",
       "96  S097  D12\n",
       "97  S098  D20\n",
       "98  S099  D12\n",
       "99  S100  D16\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list = []\n",
    "for i in range(1,len(pred_label_list)+1):\n",
    "    index_list.append(f\"S{i:03}\")\n",
    "\n",
    "#prediction = pd.DataFrame(pred_label_list, index = index_list)\n",
    "\n",
    "prediction = pd.DataFrame(columns=['id', 'pred'])\n",
    "\n",
    "prediction[\"id\"] = index_list\n",
    "prediction[\"pred\"] = pred_label_list\n",
    "\n",
    "prediction = prediction.reset_index(drop=True)\n",
    "\n",
    "#prediction = pd.DataFrame(pred_label_list, index=[f\"S{i:03}\" for i in range(1, len(pred_label_list)+1)])\n",
    "\n",
    "#prediction.columns = [\"id\",\"pred\"]\n",
    "#prediction.index = index_list\n",
    "prediction.to_csv('20221119_하준서_simple_seq.answer.csv', index = False)\n",
    "\n",
    "#index_list\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 759.513283,
   "end_time": "2023-03-24T10:58:40.670636",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-24T10:46:01.157353",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
