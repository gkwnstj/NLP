{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d3fa0d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-30T12:00:55.357059Z",
     "iopub.status.busy": "2023-04-30T12:00:55.356643Z",
     "iopub.status.idle": "2023-04-30T12:00:55.377848Z",
     "shell.execute_reply": "2023-04-30T12:00:55.376795Z"
    },
    "papermill": {
     "duration": 0.036034,
     "end_time": "2023-04-30T12:00:55.380170",
     "exception": false,
     "start_time": "2023-04-30T12:00:55.344136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2023-spring-nlp-lab3-problem1/classification_class.pred.csv\n",
      "/kaggle/input/datadata/classification_datasets/test_set.csv\n",
      "/kaggle/input/datadata/classification_datasets/train_set.csv\n",
      "/kaggle/input/datadata/classification_datasets/glove_word.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ae6a2",
   "metadata": {
    "papermill": {
     "duration": 0.009492,
     "end_time": "2023-04-30T12:00:55.399639",
     "exception": false,
     "start_time": "2023-04-30T12:00:55.390147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33553851",
   "metadata": {
    "papermill": {
     "duration": 0.009189,
     "end_time": "2023-04-30T12:00:55.418486",
     "exception": false,
     "start_time": "2023-04-30T12:00:55.409297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7000b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:00:55.440937Z",
     "iopub.status.busy": "2023-04-30T12:00:55.439418Z",
     "iopub.status.idle": "2023-04-30T12:00:55.484635Z",
     "shell.execute_reply": "2023-04-30T12:00:55.483485Z"
    },
    "papermill": {
     "duration": 0.0596,
     "end_time": "2023-04-30T12:00:55.487406",
     "exception": false,
     "start_time": "2023-04-30T12:00:55.427806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               sentence  label\n",
       " 0     Cortez goodness Englishmen principle prenatal ...      1\n",
       " 1     Cortez Falklands affect legend Terrified chief...      5\n",
       " 2     Cortez Falklands whisky Stairway resources cha...      5\n",
       " 3     vermicilli Protestant daughter whisky Sinclair...      4\n",
       " 4     Kathie whisky Periodic subject yet adding Phil...      5\n",
       " ...                                                 ...    ...\n",
       " 4995  Cortez Degas Englishmen murdering mess Casper ...      1\n",
       " 4996  Cortez Robin Make-up Surveyor Madilyn might pr...      0\n",
       " 4997               Kathie Robin foods within Bros. Noah      0\n",
       " 4998  roulette titled Englishmen Presidents 007 IRL ...      3\n",
       " 4999  San folk Panther disease will Will Philip addi...      0\n",
       " \n",
       " [5000 rows x 2 columns],\n",
       " pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = pd.read_csv(\"/kaggle/input/datadata/classification_datasets/train_set.csv\", index_col = 0)\n",
    "\n",
    "train_file, type(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dceef40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:00:55.508019Z",
     "iopub.status.busy": "2023-04-30T12:00:55.507734Z",
     "iopub.status.idle": "2023-04-30T12:00:57.751071Z",
     "shell.execute_reply": "2023-04-30T12:00:57.749917Z"
    },
    "papermill": {
     "duration": 2.256208,
     "end_time": "2023-04-30T12:00:57.753478",
     "exception": false,
     "start_time": "2023-04-30T12:00:55.497270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cortez goodness Englishmen principle prenatal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Cortez, goodness, Englishmen, principle, pren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cortez Falklands affect legend Terrified chief...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Cortez, Falklands, affect, legend, Terrified,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cortez Falklands whisky Stairway resources cha...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Cortez, Falklands, whisky, Stairway, resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vermicilli Protestant daughter whisky Sinclair...</td>\n",
       "      <td>4</td>\n",
       "      <td>[vermicilli, Protestant, daughter, whisky, Sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kathie whisky Periodic subject yet adding Phil...</td>\n",
       "      <td>5</td>\n",
       "      <td>[Kathie, whisky, Periodic, subject, yet, addin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Cortez Degas Englishmen murdering mess Casper ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Cortez, Degas, Englishmen, murdering, mess, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Cortez Robin Make-up Surveyor Madilyn might pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Cortez, Robin, Make-up, Surveyor, Madilyn, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Kathie Robin foods within Bros. Noah</td>\n",
       "      <td>0</td>\n",
       "      <td>[Kathie, Robin, foods, within, Bros., Noah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>roulette titled Englishmen Presidents 007 IRL ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[roulette, titled, Englishmen, Presidents, 007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>San folk Panther disease will Will Philip addi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[San, folk, Panther, disease, will, Will, Phil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "0     Cortez goodness Englishmen principle prenatal ...      1   \n",
       "1     Cortez Falklands affect legend Terrified chief...      5   \n",
       "2     Cortez Falklands whisky Stairway resources cha...      5   \n",
       "3     vermicilli Protestant daughter whisky Sinclair...      4   \n",
       "4     Kathie whisky Periodic subject yet adding Phil...      5   \n",
       "...                                                 ...    ...   \n",
       "4995  Cortez Degas Englishmen murdering mess Casper ...      1   \n",
       "4996  Cortez Robin Make-up Surveyor Madilyn might pr...      0   \n",
       "4997               Kathie Robin foods within Bros. Noah      0   \n",
       "4998  roulette titled Englishmen Presidents 007 IRL ...      3   \n",
       "4999  San folk Panther disease will Will Philip addi...      0   \n",
       "\n",
       "                                     tokenized_sentence  \n",
       "0     [Cortez, goodness, Englishmen, principle, pren...  \n",
       "1     [Cortez, Falklands, affect, legend, Terrified,...  \n",
       "2     [Cortez, Falklands, whisky, Stairway, resource...  \n",
       "3     [vermicilli, Protestant, daughter, whisky, Sin...  \n",
       "4     [Kathie, whisky, Periodic, subject, yet, addin...  \n",
       "...                                                 ...  \n",
       "4995  [Cortez, Degas, Englishmen, murdering, mess, C...  \n",
       "4996  [Cortez, Robin, Make-up, Surveyor, Madilyn, mi...  \n",
       "4997        [Kathie, Robin, foods, within, Bros., Noah]  \n",
       "4998  [roulette, titled, Englishmen, Presidents, 007...  \n",
       "4999  [San, folk, Panther, disease, will, Will, Phil...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt') # Download the required punkt tokenizer\n",
    "nltk.download('wordnet') # Download the required WordNet corpus\n",
    "\n",
    "train_file = pd.read_csv(\"/kaggle/input/datadata/classification_datasets/train_set.csv\", index_col = 0)\n",
    "\n",
    "# Tokenize the sentences in the 'sentence' column of the train_file DataFrame\n",
    "train_file['tokenized_sentence'] = train_file['sentence'].apply(word_tokenize)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with tokenized sentences\n",
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e5193c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:00:57.776995Z",
     "iopub.status.busy": "2023-04-30T12:00:57.775265Z",
     "iopub.status.idle": "2023-04-30T12:00:59.042201Z",
     "shell.execute_reply": "2023-04-30T12:00:59.041050Z"
    },
    "papermill": {
     "duration": 1.280563,
     "end_time": "2023-04-30T12:00:59.044574",
     "exception": false,
     "start_time": "2023-04-30T12:00:57.764011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891948bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:00:59.067081Z",
     "iopub.status.busy": "2023-04-30T12:00:59.066093Z",
     "iopub.status.idle": "2023-04-30T12:01:00.985899Z",
     "shell.execute_reply": "2023-04-30T12:01:00.984916Z"
    },
    "papermill": {
     "duration": 1.933594,
     "end_time": "2023-04-30T12:01:00.988466",
     "exception": false,
     "start_time": "2023-04-30T12:00:59.054872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Cortez', 'goodness', 'Englishmen', 'principle', 'prenatal', 'symbolize', 'sword', 'Shia', 'Led', 'Noah']),\n",
       "       list(['Cortez', 'Falklands', 'affect', 'legend', 'Terrified', 'chiefly', 'belong', 'bloom', '1935', 'Noah']),\n",
       "       list(['Cortez', 'Falklands', 'whisky', 'Stairway', 'resource', 'chance', 'jeroboam', 'dummy', 'Bros.', 'Bullwinkle', 'adding', 'Darius', 'Noah']),\n",
       "       ..., list(['Kathie', 'Robin', 'food', 'within', 'Bros.', 'Noah']),\n",
       "       list(['roulette', 'titled', 'Englishmen', 'Presidents', '007', 'IRL', 'bloom', 'attends', 'Noah']),\n",
       "       list(['San', 'folk', 'Panther', 'disease', 'will', 'Will', 'Philip', 'adding', 'bulb', 'resident', 'Noah'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokenized sentences in the 'tokenized_sentence' column\n",
    "train_file['lemmatized_sentence'] = train_file['tokenized_sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with lemmatized sentences\n",
    "\n",
    "train_file['lemmatized_sentence'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1d2d8",
   "metadata": {
    "papermill": {
     "duration": 0.010571,
     "end_time": "2023-04-30T12:01:01.010170",
     "exception": false,
     "start_time": "2023-04-30T12:01:00.999599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Max_length Sentence(20)_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31809979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.032906Z",
     "iopub.status.busy": "2023-04-30T12:01:01.032516Z",
     "iopub.status.idle": "2023-04-30T12:01:01.095115Z",
     "shell.execute_reply": "2023-04-30T12:01:01.094131Z"
    },
    "papermill": {
     "duration": 0.076782,
     "end_time": "2023-04-30T12:01:01.097539",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.020757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['Cortez', 'goodness', 'Englishmen', 'principle', 'prenatal', 'symbolize', 'sword', 'Shia', 'Led', 'Noah']),\n",
       "        list(['Cortez', 'Falklands', 'affect', 'legend', 'Terrified', 'chiefly', 'belong', 'bloom', '1935', 'Noah']),\n",
       "        list(['Cortez', 'Falklands', 'whisky', 'Stairway', 'resource', 'chance', 'jeroboam', 'dummy', 'Bros.', 'Bullwinkle', 'adding', 'Darius', 'Noah']),\n",
       "        ..., list(['Kathie', 'Robin', 'food', 'within', 'Bros.', 'Noah']),\n",
       "        list(['roulette', 'titled', 'Englishmen', 'Presidents', '007', 'IRL', 'bloom', 'attends', 'Noah']),\n",
       "        list(['San', 'folk', 'Panther', 'disease', 'will', 'Will', 'Philip', 'adding', 'bulb', 'resident', 'Noah'])],\n",
       "       dtype=object),\n",
       " 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = train_file['lemmatized_sentence'].values # 데이터 배열\n",
    "\n",
    "max_words = 20 # 최대 단어 개수     # 문장 길이\n",
    "\n",
    "# 배열의 각 요소에서 20개의 단어만 선택하는 함수\n",
    "def select_words(arr):\n",
    "    return [word for word in arr[:max_words]]\n",
    "\n",
    "# 배열의 각 요소에서 20개의 단어만 선택하는 코드\n",
    "new_data = np.array([select_words(arr) if len(arr) > max_words else arr for arr in data], dtype=object)\n",
    "new_data\n",
    "\n",
    "\n",
    "max_len = max(len(lst) for lst in new_data)      # 가장 긴 리스트의 길이\n",
    "new_data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c2b334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.120809Z",
     "iopub.status.busy": "2023-04-30T12:01:01.120341Z",
     "iopub.status.idle": "2023-04-30T12:01:01.127739Z",
     "shell.execute_reply": "2023-04-30T12:01:01.126479Z"
    },
    "papermill": {
     "duration": 0.022449,
     "end_time": "2023-04-30T12:01:01.130615",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.108166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Cortez', 'goodness', 'Englishmen', 'principle', 'prenatal', 'symbolize', 'sword', 'Shia', 'Led', 'Noah']),\n",
       "       list(['Cortez', 'Falklands', 'affect', 'legend', 'Terrified', 'chiefly', 'belong', 'bloom', '1935', 'Noah']),\n",
       "       list(['Cortez', 'Falklands', 'whisky', 'Stairway', 'resource', 'chance', 'jeroboam', 'dummy', 'Bros.', 'Bullwinkle', 'adding', 'Darius', 'Noah']),\n",
       "       ..., list(['Kathie', 'Robin', 'food', 'within', 'Bros.', 'Noah']),\n",
       "       list(['roulette', 'titled', 'Englishmen', 'Presidents', '007', 'IRL', 'bloom', 'attends', 'Noah']),\n",
       "       list(['San', 'folk', 'Panther', 'disease', 'will', 'Will', 'Philip', 'adding', 'bulb', 'resident', 'Noah'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7264b78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.153963Z",
     "iopub.status.busy": "2023-04-30T12:01:01.153652Z",
     "iopub.status.idle": "2023-04-30T12:01:01.160356Z",
     "shell.execute_reply": "2023-04-30T12:01:01.159341Z"
    },
    "papermill": {
     "duration": 0.020871,
     "end_time": "2023-04-30T12:01:01.162489",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.141618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cortez',\n",
       "  'goodness',\n",
       "  'Englishmen',\n",
       "  'principle',\n",
       "  'prenatal',\n",
       "  'symbolize',\n",
       "  'sword',\n",
       "  'Shia',\n",
       "  'Led',\n",
       "  'Noah'],\n",
       " ['Cortez',\n",
       "  'Falklands',\n",
       "  'affect',\n",
       "  'legend',\n",
       "  'Terrified',\n",
       "  'chiefly',\n",
       "  'belong',\n",
       "  'bloom',\n",
       "  '1935',\n",
       "  'Noah'],\n",
       " ['Cortez',\n",
       "  'Falklands',\n",
       "  'whisky',\n",
       "  'Stairway',\n",
       "  'resource',\n",
       "  'chance',\n",
       "  'jeroboam',\n",
       "  'dummy',\n",
       "  'Bros.',\n",
       "  'Bullwinkle',\n",
       "  'adding',\n",
       "  'Darius',\n",
       "  'Noah']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_data = new_data.tolist()\n",
    "#a = sum(tuple_data , [])\n",
    "\n",
    "tuple_data[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af049521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.186110Z",
     "iopub.status.busy": "2023-04-30T12:01:01.185277Z",
     "iopub.status.idle": "2023-04-30T12:01:01.226282Z",
     "shell.execute_reply": "2023-04-30T12:01:01.225119Z"
    },
    "papermill": {
     "duration": 0.055557,
     "end_time": "2023-04-30T12:01:01.228826",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.173269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   'Cortez',\n",
       "   'goodness',\n",
       "   'Englishmen',\n",
       "   'principle',\n",
       "   'prenatal',\n",
       "   'symbolize',\n",
       "   'sword',\n",
       "   'Shia',\n",
       "   'Led',\n",
       "   'Noah'],\n",
       "  ['[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   'Cortez',\n",
       "   'Falklands',\n",
       "   'affect',\n",
       "   'legend',\n",
       "   'Terrified',\n",
       "   'chiefly',\n",
       "   'belong',\n",
       "   'bloom',\n",
       "   '1935',\n",
       "   'Noah']],\n",
       " (5000, 20))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 20\n",
    "pad_token = '[PAD]'\n",
    "num_data = len(new_data)\n",
    "new_data_padded = []\n",
    "for sentence in new_data:\n",
    "    if len(sentence) < max_len:\n",
    "        num_padding = max_len - len(sentence)\n",
    "        sentence = [pad_token] * num_padding + sentence\n",
    "    else:\n",
    "        sentence = sentence[:max_len]\n",
    "    new_data_padded.append(sentence)\n",
    "    \n",
    "new_data_padded[0:2], np.array(new_data_padded).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e725f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.252358Z",
     "iopub.status.busy": "2023-04-30T12:01:01.251824Z",
     "iopub.status.idle": "2023-04-30T12:01:01.259236Z",
     "shell.execute_reply": "2023-04-30T12:01:01.258236Z"
    },
    "papermill": {
     "duration": 0.021102,
     "end_time": "2023-04-30T12:01:01.261282",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.240180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  'Cortez',\n",
       "  'goodness',\n",
       "  'Englishmen',\n",
       "  'principle',\n",
       "  'prenatal',\n",
       "  'symbolize',\n",
       "  'sword',\n",
       "  'Shia',\n",
       "  'Led',\n",
       "  'Noah'],\n",
       " ['[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  'Cortez',\n",
       "  'Falklands',\n",
       "  'affect',\n",
       "  'legend',\n",
       "  'Terrified',\n",
       "  'chiefly',\n",
       "  'belong',\n",
       "  'bloom',\n",
       "  '1935',\n",
       "  'Noah']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = new_data_padded\n",
    "train_data[0:2]#, train_data.shape\n",
    "\n",
    "#train_data = train_data[0:2]\n",
    "\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfca90",
   "metadata": {
    "papermill": {
     "duration": 0.010553,
     "end_time": "2023-04-30T12:01:01.282724",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.272171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c95353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:01.306879Z",
     "iopub.status.busy": "2023-04-30T12:01:01.306068Z",
     "iopub.status.idle": "2023-04-30T12:01:03.088431Z",
     "shell.execute_reply": "2023-04-30T12:01:03.087184Z"
    },
    "papermill": {
     "duration": 1.797127,
     "end_time": "2023-04-30T12:01:03.091296",
     "exception": false,
     "start_time": "2023-04-30T12:01:01.294169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/kaggle/input/datadata/classification_datasets/glove_word.json', 'r') as f:   # word to vec dictionary\n",
    "\n",
    "    json_data = json.load(f)\n",
    "\n",
    "#print(json.dumps(json_data))\n",
    "\n",
    "# print(json_data)\n",
    "\n",
    "#dic = pd.read_csv(\"/kaggle/input/datadata/classification_datasets/glove_word.json\")\n",
    "dic = json_data\n",
    "\n",
    "#len(list(dic.values())[0])\n",
    "\n",
    "#dic.values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b88a9c",
   "metadata": {
    "papermill": {
     "duration": 0.010715,
     "end_time": "2023-04-30T12:01:03.113003",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.102288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pretrained_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807125e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T04:26:24.337471Z",
     "iopub.status.busy": "2023-04-27T04:26:24.337098Z",
     "iopub.status.idle": "2023-04-27T04:26:24.532304Z",
     "shell.execute_reply": "2023-04-27T04:26:24.529732Z",
     "shell.execute_reply.started": "2023-04-27T04:26:24.337432Z"
    },
    "papermill": {
     "duration": 0.011225,
     "end_time": "2023-04-30T12:01:03.134856",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.123631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#X_train, X_train.shape\n",
    "emb = np.array(list(dic.values()))\n",
    "\n",
    "#b = a[X_train]\n",
    "\n",
    "#b.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "084e85e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.157653Z",
     "iopub.status.busy": "2023-04-30T12:01:03.157318Z",
     "iopub.status.idle": "2023-04-30T12:01:03.164985Z",
     "shell.execute_reply": "2023-04-30T12:01:03.163840Z"
    },
    "papermill": {
     "duration": 0.021716,
     "end_time": "2023-04-30T12:01:03.167443",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.145727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = list(range(0,len(dic)))\n",
    "\n",
    "dic_list = list(dic.keys())\n",
    "\n",
    "id_to_word = dict(zip(idx, dic_list))\n",
    "\n",
    "word_to_id = {i: w for w, i in id_to_word.items()}\n",
    "#word_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f012241b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.190624Z",
     "iopub.status.busy": "2023-04-30T12:01:03.190115Z",
     "iopub.status.idle": "2023-04-30T12:01:03.198422Z",
     "shell.execute_reply": "2023-04-30T12:01:03.197531Z"
    },
    "papermill": {
     "duration": 0.022436,
     "end_time": "2023-04-30T12:01:03.200566",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.178130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4bc07bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.223819Z",
     "iopub.status.busy": "2023-04-30T12:01:03.223033Z",
     "iopub.status.idle": "2023-04-30T12:01:03.229979Z",
     "shell.execute_reply": "2023-04-30T12:01:03.228834Z"
    },
    "papermill": {
     "duration": 0.020793,
     "end_time": "2023-04-30T12:01:03.232142",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.211349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8728d",
   "metadata": {
    "papermill": {
     "duration": 0.010979,
     "end_time": "2023-04-30T12:01:03.254266",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.243287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train_dataset(Word to idx(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb737d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.277389Z",
     "iopub.status.busy": "2023-04-30T12:01:03.276881Z",
     "iopub.status.idle": "2023-04-30T12:01:03.338677Z",
     "shell.execute_reply": "2023-04-30T12:01:03.337769Z"
    },
    "papermill": {
     "duration": 0.075509,
     "end_time": "2023-04-30T12:01:03.340688",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.265179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 3212, 1461, 4942],\n",
       "       [   0,    0,    0, ..., 2031, 2023, 4942],\n",
       "       [   0,    0,    0, ..., 4940, 6956, 4942],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 7796, 4495, 4942],\n",
       "       [   0,    0,    0, ..., 2031, 1382, 4942],\n",
       "       [   0,    0,    0, ...,    1, 4113, 4942]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일련번호 데이터\n",
    "train_inputs = []\n",
    "for s in train_data:                                 ##### 문장별 반복\n",
    "    #print(s)\n",
    "    #print(\"print SSSSS\")\n",
    "    row= []\n",
    "    for w in s:\n",
    "        #print(w)\n",
    "        #print(\"WWWWWWWWW\")\n",
    "        if w in word_to_id:\n",
    "            row.append(word_to_id[w])\n",
    "            #print(row)\n",
    "        else:\n",
    "            row.append(word_to_id['[UNK]'])\n",
    "    #print(row)                                    ##### 번호 나열\n",
    "    train_inputs.append(row)\n",
    "train_inputs = np.array(train_inputs)\n",
    "\n",
    "train_inputs, train_inputs.shape\n",
    "\n",
    "X_train = train_inputs\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7e125",
   "metadata": {
    "papermill": {
     "duration": 0.01083,
     "end_time": "2023-04-30T12:01:03.362642",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.351812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7e7ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.386219Z",
     "iopub.status.busy": "2023-04-30T12:01:03.385932Z",
     "iopub.status.idle": "2023-04-30T12:01:03.399103Z",
     "shell.execute_reply": "2023-04-30T12:01:03.397994Z"
    },
    "papermill": {
     "duration": 0.027239,
     "end_time": "2023-04-30T12:01:03.401215",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.373976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              sentence\n",
       " 0    Cortez titled Englishmen delegation Falklands ...\n",
       " 1    Cortez Phillip whisky Gutenberg iron-covered e...\n",
       " 2    Cortez Degas Englishmen Spaghetti-o touring Ca...\n",
       " 3    Cortez whisky Englishmen Dyck Casper skunks Wo...\n",
       " 4    roulette whisky non-contagious Conception stol...\n",
       " ..                                                 ...\n",
       " 447  Cortez Peloponnesian Degas Ridder bloom Line w...\n",
       " 448  van Corporal Monopoly adding Troop East Florid...\n",
       " 449        roulette Degas crayon bloom Die safari Noah\n",
       " 450  Cortez Streetcar Crimean Michaelangelo Nations...\n",
       " 451  Cortez Degas Englishmen Future Motto entertain...\n",
       " \n",
       " [452 rows x 1 columns],\n",
       " pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = pd.read_csv(\"/kaggle/input/datadata/classification_datasets/test_set.csv\")\n",
    "\n",
    "test_file, type(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "967ce6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.425065Z",
     "iopub.status.busy": "2023-04-30T12:01:03.424297Z",
     "iopub.status.idle": "2023-04-30T12:01:03.481294Z",
     "shell.execute_reply": "2023-04-30T12:01:03.480378Z"
    },
    "papermill": {
     "duration": 0.071329,
     "end_time": "2023-04-30T12:01:03.483571",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.412242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_file = pd.read_csv(\"/kaggle/input/datadata/classification_datasets/test_set.csv\")\n",
    "\n",
    "# Tokenize the sentences in the 'sentence' column of the train_file DataFrame\n",
    "test_file['tokenized_sentence'] = test_file['sentence'].apply(word_tokenize)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with tokenized sentences\n",
    "#test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99956db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.509108Z",
     "iopub.status.busy": "2023-04-30T12:01:03.507533Z",
     "iopub.status.idle": "2023-04-30T12:01:03.534426Z",
     "shell.execute_reply": "2023-04-30T12:01:03.533539Z"
    },
    "papermill": {
     "duration": 0.041302,
     "end_time": "2023-04-30T12:01:03.536567",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.495265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokenized sentences in the 'tokenized_sentence' column\n",
    "test_file['lemmatized_sentence'] = test_file['tokenized_sentence'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with lemmatized sentences\n",
    "\n",
    "#test_file['lemmatized_sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65afe206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.560925Z",
     "iopub.status.busy": "2023-04-30T12:01:03.560122Z",
     "iopub.status.idle": "2023-04-30T12:01:03.567323Z",
     "shell.execute_reply": "2023-04-30T12:01:03.566451Z"
    },
    "papermill": {
     "duration": 0.021602,
     "end_time": "2023-04-30T12:01:03.569414",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.547812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = test_file['lemmatized_sentence'].values # 데이터 배열\n",
    "\n",
    "max_words = 20 # 최대 단어 개수     # 문장 길이\n",
    "\n",
    "# 배열의 각 요소에서 20개의 단어만 선택하는 함수\n",
    "def select_words(arr):\n",
    "    return [word for word in arr[:max_words]]\n",
    "\n",
    "# 배열의 각 요소에서 20개의 단어만 선택하는 코드\n",
    "new_data = np.array([select_words(arr) if len(arr) > max_words else arr for arr in data], dtype=object)\n",
    "new_data\n",
    "\n",
    "\n",
    "max_len = max(len(lst) for lst in new_data)      # 가장 긴 리스트의 길이\n",
    "#new_data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0437dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.593055Z",
     "iopub.status.busy": "2023-04-30T12:01:03.592284Z",
     "iopub.status.idle": "2023-04-30T12:01:03.596983Z",
     "shell.execute_reply": "2023-04-30T12:01:03.596101Z"
    },
    "papermill": {
     "duration": 0.018533,
     "end_time": "2023-04-30T12:01:03.598980",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.580447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuple_data = new_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c28e9d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.655989Z",
     "iopub.status.busy": "2023-04-30T12:01:03.655094Z",
     "iopub.status.idle": "2023-04-30T12:01:03.680069Z",
     "shell.execute_reply": "2023-04-30T12:01:03.678194Z"
    },
    "papermill": {
     "duration": 0.060302,
     "end_time": "2023-04-30T12:01:03.684670",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.624368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   'Cortez',\n",
       "   'titled',\n",
       "   'Englishmen',\n",
       "   'delegation',\n",
       "   'Falklands',\n",
       "   'adding',\n",
       "   'Pass',\n",
       "   'bronze',\n",
       "   'singing',\n",
       "   '007',\n",
       "   'bloom',\n",
       "   'Define',\n",
       "   'Noah'],\n",
       "  ['[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   '[PAD]',\n",
       "   'Cortez',\n",
       "   'Phillip',\n",
       "   'whisky',\n",
       "   'Gutenberg',\n",
       "   'iron-covered',\n",
       "   'exclusively',\n",
       "   'dialing',\n",
       "   'global',\n",
       "   'Noah']],\n",
       " (452, 20))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 20\n",
    "pad_token = '[PAD]'\n",
    "num_data = len(new_data)\n",
    "new_data_padded = []\n",
    "for sentence in new_data:\n",
    "    if len(sentence) < max_len:\n",
    "        num_padding = max_len - len(sentence)\n",
    "        sentence = [pad_token] * num_padding + sentence\n",
    "    else:\n",
    "        sentence = sentence[:max_len]\n",
    "    new_data_padded.append(sentence)\n",
    "    \n",
    "new_data_padded[0:2], np.array(new_data_padded).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5a6ae64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.731678Z",
     "iopub.status.busy": "2023-04-30T12:01:03.731273Z",
     "iopub.status.idle": "2023-04-30T12:01:03.736526Z",
     "shell.execute_reply": "2023-04-30T12:01:03.735269Z"
    },
    "papermill": {
     "duration": 0.031776,
     "end_time": "2023-04-30T12:01:03.741403",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.709627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = new_data_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5930a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.785762Z",
     "iopub.status.busy": "2023-04-30T12:01:03.785460Z",
     "iopub.status.idle": "2023-04-30T12:01:03.804595Z",
     "shell.execute_reply": "2023-04-30T12:01:03.803528Z"
    },
    "papermill": {
     "duration": 0.045184,
     "end_time": "2023-04-30T12:01:03.806973",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.761789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2031, 5719, 4942],\n",
       "       [   0,    0,    0, ..., 4054, 2962, 4942],\n",
       "       [   0,    0,    0, ..., 1490, 4451, 4942],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1971, 8927, 4942],\n",
       "       [   0,    0,    0, ...,  521, 1009, 4942],\n",
       "       [   0,    0,    0, ..., 3998, 6090, 4942]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일련번호 데이터\n",
    "test_inputs = []\n",
    "for s in test_data:                                 ##### 문장별 반복\n",
    "    #print(s)\n",
    "    #print(\"print SSSSS\")\n",
    "    row= []\n",
    "    for w in s:\n",
    "        #print(w)\n",
    "        #print(\"WWWWWWWWW\")\n",
    "        if w in word_to_id:\n",
    "            row.append(word_to_id[w])\n",
    "            #print(row)\n",
    "        else:\n",
    "            row.append(word_to_id['[UNK]'])\n",
    "    #print(row)                                    ##### 번호 나열\n",
    "    test_inputs.append(row)\n",
    "test_inputs = np.array(test_inputs)\n",
    "\n",
    "test_inputs[0], test_inputs.shape\n",
    "\n",
    "X_test = test_inputs\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b02af",
   "metadata": {
    "papermill": {
     "duration": 0.011018,
     "end_time": "2023-04-30T12:01:03.829206",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.818188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd995dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.853193Z",
     "iopub.status.busy": "2023-04-30T12:01:03.852402Z",
     "iopub.status.idle": "2023-04-30T12:01:03.863968Z",
     "shell.execute_reply": "2023-04-30T12:01:03.862905Z"
    },
    "papermill": {
     "duration": 0.025884,
     "end_time": "2023-04-30T12:01:03.866155",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.840271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        ...,\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0]], dtype=uint8),\n",
       " (5000, 6))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(train_file['label']).values\n",
    "y, y.shape\n",
    "\n",
    "y_train = y\n",
    "y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f8d9a",
   "metadata": {
    "papermill": {
     "duration": 0.011027,
     "end_time": "2023-04-30T12:01:03.888488",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.877461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dfa1cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.912871Z",
     "iopub.status.busy": "2023-04-30T12:01:03.911990Z",
     "iopub.status.idle": "2023-04-30T12:01:03.919722Z",
     "shell.execute_reply": "2023-04-30T12:01:03.918755Z"
    },
    "papermill": {
     "duration": 0.021942,
     "end_time": "2023-04-30T12:01:03.921878",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.899936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ..., 3212, 1461, 4942],\n",
       "        [   0,    0,    0, ..., 2031, 2023, 4942],\n",
       "        [   0,    0,    0, ..., 4940, 6956, 4942],\n",
       "        ...,\n",
       "        [   0,    0,    0, ..., 7796, 4495, 4942],\n",
       "        [   0,    0,    0, ..., 2031, 1382, 4942],\n",
       "        [   0,    0,    0, ...,    1, 4113, 4942]]),\n",
       " (5000, 20),\n",
       " array([[   0,    0,    0, ..., 2031, 5719, 4942],\n",
       "        [   0,    0,    0, ..., 4054, 2962, 4942],\n",
       "        [   0,    0,    0, ..., 1490, 4451, 4942],\n",
       "        ...,\n",
       "        [   0,    0,    0, ..., 1971, 8927, 4942],\n",
       "        [   0,    0,    0, ...,  521, 1009, 4942],\n",
       "        [   0,    0,    0, ..., 3998, 6090, 4942]]),\n",
       " (452, 20),\n",
       " array([[0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        ...,\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0]], dtype=uint8),\n",
       " (5000, 6))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train.shape, X_test, X_test.shape, y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765f3085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:03.947029Z",
     "iopub.status.busy": "2023-04-30T12:01:03.946108Z",
     "iopub.status.idle": "2023-04-30T12:01:14.573405Z",
     "shell.execute_reply": "2023-04-30T12:01:14.572066Z"
    },
    "papermill": {
     "duration": 10.642196,
     "end_time": "2023-04-30T12:01:14.575860",
     "exception": false,
     "start_time": "2023-04-30T12:01:03.933664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9a841",
   "metadata": {
    "papermill": {
     "duration": 0.011782,
     "end_time": "2023-04-30T12:01:14.600320",
     "exception": false,
     "start_time": "2023-04-30T12:01:14.588538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rnn1 = nn.RNN(300*20, 512, batch_first = True)\n",
    "zero_state = torch.zeros(1, batch_size, 512)\n",
    "rnn_output1, hidden = rnn1(flatten, zero_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74806f86",
   "metadata": {
    "papermill": {
     "duration": 0.01712,
     "end_time": "2023-04-30T12:01:14.639719",
     "exception": false,
     "start_time": "2023-04-30T12:01:14.622599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaeb8657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:14.687187Z",
     "iopub.status.busy": "2023-04-30T12:01:14.686311Z",
     "iopub.status.idle": "2023-04-30T12:01:17.368689Z",
     "shell.execute_reply": "2023-04-30T12:01:17.367232Z"
    },
    "papermill": {
     "duration": 2.705598,
     "end_time": "2023-04-30T12:01:17.371005",
     "exception": false,
     "start_time": "2023-04-30T12:01:14.665407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device in training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gpudevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {gpudevice} device in training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c12c830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:17.398737Z",
     "iopub.status.busy": "2023-04-30T12:01:17.397463Z",
     "iopub.status.idle": "2023-04-30T12:01:20.568916Z",
     "shell.execute_reply": "2023-04-30T12:01:20.567926Z"
    },
    "papermill": {
     "duration": 3.187941,
     "end_time": "2023-04-30T12:01:20.571271",
     "exception": false,
     "start_time": "2023-04-30T12:01:17.383330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.2419e-01, -2.8883e-01,  1.3854e-01,  ...,  1.9312e-01,\n",
       "          -7.7683e-02, -1.4481e-01],\n",
       "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 2.6790e-02, -6.0759e-02,  1.1329e-02,  ..., -4.7001e-01,\n",
       "          -5.7732e-03, -2.3301e-01],\n",
       "         [-3.1826e-02, -6.3782e-01, -4.0954e-05,  ..., -1.4163e-01,\n",
       "          -2.7359e-01,  1.5005e-01],\n",
       "         [-4.8489e-01,  2.4163e-01, -5.0128e-01,  ..., -7.2599e-01,\n",
       "           2.1318e-02,  3.4059e-01]], device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([9225, 300]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.array(list(dic.values()))\n",
    "emb = torch.as_tensor(emb).to(gpudevice)\n",
    "emb, emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b177ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T11:59:10.060734Z",
     "iopub.status.busy": "2023-04-30T11:59:10.060024Z",
     "iopub.status.idle": "2023-04-30T11:59:10.067637Z",
     "shell.execute_reply": "2023-04-30T11:59:10.066422Z",
     "shell.execute_reply.started": "2023-04-30T11:59:10.060693Z"
    },
    "papermill": {
     "duration": 0.012452,
     "end_time": "2023-04-30T12:01:20.596017",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.583565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def create_emb_layer(weights_matrix, trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if trainable:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "    else :\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af47d2",
   "metadata": {
    "papermill": {
     "duration": 0.012208,
     "end_time": "2023-04-30T12:01:20.630604",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.618396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf648b6",
   "metadata": {
    "papermill": {
     "duration": 0.012546,
     "end_time": "2023-04-30T12:01:20.655760",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.643214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_hy = nn.Parameter(torch.randn(hidden_size, output_size))\n",
    "        self.bias_h = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.bias_y = nn.Parameter(torch.randn(output_size))\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_size, device=x.device)\n",
    "        h = h.unsqueeze(1).expand(-1, x.size(1), -1)  # h.shape -> (256,20,hidden_size)\n",
    "        h = torch.tanh(torch.matmul(x, self.W_xh) + torch.matmul(h, self.W_hh) + self.bias_h)\n",
    "        y = torch.matmul(h, self.W_hy) + self.bias_y\n",
    "        y = nn.Softmax(dim=1)(y)\n",
    "        return y, h\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"VanillaRNN(input_size={self.W_xh.shape[0]}, hidden_size={self.hidden_size}, output_size={self.W_hy.shape[1]})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "969a171b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:20.681712Z",
     "iopub.status.busy": "2023-04-30T12:01:20.681362Z",
     "iopub.status.idle": "2023-04-30T12:01:20.691380Z",
     "shell.execute_reply": "2023-04-30T12:01:20.690445Z"
    },
    "papermill": {
     "duration": 0.025425,
     "end_time": "2023-04-30T12:01:20.693541",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.668116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BatchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))   # (512, 512)\n",
    "        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size))    # (300, 512)\n",
    "        self.W_hy = nn.Parameter(torch.randn(hidden_size, output_size))   # (512, 6)\n",
    "        self.bias_h = nn.Parameter(torch.randn(hidden_size))              # (512)\n",
    "        #self.bias_y = nn.Parameter(torch.randn(output_size))              # (6)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        batch_size, seq_len, input_size = x.size()  # batch_size를 추출합니다.   (256, 20, 300)\n",
    "        #print(batch_size, seq_len, input_size)\n",
    "        if h is None:\n",
    "            #print(\" h is None \")\n",
    "            h = torch.zeros(batch_size, seq_len ,self.hidden_size, device=x.device)\n",
    "        #print(\"before h :\", h.shape)    \n",
    "        #h = h.unsqueeze(1).expand(batch_size, seq_len, self.hidden_size)  # h.shape -> (batch_size, seq_len, hidden_size) 이거 없애고 위에 zeros에 seq_len 더해줬음\n",
    "        #print(\"h.unsqueeze(1).expand(-1, seq_len, self.hidden_size) : \", h.shape)\n",
    "        h = torch.tanh(torch.matmul(x, self.W_xh) + torch.matmul(h, self.W_hh) + self.bias_h)  # (256, 20, 300) * (300,512) = (20,512) // (256, 20, 512) * (512, 512)= (20,512)\n",
    "        y = torch.matmul(h, self.W_hy)# + self.bias_y # (20,512) * (512,6) = (20,6) // + (6)\n",
    "        #print(\"matmul : \", y.shape)\n",
    "        #y = nn.Softmax(dim=2)(y)  # dim=2로 설정하여 시간 축 (seq_len)에 대해 소프트맥스를 적용합니다.\n",
    "        #print(\"softmax : \", y.shape)\n",
    "        return y, h\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BatchRNN(input_size={self.W_xh.shape[0]}, hidden_size={self.hidden_size}, output_size={self.W_hy.shape[1]})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "461ae189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:20.719682Z",
     "iopub.status.busy": "2023-04-30T12:01:20.718642Z",
     "iopub.status.idle": "2023-04-30T12:01:20.729156Z",
     "shell.execute_reply": "2023-04-30T12:01:20.728214Z"
    },
    "papermill": {
     "duration": 0.025561,
     "end_time": "2023-04-30T12:01:20.731060",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.705499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 첫 번째 BatchRNN 레이어\n",
    "        self.batch_rnn1 = BatchRNN(input_size, hidden_size, hidden_size)   # (300, 128, 128) (input_size, hidden_size, hidden_size)\n",
    "        \n",
    "        # 중간에 쌓을 BatchRNN 레이어들\n",
    "        self.batch_rnns = nn.ModuleList([BatchRNN(hidden_size, hidden_size, 256) for _ in range(num_layers-2)]) # (128, 128, 128)   # (hidden_size,hidden_size,hidden_size)\n",
    "        \n",
    "        # 마지막 BatchRNN 레이어\n",
    "        self.batch_rnn2 = BatchRNN(256, hidden_size, output_size)  # org = (hidden_size, hidden_size, output_size) (128, 128, 512)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)  # hidden_size\n",
    "        self.layer_norm2 = nn.LayerNorm(256)  # hidden_size\n",
    "        self.layer_norm3 = nn.LayerNorm(output_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        \n",
    "        # 첫 번째 BatchRNN 레이어를 통과한 결과를 입력으로 사용합니다.\n",
    "        y, h = self.batch_rnn1(x, h)\n",
    "        #print(\"y_rnn1 : \", y.shape)     # (256,20,128)\n",
    "        y = self.layer_norm1(y)\n",
    "        \n",
    "        # 중간에 쌓은 BatchRNN 레이어를 차례대로 통과시킵니다.\n",
    "        for batch_rnn in self.batch_rnns:\n",
    "            y, h = batch_rnn(y, h)\n",
    "            #print(\"y_rnn2 : \", y.shape)\n",
    "            y = self.layer_norm2(y)\n",
    "        \n",
    "        # 마지막 BatchRNN 레이어를 통과시킨 결과를 반환합니다.\n",
    "        y, h = self.batch_rnn2(y, h)\n",
    "        #print(\"y : \", y.shape)\n",
    "        #print(\"h : \", h.shape)\n",
    "        y = self.layer_norm3(y)\n",
    "        return y, h\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DeepRNN(input_size={self.batch_rnn1.W_xh.shape[0]}, hidden_size={self.hidden_size}, output_size={self.batch_rnn2.W_hy.shape[1]}, num_layers={self.num_layers})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a5092c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:20.757658Z",
     "iopub.status.busy": "2023-04-30T12:01:20.757386Z",
     "iopub.status.idle": "2023-04-30T12:01:20.819315Z",
     "shell.execute_reply": "2023-04-30T12:01:20.818369Z"
    },
    "papermill": {
     "duration": 0.077635,
     "end_time": "2023-04-30T12:01:20.821418",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.743783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textclassifier(\n",
      "  (rnn): DeepRNN(input_size=300, hidden_size=512, output_size=128, num_layers=3)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Linear(in_features=2560, out_features=6, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# 데이터를 Tensor로 변환\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.long).to(gpudevice)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.long).to(gpudevice)\n",
    "\n",
    "# 데이터를 train set과 validation set으로 나눔\n",
    "n_samples = X_train.shape[0]\n",
    "val_size = int(0.2 * n_samples)  # 20%를 validation set으로 사용\n",
    "train_size = n_samples - val_size\n",
    "\n",
    "train_indices, val_indices = np.split(np.arange(n_samples), [train_size])\n",
    "\n",
    "train_data = data_utils.TensorDataset(X_tensor[train_indices], y_tensor[train_indices])\n",
    "val_data = data_utils.TensorDataset(X_tensor[val_indices], y_tensor[val_indices])\n",
    "\n",
    "trainloader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 모델 구현\n",
    "class Textclassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, pre_trained): #pre_trained : torch.Size([9225, 300])\n",
    "        super().__init__()\n",
    "        #self.emb = create_emb_layer(pre_trained, False)  \n",
    "        self.emb = pre_trained\n",
    "        self.rnn = DeepRNN(input_size, hidden_size, output_size, num_layers)\n",
    "        #self.dropout = nn.Dropout(p=0.2)\n",
    "        #self.layer_norm = nn.LayerNorm(output_size)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.fc = nn.Linear(output_size*20, 6)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = torch.tensor(self.emb[text], dtype = torch.float32)     # (batch_size,20,300)\n",
    "        \n",
    "        #print(\"embedded : \", embedded.shape)\n",
    "        \n",
    "        y,_ = self.rnn(embedded)#, h0)#, self.zero_state)\n",
    "        #y = self.dropout(y)\n",
    "        \n",
    "        #print(\"y : \", y.shape)\n",
    "        \n",
    "        #norm = self.layer_norm(y)\n",
    "        \n",
    "        flatten = self.flatten(y)\n",
    "        \n",
    "        #print(\"flatten : \", flatten)\n",
    "\n",
    "        fc_output = self.fc(flatten)\n",
    "        \n",
    "        #print(\"fc_output : \", fc_output.shape)\n",
    "        \n",
    "        softmax_output = self.softmax(fc_output)\n",
    "        \n",
    "        #print(\"softmax_output : \", softmax_output.shape)\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        \n",
    "        return softmax_output\n",
    "\n",
    "model = Textclassifier(input_size = 300, hidden_size = 512, output_size= 128, num_layers = 3, pre_trained = emb).to(gpudevice)  ## emb is embedding vector\n",
    "#print(model.state_dict())     ### 초기 가중치 확인\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3af8a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:01:20.847299Z",
     "iopub.status.busy": "2023-04-30T12:01:20.846507Z",
     "iopub.status.idle": "2023-04-30T12:10:59.564448Z",
     "shell.execute_reply": "2023-04-30T12:10:59.562923Z"
    },
    "papermill": {
     "duration": 578.733109,
     "end_time": "2023-04-30T12:10:59.566892",
     "exception": false,
     "start_time": "2023-04-30T12:01:20.833783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 27.496, accuracy: 38.700%\n",
      "[2] loss: 25.710, accuracy: 46.500%\n",
      "[3] loss: 24.608, accuracy: 47.400%\n",
      "[4] loss: 23.885, accuracy: 48.400%\n",
      "[5] loss: 23.432, accuracy: 51.700%\n",
      "[6] loss: 22.556, accuracy: 57.700%\n",
      "[7] loss: 21.694, accuracy: 59.000%\n",
      "[8] loss: 21.126, accuracy: 60.300%\n",
      "[9] loss: 20.717, accuracy: 60.200%\n",
      "[10] loss: 20.386, accuracy: 60.600%\n",
      "[11] loss: 20.156, accuracy: 61.500%\n",
      "[12] loss: 19.960, accuracy: 61.800%\n",
      "[13] loss: 19.812, accuracy: 62.100%\n",
      "[14] loss: 19.723, accuracy: 62.000%\n",
      "[15] loss: 19.668, accuracy: 62.900%\n",
      "[16] loss: 19.620, accuracy: 62.700%\n",
      "[17] loss: 19.578, accuracy: 63.500%\n",
      "[18] loss: 19.529, accuracy: 63.400%\n",
      "[19] loss: 19.104, accuracy: 63.900%\n",
      "[20] loss: 18.561, accuracy: 67.400%\n",
      "[21] loss: 18.104, accuracy: 68.000%\n",
      "[22] loss: 17.705, accuracy: 69.000%\n",
      "[23] loss: 17.423, accuracy: 69.800%\n",
      "[24] loss: 17.309, accuracy: 70.000%\n",
      "[25] loss: 17.217, accuracy: 70.900%\n",
      "[26] loss: 17.169, accuracy: 71.200%\n",
      "[27] loss: 17.142, accuracy: 71.000%\n",
      "[28] loss: 17.138, accuracy: 71.700%\n",
      "[29] loss: 17.123, accuracy: 71.100%\n",
      "[30] loss: 17.118, accuracy: 71.700%\n",
      "[31] loss: 17.113, accuracy: 71.300%\n",
      "[32] loss: 17.103, accuracy: 71.600%\n",
      "[33] loss: 17.101, accuracy: 71.200%\n",
      "[34] loss: 17.101, accuracy: 71.800%\n",
      "[35] loss: 17.097, accuracy: 70.500%\n",
      "[36] loss: 17.082, accuracy: 70.700%\n",
      "[37] loss: 17.083, accuracy: 71.500%\n",
      "[38] loss: 17.084, accuracy: 71.700%\n",
      "[39] loss: 17.081, accuracy: 71.500%\n",
      "[40] loss: 17.067, accuracy: 72.100%\n",
      "[41] loss: 17.066, accuracy: 71.600%\n",
      "[42] loss: 17.056, accuracy: 71.500%\n",
      "[43] loss: 17.064, accuracy: 71.600%\n",
      "[44] loss: 17.055, accuracy: 71.300%\n",
      "[45] loss: 17.055, accuracy: 72.300%\n",
      "[46] loss: 17.053, accuracy: 71.500%\n",
      "[47] loss: 17.052, accuracy: 71.600%\n",
      "[48] loss: 17.043, accuracy: 71.700%\n",
      "[49] loss: 17.041, accuracy: 71.900%\n",
      "[50] loss: 17.025, accuracy: 71.400%\n",
      "[51] loss: 17.030, accuracy: 70.200%\n",
      "[52] loss: 17.019, accuracy: 71.600%\n",
      "[53] loss: 17.024, accuracy: 70.700%\n",
      "[54] loss: 17.027, accuracy: 71.600%\n",
      "[55] loss: 17.029, accuracy: 71.200%\n",
      "[56] loss: 17.024, accuracy: 71.600%\n",
      "[57] loss: 17.028, accuracy: 71.700%\n",
      "[58] loss: 17.056, accuracy: 71.500%\n",
      "[59] loss: 17.077, accuracy: 70.900%\n",
      "[60] loss: 17.116, accuracy: 69.900%\n",
      "[61] loss: 17.312, accuracy: 68.600%\n",
      "[62] loss: 18.280, accuracy: 65.900%\n",
      "[63] loss: 18.294, accuracy: 70.000%\n",
      "[64] loss: 17.832, accuracy: 71.900%\n",
      "[65] loss: 17.414, accuracy: 73.000%\n",
      "[66] loss: 17.213, accuracy: 73.000%\n",
      "[67] loss: 17.097, accuracy: 74.000%\n",
      "[68] loss: 17.046, accuracy: 75.300%\n",
      "[69] loss: 17.020, accuracy: 75.400%\n",
      "[70] loss: 17.010, accuracy: 75.300%\n",
      "[71] loss: 16.996, accuracy: 75.300%\n",
      "[72] loss: 16.991, accuracy: 74.800%\n",
      "[73] loss: 16.997, accuracy: 74.300%\n",
      "[74] loss: 16.987, accuracy: 74.800%\n",
      "[75] loss: 16.991, accuracy: 74.900%\n",
      "[76] loss: 16.994, accuracy: 74.800%\n",
      "[77] loss: 16.987, accuracy: 75.000%\n",
      "[78] loss: 16.988, accuracy: 75.000%\n",
      "[79] loss: 16.982, accuracy: 74.200%\n",
      "[80] loss: 16.983, accuracy: 74.500%\n",
      "[81] loss: 16.985, accuracy: 74.600%\n",
      "[82] loss: 16.983, accuracy: 74.300%\n",
      "[83] loss: 16.978, accuracy: 74.900%\n",
      "[84] loss: 16.981, accuracy: 74.100%\n",
      "[85] loss: 16.983, accuracy: 74.200%\n",
      "[86] loss: 16.993, accuracy: 74.400%\n",
      "[87] loss: 16.989, accuracy: 74.000%\n",
      "[88] loss: 16.991, accuracy: 74.300%\n",
      "[89] loss: 16.986, accuracy: 74.200%\n",
      "[90] loss: 16.984, accuracy: 73.800%\n",
      "[91] loss: 16.982, accuracy: 73.900%\n",
      "[92] loss: 16.984, accuracy: 74.200%\n",
      "[93] loss: 16.986, accuracy: 73.900%\n",
      "[94] loss: 16.977, accuracy: 73.800%\n",
      "[95] loss: 16.980, accuracy: 73.900%\n",
      "[96] loss: 16.975, accuracy: 74.100%\n",
      "[97] loss: 16.915, accuracy: 74.100%\n",
      "[98] loss: 16.905, accuracy: 73.900%\n",
      "[99] loss: 16.893, accuracy: 74.200%\n",
      "[100] loss: 16.882, accuracy: 74.400%\n",
      "[101] loss: 16.881, accuracy: 74.700%\n",
      "[102] loss: 16.854, accuracy: 75.900%\n",
      "[103] loss: 16.830, accuracy: 75.400%\n",
      "[104] loss: 16.818, accuracy: 74.900%\n",
      "[105] loss: 16.815, accuracy: 75.000%\n",
      "[106] loss: 16.811, accuracy: 75.100%\n",
      "[107] loss: 16.809, accuracy: 75.800%\n",
      "[108] loss: 16.806, accuracy: 75.800%\n",
      "[109] loss: 16.808, accuracy: 75.400%\n",
      "[110] loss: 16.806, accuracy: 75.800%\n",
      "[111] loss: 16.806, accuracy: 75.800%\n",
      "[112] loss: 16.804, accuracy: 75.600%\n",
      "[113] loss: 16.810, accuracy: 76.000%\n",
      "[114] loss: 16.810, accuracy: 76.100%\n",
      "[115] loss: 16.812, accuracy: 75.800%\n",
      "[116] loss: 16.805, accuracy: 75.700%\n",
      "[117] loss: 16.808, accuracy: 76.400%\n",
      "[118] loss: 16.812, accuracy: 75.300%\n",
      "[119] loss: 16.858, accuracy: 76.800%\n",
      "[120] loss: 17.648, accuracy: 69.900%\n",
      "[121] loss: 19.287, accuracy: 68.300%\n",
      "[122] loss: 18.879, accuracy: 73.400%\n",
      "[123] loss: 18.201, accuracy: 73.300%\n",
      "[124] loss: 17.876, accuracy: 72.600%\n",
      "[125] loss: 17.490, accuracy: 73.600%\n",
      "[126] loss: 17.275, accuracy: 73.200%\n",
      "[127] loss: 17.131, accuracy: 74.700%\n",
      "[128] loss: 17.038, accuracy: 75.400%\n",
      "[129] loss: 16.954, accuracy: 76.200%\n",
      "[130] loss: 16.937, accuracy: 76.100%\n",
      "[131] loss: 16.907, accuracy: 75.600%\n",
      "[132] loss: 16.898, accuracy: 75.300%\n",
      "[133] loss: 16.871, accuracy: 75.900%\n",
      "[134] loss: 16.864, accuracy: 76.700%\n",
      "[135] loss: 16.859, accuracy: 76.100%\n",
      "[136] loss: 16.857, accuracy: 75.900%\n",
      "[137] loss: 16.846, accuracy: 75.900%\n",
      "[138] loss: 16.843, accuracy: 75.800%\n",
      "[139] loss: 16.840, accuracy: 76.800%\n",
      "[140] loss: 16.836, accuracy: 76.100%\n",
      "[141] loss: 16.842, accuracy: 76.600%\n",
      "[142] loss: 16.830, accuracy: 76.800%\n",
      "[143] loss: 16.834, accuracy: 76.600%\n",
      "[144] loss: 16.831, accuracy: 77.100%\n",
      "[145] loss: 16.827, accuracy: 76.700%\n",
      "[146] loss: 16.827, accuracy: 76.600%\n",
      "[147] loss: 16.830, accuracy: 77.100%\n",
      "[148] loss: 16.826, accuracy: 76.900%\n",
      "[149] loss: 16.823, accuracy: 77.300%\n",
      "[150] loss: 16.825, accuracy: 77.200%\n",
      "[151] loss: 16.825, accuracy: 76.700%\n",
      "[152] loss: 16.823, accuracy: 76.600%\n",
      "[153] loss: 16.826, accuracy: 76.700%\n",
      "[154] loss: 16.821, accuracy: 77.200%\n",
      "[155] loss: 16.821, accuracy: 77.100%\n",
      "[156] loss: 16.824, accuracy: 77.200%\n",
      "[157] loss: 16.826, accuracy: 77.800%\n",
      "[158] loss: 16.827, accuracy: 77.400%\n",
      "[159] loss: 16.824, accuracy: 77.300%\n",
      "[160] loss: 16.825, accuracy: 77.800%\n",
      "[161] loss: 16.827, accuracy: 77.400%\n",
      "[162] loss: 16.822, accuracy: 77.100%\n",
      "[163] loss: 16.823, accuracy: 77.000%\n",
      "[164] loss: 16.823, accuracy: 76.900%\n",
      "[165] loss: 16.819, accuracy: 77.200%\n",
      "[166] loss: 16.823, accuracy: 77.200%\n",
      "[167] loss: 16.823, accuracy: 77.300%\n",
      "[168] loss: 16.819, accuracy: 77.100%\n",
      "[169] loss: 16.821, accuracy: 77.200%\n",
      "[170] loss: 16.819, accuracy: 77.500%\n",
      "[171] loss: 16.821, accuracy: 77.500%\n",
      "[172] loss: 16.821, accuracy: 77.500%\n",
      "[173] loss: 16.821, accuracy: 77.700%\n",
      "[174] loss: 16.821, accuracy: 77.300%\n",
      "[175] loss: 16.819, accuracy: 77.900%\n",
      "[176] loss: 16.819, accuracy: 78.000%\n",
      "[177] loss: 16.824, accuracy: 77.300%\n",
      "[178] loss: 16.824, accuracy: 77.600%\n",
      "[179] loss: 16.828, accuracy: 77.000%\n",
      "[180] loss: 16.824, accuracy: 77.900%\n",
      "[181] loss: 16.826, accuracy: 76.200%\n",
      "[182] loss: 24.749, accuracy: 41.700%\n",
      "[183] loss: 25.130, accuracy: 46.300%\n",
      "[184] loss: 24.367, accuracy: 47.400%\n",
      "[185] loss: 23.753, accuracy: 51.900%\n",
      "[186] loss: 23.214, accuracy: 53.000%\n",
      "[187] loss: 23.012, accuracy: 52.300%\n",
      "[188] loss: 22.778, accuracy: 53.700%\n",
      "[189] loss: 22.613, accuracy: 52.800%\n",
      "[190] loss: 22.428, accuracy: 53.200%\n",
      "[191] loss: 22.338, accuracy: 53.500%\n",
      "[192] loss: 22.213, accuracy: 52.600%\n",
      "[193] loss: 22.190, accuracy: 52.600%\n",
      "[194] loss: 22.218, accuracy: 52.000%\n",
      "[195] loss: 22.146, accuracy: 54.300%\n",
      "[196] loss: 22.090, accuracy: 54.500%\n",
      "[197] loss: 22.057, accuracy: 53.500%\n",
      "[198] loss: 22.045, accuracy: 53.600%\n",
      "[199] loss: 22.012, accuracy: 53.900%\n",
      "[200] loss: 22.011, accuracy: 53.800%\n",
      "[201] loss: 21.989, accuracy: 53.200%\n",
      "[202] loss: 21.991, accuracy: 53.900%\n",
      "[203] loss: 21.965, accuracy: 54.600%\n",
      "[204] loss: 21.935, accuracy: 53.300%\n",
      "[205] loss: 21.937, accuracy: 54.200%\n",
      "[206] loss: 21.954, accuracy: 54.000%\n",
      "[207] loss: 21.937, accuracy: 53.600%\n",
      "[208] loss: 21.931, accuracy: 54.000%\n",
      "[209] loss: 21.929, accuracy: 54.200%\n",
      "[210] loss: 21.966, accuracy: 54.400%\n",
      "[211] loss: 21.903, accuracy: 53.800%\n",
      "[212] loss: 21.899, accuracy: 53.900%\n",
      "[213] loss: 21.886, accuracy: 53.900%\n",
      "[214] loss: 21.905, accuracy: 54.200%\n",
      "[215] loss: 21.893, accuracy: 54.500%\n",
      "[216] loss: 21.899, accuracy: 53.800%\n",
      "[217] loss: 21.908, accuracy: 53.900%\n",
      "[218] loss: 21.882, accuracy: 54.100%\n",
      "[219] loss: 21.889, accuracy: 54.200%\n",
      "[220] loss: 21.861, accuracy: 54.100%\n",
      "[221] loss: 21.889, accuracy: 54.100%\n",
      "[222] loss: 21.861, accuracy: 55.100%\n",
      "[223] loss: 21.892, accuracy: 54.300%\n",
      "[224] loss: 21.875, accuracy: 54.500%\n",
      "[225] loss: 21.876, accuracy: 54.300%\n",
      "[226] loss: 21.906, accuracy: 54.000%\n",
      "[227] loss: 21.897, accuracy: 54.000%\n",
      "[228] loss: 21.890, accuracy: 54.500%\n",
      "[229] loss: 21.899, accuracy: 53.300%\n",
      "[230] loss: 21.866, accuracy: 53.400%\n",
      "[231] loss: 21.884, accuracy: 53.000%\n",
      "[232] loss: 21.896, accuracy: 53.300%\n",
      "[233] loss: 21.896, accuracy: 54.400%\n",
      "[234] loss: 21.916, accuracy: 54.100%\n",
      "[235] loss: 21.914, accuracy: 53.200%\n",
      "[236] loss: 21.877, accuracy: 53.100%\n",
      "[237] loss: 21.884, accuracy: 54.400%\n",
      "[238] loss: 21.881, accuracy: 53.500%\n",
      "[239] loss: 21.893, accuracy: 54.000%\n",
      "[240] loss: 21.908, accuracy: 53.400%\n",
      "[241] loss: 22.023, accuracy: 52.500%\n",
      "[242] loss: 23.200, accuracy: 46.400%\n",
      "[243] loss: 23.473, accuracy: 50.600%\n",
      "[244] loss: 23.118, accuracy: 52.300%\n",
      "[245] loss: 23.000, accuracy: 51.400%\n",
      "[246] loss: 22.598, accuracy: 53.300%\n",
      "[247] loss: 22.328, accuracy: 53.700%\n",
      "[248] loss: 22.184, accuracy: 53.800%\n",
      "[249] loss: 22.094, accuracy: 55.200%\n",
      "[250] loss: 22.071, accuracy: 54.900%\n",
      "[251] loss: 22.015, accuracy: 54.500%\n",
      "[252] loss: 21.974, accuracy: 55.800%\n",
      "[253] loss: 21.925, accuracy: 55.300%\n",
      "[254] loss: 21.925, accuracy: 54.900%\n",
      "[255] loss: 21.901, accuracy: 55.300%\n",
      "[256] loss: 21.891, accuracy: 55.700%\n",
      "[257] loss: 21.899, accuracy: 55.500%\n",
      "[258] loss: 21.894, accuracy: 55.900%\n",
      "[259] loss: 21.850, accuracy: 55.700%\n",
      "[260] loss: 21.868, accuracy: 55.200%\n",
      "[261] loss: 21.869, accuracy: 55.200%\n",
      "[262] loss: 21.844, accuracy: 55.300%\n",
      "[263] loss: 21.841, accuracy: 55.700%\n",
      "[264] loss: 21.857, accuracy: 55.600%\n",
      "[265] loss: 21.874, accuracy: 55.500%\n",
      "[266] loss: 21.862, accuracy: 55.800%\n",
      "[267] loss: 21.841, accuracy: 55.300%\n",
      "[268] loss: 21.840, accuracy: 55.400%\n",
      "[269] loss: 21.851, accuracy: 55.400%\n",
      "[270] loss: 21.851, accuracy: 54.800%\n",
      "[271] loss: 21.886, accuracy: 56.200%\n",
      "[272] loss: 21.837, accuracy: 54.900%\n",
      "[273] loss: 21.845, accuracy: 56.100%\n",
      "[274] loss: 21.840, accuracy: 55.500%\n",
      "[275] loss: 21.845, accuracy: 54.700%\n",
      "[276] loss: 21.856, accuracy: 55.600%\n",
      "[277] loss: 21.850, accuracy: 54.600%\n",
      "[278] loss: 21.880, accuracy: 54.500%\n",
      "[279] loss: 21.884, accuracy: 54.500%\n",
      "[280] loss: 21.872, accuracy: 54.500%\n",
      "[281] loss: 21.869, accuracy: 54.700%\n",
      "[282] loss: 21.895, accuracy: 53.600%\n",
      "[283] loss: 21.878, accuracy: 55.100%\n",
      "[284] loss: 21.997, accuracy: 49.200%\n",
      "[285] loss: 23.052, accuracy: 44.300%\n",
      "[286] loss: 23.526, accuracy: 52.500%\n",
      "[287] loss: 23.292, accuracy: 51.600%\n",
      "[288] loss: 22.666, accuracy: 51.500%\n",
      "[289] loss: 22.558, accuracy: 53.800%\n",
      "[290] loss: 22.477, accuracy: 51.400%\n",
      "[291] loss: 22.287, accuracy: 52.800%\n",
      "[292] loss: 22.162, accuracy: 52.700%\n",
      "[293] loss: 22.092, accuracy: 53.100%\n",
      "[294] loss: 22.028, accuracy: 52.500%\n",
      "[295] loss: 21.986, accuracy: 53.300%\n",
      "[296] loss: 21.959, accuracy: 53.400%\n",
      "[297] loss: 21.938, accuracy: 54.000%\n",
      "[298] loss: 21.939, accuracy: 53.600%\n",
      "[299] loss: 21.892, accuracy: 54.100%\n",
      "[300] loss: 21.919, accuracy: 54.300%\n",
      "[301] loss: 21.888, accuracy: 53.600%\n",
      "[302] loss: 21.872, accuracy: 54.300%\n",
      "[303] loss: 21.852, accuracy: 53.900%\n",
      "[304] loss: 21.863, accuracy: 53.800%\n",
      "[305] loss: 21.850, accuracy: 53.900%\n",
      "[306] loss: 21.856, accuracy: 53.700%\n",
      "[307] loss: 21.882, accuracy: 54.100%\n",
      "[308] loss: 21.868, accuracy: 54.000%\n",
      "[309] loss: 21.873, accuracy: 54.100%\n",
      "[310] loss: 21.872, accuracy: 53.900%\n",
      "[311] loss: 21.855, accuracy: 54.000%\n",
      "[312] loss: 21.864, accuracy: 54.400%\n",
      "[313] loss: 21.837, accuracy: 53.800%\n",
      "[314] loss: 21.850, accuracy: 53.700%\n",
      "[315] loss: 21.880, accuracy: 54.400%\n",
      "[316] loss: 21.857, accuracy: 54.300%\n",
      "[317] loss: 21.882, accuracy: 53.900%\n",
      "[318] loss: 21.873, accuracy: 54.700%\n",
      "[319] loss: 21.873, accuracy: 54.200%\n",
      "[320] loss: 21.865, accuracy: 54.200%\n",
      "[321] loss: 21.864, accuracy: 54.500%\n",
      "[322] loss: 21.841, accuracy: 54.200%\n",
      "[323] loss: 21.855, accuracy: 53.600%\n",
      "[324] loss: 21.873, accuracy: 54.500%\n",
      "[325] loss: 21.846, accuracy: 53.400%\n",
      "[326] loss: 21.878, accuracy: 54.500%\n",
      "[327] loss: 21.909, accuracy: 54.000%\n",
      "[328] loss: 22.069, accuracy: 49.500%\n",
      "[329] loss: 23.559, accuracy: 52.300%\n",
      "[330] loss: 23.537, accuracy: 52.900%\n",
      "[331] loss: 22.877, accuracy: 53.400%\n",
      "[332] loss: 22.642, accuracy: 53.000%\n",
      "[333] loss: 22.431, accuracy: 53.900%\n",
      "[334] loss: 22.274, accuracy: 53.300%\n",
      "[335] loss: 22.154, accuracy: 54.900%\n",
      "[336] loss: 22.067, accuracy: 53.600%\n",
      "[337] loss: 22.046, accuracy: 54.200%\n",
      "[338] loss: 22.016, accuracy: 54.100%\n",
      "[339] loss: 21.972, accuracy: 53.900%\n",
      "[340] loss: 21.934, accuracy: 54.700%\n",
      "[341] loss: 21.902, accuracy: 54.200%\n",
      "[342] loss: 21.918, accuracy: 54.900%\n",
      "[343] loss: 21.891, accuracy: 54.300%\n",
      "[344] loss: 21.903, accuracy: 54.800%\n",
      "[345] loss: 21.882, accuracy: 54.100%\n",
      "[346] loss: 21.877, accuracy: 54.700%\n",
      "[347] loss: 21.892, accuracy: 54.600%\n",
      "[348] loss: 21.878, accuracy: 55.100%\n",
      "[349] loss: 21.877, accuracy: 55.300%\n",
      "[350] loss: 21.877, accuracy: 55.000%\n",
      "[351] loss: 21.877, accuracy: 54.100%\n",
      "[352] loss: 21.883, accuracy: 54.300%\n",
      "[353] loss: 21.854, accuracy: 54.000%\n",
      "[354] loss: 21.872, accuracy: 55.200%\n",
      "[355] loss: 21.880, accuracy: 55.000%\n",
      "[356] loss: 21.857, accuracy: 55.000%\n",
      "[357] loss: 21.856, accuracy: 54.300%\n",
      "[358] loss: 21.845, accuracy: 54.200%\n",
      "[359] loss: 21.853, accuracy: 55.200%\n",
      "[360] loss: 21.852, accuracy: 54.800%\n",
      "[361] loss: 21.869, accuracy: 54.300%\n",
      "[362] loss: 21.856, accuracy: 54.900%\n",
      "[363] loss: 21.877, accuracy: 55.500%\n",
      "[364] loss: 21.864, accuracy: 54.500%\n",
      "[365] loss: 21.890, accuracy: 52.900%\n",
      "[366] loss: 22.021, accuracy: 53.100%\n",
      "[367] loss: 22.262, accuracy: 52.100%\n",
      "[368] loss: 22.637, accuracy: 52.600%\n",
      "[369] loss: 22.670, accuracy: 50.700%\n",
      "[370] loss: 22.564, accuracy: 53.900%\n",
      "[371] loss: 22.341, accuracy: 52.500%\n",
      "[372] loss: 22.246, accuracy: 54.100%\n",
      "[373] loss: 22.112, accuracy: 53.900%\n",
      "[374] loss: 22.031, accuracy: 54.700%\n",
      "[375] loss: 21.980, accuracy: 54.200%\n",
      "[376] loss: 21.921, accuracy: 54.800%\n",
      "[377] loss: 21.908, accuracy: 54.300%\n",
      "[378] loss: 21.895, accuracy: 55.100%\n",
      "[379] loss: 21.878, accuracy: 54.400%\n",
      "[380] loss: 21.878, accuracy: 54.600%\n",
      "[381] loss: 21.861, accuracy: 54.200%\n",
      "[382] loss: 21.880, accuracy: 54.400%\n",
      "[383] loss: 21.861, accuracy: 54.700%\n",
      "[384] loss: 21.854, accuracy: 54.600%\n",
      "[385] loss: 21.853, accuracy: 54.600%\n",
      "[386] loss: 21.862, accuracy: 54.600%\n",
      "[387] loss: 21.861, accuracy: 55.000%\n",
      "[388] loss: 21.837, accuracy: 54.100%\n",
      "[389] loss: 21.846, accuracy: 54.800%\n",
      "[390] loss: 21.848, accuracy: 54.600%\n",
      "[391] loss: 21.837, accuracy: 54.700%\n",
      "[392] loss: 21.848, accuracy: 55.200%\n",
      "[393] loss: 21.854, accuracy: 54.100%\n",
      "[394] loss: 21.837, accuracy: 54.500%\n",
      "[395] loss: 21.826, accuracy: 54.200%\n",
      "[396] loss: 21.824, accuracy: 54.500%\n",
      "[397] loss: 21.852, accuracy: 54.100%\n",
      "[398] loss: 21.848, accuracy: 54.600%\n",
      "[399] loss: 21.837, accuracy: 54.400%\n",
      "[400] loss: 21.850, accuracy: 54.000%\n",
      "[401] loss: 21.872, accuracy: 54.300%\n",
      "[402] loss: 21.859, accuracy: 54.200%\n",
      "[403] loss: 21.868, accuracy: 54.900%\n",
      "[404] loss: 21.929, accuracy: 52.100%\n",
      "[405] loss: 22.045, accuracy: 53.300%\n",
      "[406] loss: 22.733, accuracy: 47.700%\n",
      "[407] loss: 23.417, accuracy: 51.400%\n",
      "[408] loss: 22.788, accuracy: 53.500%\n",
      "[409] loss: 22.410, accuracy: 54.300%\n",
      "[410] loss: 22.143, accuracy: 54.300%\n",
      "[411] loss: 22.089, accuracy: 53.500%\n",
      "[412] loss: 22.013, accuracy: 54.500%\n",
      "[413] loss: 21.970, accuracy: 54.600%\n",
      "[414] loss: 21.933, accuracy: 54.300%\n",
      "[415] loss: 21.897, accuracy: 54.400%\n",
      "[416] loss: 21.906, accuracy: 54.300%\n",
      "[417] loss: 21.888, accuracy: 54.500%\n",
      "[418] loss: 21.894, accuracy: 54.400%\n",
      "[419] loss: 21.861, accuracy: 54.200%\n",
      "[420] loss: 21.856, accuracy: 54.100%\n",
      "[421] loss: 21.851, accuracy: 54.500%\n",
      "[422] loss: 21.827, accuracy: 54.200%\n",
      "[423] loss: 21.816, accuracy: 54.700%\n",
      "[424] loss: 21.838, accuracy: 53.900%\n",
      "[425] loss: 21.842, accuracy: 54.400%\n",
      "[426] loss: 21.846, accuracy: 54.500%\n",
      "[427] loss: 21.838, accuracy: 54.100%\n",
      "[428] loss: 21.831, accuracy: 54.200%\n",
      "[429] loss: 21.846, accuracy: 54.300%\n",
      "[430] loss: 21.827, accuracy: 54.800%\n",
      "[431] loss: 21.831, accuracy: 54.500%\n",
      "[432] loss: 21.820, accuracy: 54.500%\n",
      "[433] loss: 21.841, accuracy: 53.900%\n",
      "[434] loss: 21.821, accuracy: 54.000%\n",
      "[435] loss: 21.857, accuracy: 53.600%\n",
      "[436] loss: 21.839, accuracy: 54.600%\n",
      "[437] loss: 21.868, accuracy: 54.400%\n",
      "[438] loss: 21.819, accuracy: 54.400%\n",
      "[439] loss: 21.835, accuracy: 53.700%\n",
      "[440] loss: 21.850, accuracy: 54.200%\n",
      "[441] loss: 21.828, accuracy: 55.000%\n",
      "[442] loss: 21.840, accuracy: 54.600%\n",
      "[443] loss: 21.862, accuracy: 54.200%\n",
      "[444] loss: 21.842, accuracy: 54.100%\n",
      "[445] loss: 21.865, accuracy: 53.700%\n",
      "[446] loss: 21.861, accuracy: 53.900%\n",
      "[447] loss: 21.863, accuracy: 53.800%\n",
      "[448] loss: 21.853, accuracy: 54.400%\n",
      "[449] loss: 21.881, accuracy: 53.700%\n",
      "[450] loss: 21.960, accuracy: 54.100%\n",
      "[451] loss: 22.483, accuracy: 50.300%\n",
      "[452] loss: 23.129, accuracy: 52.700%\n",
      "[453] loss: 22.950, accuracy: 52.600%\n",
      "[454] loss: 22.590, accuracy: 52.900%\n",
      "[455] loss: 22.293, accuracy: 55.000%\n",
      "[456] loss: 22.207, accuracy: 54.400%\n",
      "[457] loss: 22.095, accuracy: 53.900%\n",
      "[458] loss: 22.018, accuracy: 55.600%\n",
      "[459] loss: 21.975, accuracy: 54.200%\n",
      "[460] loss: 21.929, accuracy: 54.700%\n",
      "[461] loss: 21.897, accuracy: 56.000%\n",
      "[462] loss: 21.877, accuracy: 55.900%\n",
      "[463] loss: 21.872, accuracy: 55.900%\n",
      "[464] loss: 21.876, accuracy: 56.100%\n",
      "[465] loss: 21.880, accuracy: 55.500%\n",
      "[466] loss: 21.847, accuracy: 55.800%\n",
      "[467] loss: 21.850, accuracy: 55.300%\n",
      "[468] loss: 21.869, accuracy: 55.500%\n",
      "[469] loss: 21.827, accuracy: 55.600%\n",
      "[470] loss: 21.811, accuracy: 56.000%\n",
      "[471] loss: 21.819, accuracy: 55.400%\n",
      "[472] loss: 21.791, accuracy: 54.900%\n",
      "[473] loss: 21.785, accuracy: 55.500%\n",
      "[474] loss: 21.819, accuracy: 55.400%\n",
      "[475] loss: 21.820, accuracy: 55.400%\n",
      "[476] loss: 21.841, accuracy: 55.200%\n",
      "[477] loss: 21.833, accuracy: 55.500%\n",
      "[478] loss: 21.834, accuracy: 54.800%\n",
      "[479] loss: 21.833, accuracy: 54.900%\n",
      "[480] loss: 21.835, accuracy: 54.500%\n",
      "[481] loss: 21.809, accuracy: 55.400%\n",
      "[482] loss: 21.811, accuracy: 54.900%\n",
      "[483] loss: 21.835, accuracy: 54.600%\n",
      "[484] loss: 21.819, accuracy: 55.000%\n",
      "[485] loss: 21.817, accuracy: 54.800%\n",
      "[486] loss: 21.856, accuracy: 55.600%\n",
      "[487] loss: 21.843, accuracy: 54.900%\n",
      "[488] loss: 21.825, accuracy: 54.700%\n",
      "[489] loss: 21.826, accuracy: 55.500%\n",
      "[490] loss: 21.826, accuracy: 54.700%\n",
      "[491] loss: 21.835, accuracy: 54.700%\n",
      "[492] loss: 21.847, accuracy: 54.600%\n",
      "[493] loss: 21.829, accuracy: 54.100%\n",
      "[494] loss: 21.859, accuracy: 54.000%\n",
      "[495] loss: 21.845, accuracy: 54.500%\n",
      "[496] loss: 21.822, accuracy: 54.000%\n",
      "[497] loss: 21.863, accuracy: 54.300%\n",
      "[498] loss: 21.876, accuracy: 53.200%\n",
      "[499] loss: 22.096, accuracy: 51.800%\n",
      "[500] loss: 22.791, accuracy: 51.600%\n",
      "[501] loss: 22.981, accuracy: 53.700%\n",
      "[502] loss: 22.582, accuracy: 53.200%\n",
      "[503] loss: 22.318, accuracy: 53.700%\n",
      "[504] loss: 22.159, accuracy: 54.700%\n",
      "[505] loss: 22.055, accuracy: 54.500%\n",
      "[506] loss: 21.987, accuracy: 54.500%\n",
      "[507] loss: 21.940, accuracy: 54.900%\n",
      "[508] loss: 21.877, accuracy: 55.300%\n",
      "[509] loss: 21.889, accuracy: 54.900%\n",
      "[510] loss: 21.863, accuracy: 55.600%\n",
      "[511] loss: 21.848, accuracy: 55.200%\n",
      "[512] loss: 21.832, accuracy: 55.000%\n",
      "[513] loss: 21.833, accuracy: 54.900%\n",
      "[514] loss: 21.838, accuracy: 55.200%\n",
      "[515] loss: 21.822, accuracy: 55.300%\n",
      "[516] loss: 21.826, accuracy: 55.000%\n",
      "[517] loss: 21.810, accuracy: 54.700%\n",
      "[518] loss: 21.683, accuracy: 57.500%\n",
      "[519] loss: 20.946, accuracy: 61.100%\n",
      "[520] loss: 20.525, accuracy: 62.900%\n",
      "[521] loss: 20.236, accuracy: 65.000%\n",
      "[522] loss: 20.107, accuracy: 64.400%\n",
      "[523] loss: 19.895, accuracy: 65.500%\n",
      "[524] loss: 19.814, accuracy: 65.800%\n",
      "[525] loss: 19.699, accuracy: 65.800%\n",
      "[526] loss: 19.608, accuracy: 65.900%\n",
      "[527] loss: 19.550, accuracy: 66.300%\n",
      "[528] loss: 19.478, accuracy: 65.700%\n",
      "[529] loss: 19.450, accuracy: 65.800%\n",
      "[530] loss: 19.415, accuracy: 67.000%\n",
      "[531] loss: 19.411, accuracy: 66.300%\n",
      "[532] loss: 19.396, accuracy: 67.200%\n",
      "[533] loss: 19.382, accuracy: 66.000%\n",
      "[534] loss: 19.367, accuracy: 66.900%\n",
      "[535] loss: 19.369, accuracy: 67.300%\n",
      "[536] loss: 19.369, accuracy: 66.600%\n",
      "[537] loss: 19.369, accuracy: 66.700%\n",
      "[538] loss: 19.363, accuracy: 67.100%\n",
      "[539] loss: 19.380, accuracy: 66.300%\n",
      "[540] loss: 19.351, accuracy: 66.700%\n",
      "[541] loss: 19.358, accuracy: 66.000%\n",
      "[542] loss: 19.358, accuracy: 66.300%\n",
      "[543] loss: 19.348, accuracy: 66.100%\n",
      "[544] loss: 19.359, accuracy: 67.300%\n",
      "[545] loss: 19.343, accuracy: 66.500%\n",
      "[546] loss: 19.346, accuracy: 67.000%\n",
      "[547] loss: 19.357, accuracy: 66.900%\n",
      "[548] loss: 19.368, accuracy: 66.700%\n",
      "[549] loss: 19.371, accuracy: 66.300%\n",
      "[550] loss: 19.368, accuracy: 66.700%\n",
      "[551] loss: 19.349, accuracy: 66.900%\n",
      "[552] loss: 19.357, accuracy: 67.300%\n",
      "[553] loss: 19.359, accuracy: 66.500%\n",
      "[554] loss: 19.341, accuracy: 67.000%\n",
      "[555] loss: 19.351, accuracy: 66.500%\n",
      "[556] loss: 19.354, accuracy: 67.000%\n",
      "[557] loss: 19.353, accuracy: 67.600%\n",
      "[558] loss: 19.400, accuracy: 66.600%\n",
      "[559] loss: 19.505, accuracy: 64.100%\n",
      "[560] loss: 20.373, accuracy: 63.800%\n",
      "[561] loss: 20.818, accuracy: 65.400%\n",
      "[562] loss: 20.238, accuracy: 66.700%\n",
      "[563] loss: 19.898, accuracy: 67.500%\n",
      "[564] loss: 19.673, accuracy: 69.600%\n",
      "[565] loss: 19.516, accuracy: 68.800%\n",
      "[566] loss: 19.453, accuracy: 68.600%\n",
      "[567] loss: 19.413, accuracy: 68.700%\n",
      "[568] loss: 19.402, accuracy: 69.000%\n",
      "[569] loss: 19.378, accuracy: 69.400%\n",
      "[570] loss: 19.350, accuracy: 70.200%\n",
      "[571] loss: 18.265, accuracy: 70.100%\n",
      "[572] loss: 17.913, accuracy: 72.500%\n",
      "[573] loss: 17.782, accuracy: 73.100%\n",
      "[574] loss: 17.684, accuracy: 74.300%\n",
      "[575] loss: 17.458, accuracy: 73.800%\n",
      "[576] loss: 17.360, accuracy: 74.000%\n",
      "[577] loss: 17.283, accuracy: 76.600%\n",
      "[578] loss: 17.198, accuracy: 76.800%\n",
      "[579] loss: 17.109, accuracy: 77.300%\n",
      "[580] loss: 17.077, accuracy: 77.500%\n",
      "[581] loss: 17.057, accuracy: 77.600%\n",
      "[582] loss: 17.043, accuracy: 78.100%\n",
      "[583] loss: 17.034, accuracy: 77.600%\n",
      "[584] loss: 17.033, accuracy: 76.500%\n",
      "[585] loss: 17.038, accuracy: 77.100%\n",
      "[586] loss: 17.036, accuracy: 77.100%\n",
      "[587] loss: 17.034, accuracy: 77.200%\n",
      "[588] loss: 17.054, accuracy: 77.000%\n",
      "[589] loss: 17.035, accuracy: 76.700%\n",
      "[590] loss: 17.031, accuracy: 76.300%\n",
      "[591] loss: 17.026, accuracy: 76.900%\n",
      "[592] loss: 17.013, accuracy: 77.300%\n",
      "[593] loss: 17.021, accuracy: 77.100%\n",
      "[594] loss: 17.011, accuracy: 77.600%\n",
      "[595] loss: 17.022, accuracy: 77.200%\n",
      "[596] loss: 17.014, accuracy: 77.700%\n",
      "[597] loss: 17.016, accuracy: 76.700%\n",
      "[598] loss: 17.012, accuracy: 76.900%\n",
      "[599] loss: 17.011, accuracy: 76.600%\n",
      "[600] loss: 17.015, accuracy: 77.100%\n",
      "[601] loss: 17.012, accuracy: 77.000%\n",
      "[602] loss: 17.012, accuracy: 76.700%\n",
      "[603] loss: 17.007, accuracy: 77.000%\n",
      "[604] loss: 17.011, accuracy: 77.100%\n",
      "[605] loss: 17.009, accuracy: 77.200%\n",
      "[606] loss: 17.015, accuracy: 76.900%\n",
      "[607] loss: 17.002, accuracy: 77.000%\n",
      "[608] loss: 17.006, accuracy: 77.500%\n",
      "[609] loss: 17.013, accuracy: 77.400%\n",
      "[610] loss: 17.014, accuracy: 76.600%\n",
      "[611] loss: 17.013, accuracy: 77.200%\n",
      "[612] loss: 17.004, accuracy: 76.900%\n",
      "[613] loss: 17.009, accuracy: 76.600%\n",
      "[614] loss: 17.003, accuracy: 76.700%\n",
      "[615] loss: 17.009, accuracy: 77.100%\n",
      "[616] loss: 17.004, accuracy: 76.500%\n",
      "[617] loss: 17.011, accuracy: 76.800%\n",
      "[618] loss: 17.007, accuracy: 76.100%\n",
      "[619] loss: 17.012, accuracy: 76.700%\n",
      "[620] loss: 17.009, accuracy: 75.900%\n",
      "[621] loss: 17.004, accuracy: 75.900%\n",
      "[622] loss: 17.012, accuracy: 76.500%\n",
      "[623] loss: 17.010, accuracy: 76.400%\n",
      "[624] loss: 17.011, accuracy: 76.600%\n",
      "[625] loss: 17.013, accuracy: 74.500%\n",
      "[626] loss: 25.112, accuracy: 37.000%\n",
      "[627] loss: 26.431, accuracy: 39.400%\n",
      "[628] loss: 26.027, accuracy: 40.600%\n",
      "[629] loss: 25.840, accuracy: 40.800%\n",
      "[630] loss: 25.682, accuracy: 41.100%\n",
      "[631] loss: 25.580, accuracy: 40.900%\n",
      "[632] loss: 25.480, accuracy: 41.400%\n",
      "[633] loss: 25.426, accuracy: 42.000%\n",
      "[634] loss: 25.446, accuracy: 41.400%\n",
      "[635] loss: 25.408, accuracy: 41.400%\n",
      "[636] loss: 25.361, accuracy: 41.600%\n",
      "[637] loss: 25.348, accuracy: 41.200%\n",
      "[638] loss: 25.330, accuracy: 41.800%\n",
      "[639] loss: 25.278, accuracy: 41.800%\n",
      "[640] loss: 25.247, accuracy: 42.000%\n",
      "[641] loss: 25.221, accuracy: 41.700%\n",
      "[642] loss: 24.213, accuracy: 50.000%\n",
      "[643] loss: 23.334, accuracy: 53.000%\n",
      "[644] loss: 22.964, accuracy: 54.400%\n",
      "[645] loss: 22.617, accuracy: 55.900%\n",
      "[646] loss: 22.378, accuracy: 54.600%\n",
      "[647] loss: 22.228, accuracy: 55.400%\n",
      "[648] loss: 22.145, accuracy: 54.900%\n",
      "[649] loss: 22.081, accuracy: 55.400%\n",
      "[650] loss: 22.049, accuracy: 55.400%\n",
      "[651] loss: 22.012, accuracy: 55.800%\n",
      "[652] loss: 21.981, accuracy: 55.400%\n",
      "[653] loss: 21.962, accuracy: 55.200%\n",
      "[654] loss: 21.938, accuracy: 55.400%\n",
      "[655] loss: 21.949, accuracy: 54.800%\n",
      "[656] loss: 21.947, accuracy: 55.500%\n",
      "[657] loss: 21.926, accuracy: 55.600%\n",
      "[658] loss: 21.903, accuracy: 55.100%\n",
      "[659] loss: 21.908, accuracy: 55.200%\n",
      "[660] loss: 21.879, accuracy: 55.600%\n",
      "[661] loss: 21.891, accuracy: 55.900%\n",
      "[662] loss: 21.891, accuracy: 55.300%\n",
      "[663] loss: 21.910, accuracy: 55.400%\n",
      "[664] loss: 21.888, accuracy: 55.800%\n",
      "[665] loss: 21.876, accuracy: 55.100%\n",
      "[666] loss: 21.877, accuracy: 55.300%\n",
      "[667] loss: 21.835, accuracy: 55.900%\n",
      "[668] loss: 21.882, accuracy: 55.700%\n",
      "[669] loss: 21.863, accuracy: 54.900%\n",
      "[670] loss: 21.876, accuracy: 55.600%\n",
      "[671] loss: 21.838, accuracy: 55.300%\n",
      "[672] loss: 21.854, accuracy: 55.100%\n",
      "[673] loss: 21.856, accuracy: 54.300%\n",
      "[674] loss: 21.842, accuracy: 55.400%\n",
      "[675] loss: 21.886, accuracy: 55.400%\n",
      "[676] loss: 21.865, accuracy: 55.000%\n",
      "[677] loss: 21.858, accuracy: 54.700%\n",
      "[678] loss: 21.837, accuracy: 54.400%\n",
      "[679] loss: 21.842, accuracy: 54.800%\n",
      "[680] loss: 21.841, accuracy: 54.500%\n",
      "[681] loss: 21.833, accuracy: 54.700%\n",
      "[682] loss: 21.819, accuracy: 54.700%\n",
      "[683] loss: 21.843, accuracy: 54.700%\n",
      "[684] loss: 21.863, accuracy: 54.100%\n",
      "[685] loss: 21.856, accuracy: 54.800%\n",
      "[686] loss: 21.855, accuracy: 53.900%\n",
      "[687] loss: 21.835, accuracy: 54.400%\n",
      "[688] loss: 21.843, accuracy: 55.000%\n",
      "[689] loss: 21.836, accuracy: 54.300%\n",
      "[690] loss: 21.843, accuracy: 54.900%\n",
      "[691] loss: 21.866, accuracy: 54.400%\n",
      "[692] loss: 21.898, accuracy: 55.100%\n",
      "[693] loss: 21.923, accuracy: 53.700%\n",
      "[694] loss: 21.934, accuracy: 55.500%\n",
      "[695] loss: 21.930, accuracy: 55.100%\n",
      "[696] loss: 21.994, accuracy: 54.800%\n",
      "[697] loss: 22.096, accuracy: 53.700%\n",
      "[698] loss: 22.550, accuracy: 51.600%\n",
      "[699] loss: 22.508, accuracy: 54.600%\n",
      "[700] loss: 22.317, accuracy: 54.900%\n",
      "[701] loss: 22.086, accuracy: 54.100%\n",
      "[702] loss: 22.038, accuracy: 55.000%\n",
      "[703] loss: 21.956, accuracy: 54.200%\n",
      "[704] loss: 21.924, accuracy: 54.400%\n",
      "[705] loss: 21.907, accuracy: 55.300%\n",
      "[706] loss: 21.869, accuracy: 55.300%\n",
      "[707] loss: 21.858, accuracy: 56.100%\n",
      "[708] loss: 21.884, accuracy: 55.700%\n",
      "[709] loss: 21.859, accuracy: 56.000%\n",
      "[710] loss: 21.842, accuracy: 55.800%\n",
      "[711] loss: 21.828, accuracy: 55.900%\n",
      "[712] loss: 21.815, accuracy: 55.800%\n",
      "[713] loss: 21.832, accuracy: 56.100%\n",
      "[714] loss: 21.829, accuracy: 56.200%\n",
      "[715] loss: 21.831, accuracy: 56.300%\n",
      "[716] loss: 21.833, accuracy: 55.700%\n",
      "[717] loss: 21.824, accuracy: 56.200%\n",
      "[718] loss: 21.839, accuracy: 55.600%\n",
      "[719] loss: 21.827, accuracy: 56.100%\n",
      "[720] loss: 21.815, accuracy: 55.800%\n",
      "[721] loss: 21.805, accuracy: 56.200%\n",
      "[722] loss: 21.819, accuracy: 56.000%\n",
      "[723] loss: 21.834, accuracy: 55.800%\n",
      "[724] loss: 21.813, accuracy: 56.200%\n",
      "[725] loss: 21.808, accuracy: 55.900%\n",
      "[726] loss: 21.798, accuracy: 55.700%\n",
      "[727] loss: 21.840, accuracy: 55.700%\n",
      "[728] loss: 21.849, accuracy: 55.100%\n",
      "[729] loss: 21.869, accuracy: 55.700%\n",
      "[730] loss: 21.835, accuracy: 55.600%\n",
      "[731] loss: 21.815, accuracy: 55.500%\n",
      "[732] loss: 21.812, accuracy: 54.500%\n",
      "[733] loss: 21.794, accuracy: 55.100%\n",
      "[734] loss: 21.808, accuracy: 54.800%\n",
      "[735] loss: 21.839, accuracy: 55.200%\n",
      "[736] loss: 21.844, accuracy: 54.800%\n",
      "[737] loss: 21.843, accuracy: 54.300%\n",
      "[738] loss: 21.843, accuracy: 54.800%\n",
      "[739] loss: 21.846, accuracy: 55.500%\n",
      "[740] loss: 21.838, accuracy: 55.500%\n",
      "[741] loss: 21.835, accuracy: 55.700%\n",
      "[742] loss: 21.815, accuracy: 55.000%\n",
      "[743] loss: 21.845, accuracy: 54.800%\n",
      "[744] loss: 21.818, accuracy: 54.600%\n",
      "[745] loss: 21.834, accuracy: 55.300%\n",
      "[746] loss: 21.814, accuracy: 55.200%\n",
      "[747] loss: 21.796, accuracy: 54.700%\n",
      "[748] loss: 21.815, accuracy: 54.500%\n",
      "[749] loss: 21.817, accuracy: 54.600%\n",
      "[750] loss: 21.831, accuracy: 54.400%\n",
      "[751] loss: 21.850, accuracy: 55.400%\n",
      "[752] loss: 21.855, accuracy: 55.200%\n",
      "[753] loss: 21.824, accuracy: 54.300%\n",
      "[754] loss: 21.841, accuracy: 54.400%\n",
      "[755] loss: 21.826, accuracy: 54.800%\n",
      "[756] loss: 21.843, accuracy: 54.400%\n",
      "[757] loss: 21.826, accuracy: 54.200%\n",
      "[758] loss: 21.840, accuracy: 55.200%\n",
      "[759] loss: 21.826, accuracy: 54.800%\n",
      "[760] loss: 21.866, accuracy: 54.800%\n",
      "[761] loss: 21.832, accuracy: 54.800%\n",
      "[762] loss: 21.813, accuracy: 54.500%\n",
      "[763] loss: 21.815, accuracy: 54.600%\n",
      "[764] loss: 21.819, accuracy: 53.900%\n",
      "[765] loss: 21.825, accuracy: 54.400%\n",
      "[766] loss: 21.848, accuracy: 54.800%\n",
      "[767] loss: 21.841, accuracy: 54.100%\n",
      "[768] loss: 21.852, accuracy: 53.800%\n",
      "[769] loss: 21.871, accuracy: 53.500%\n",
      "[770] loss: 22.004, accuracy: 52.500%\n",
      "[771] loss: 22.657, accuracy: 50.800%\n",
      "[772] loss: 22.598, accuracy: 54.300%\n",
      "[773] loss: 22.382, accuracy: 53.500%\n",
      "[774] loss: 22.190, accuracy: 53.700%\n",
      "[775] loss: 22.121, accuracy: 54.400%\n",
      "[776] loss: 22.017, accuracy: 55.000%\n",
      "[777] loss: 21.942, accuracy: 54.700%\n",
      "[778] loss: 21.910, accuracy: 54.200%\n",
      "[779] loss: 20.819, accuracy: 59.500%\n",
      "[780] loss: 20.257, accuracy: 61.200%\n",
      "[781] loss: 20.040, accuracy: 63.600%\n",
      "[782] loss: 19.854, accuracy: 62.000%\n",
      "[783] loss: 19.739, accuracy: 61.700%\n",
      "[784] loss: 19.656, accuracy: 63.600%\n",
      "[785] loss: 19.577, accuracy: 63.600%\n",
      "[786] loss: 19.578, accuracy: 63.100%\n",
      "[787] loss: 19.558, accuracy: 63.500%\n",
      "[788] loss: 19.540, accuracy: 63.600%\n",
      "[789] loss: 19.537, accuracy: 63.600%\n",
      "[790] loss: 19.516, accuracy: 63.300%\n",
      "[791] loss: 19.484, accuracy: 64.200%\n",
      "[792] loss: 19.473, accuracy: 63.200%\n",
      "[793] loss: 19.453, accuracy: 64.100%\n",
      "[794] loss: 19.413, accuracy: 62.900%\n",
      "[795] loss: 19.357, accuracy: 66.800%\n",
      "[796] loss: 19.759, accuracy: 61.900%\n",
      "[797] loss: 19.891, accuracy: 60.700%\n",
      "[798] loss: 19.471, accuracy: 67.100%\n",
      "[799] loss: 18.500, accuracy: 75.200%\n",
      "[800] loss: 18.042, accuracy: 76.800%\n",
      "[801] loss: 17.677, accuracy: 75.800%\n",
      "[802] loss: 17.393, accuracy: 76.700%\n",
      "[803] loss: 17.247, accuracy: 77.100%\n",
      "[804] loss: 17.126, accuracy: 77.300%\n",
      "[805] loss: 17.030, accuracy: 78.300%\n",
      "[806] loss: 16.988, accuracy: 78.000%\n",
      "[807] loss: 16.962, accuracy: 78.000%\n",
      "[808] loss: 16.940, accuracy: 77.500%\n",
      "[809] loss: 16.933, accuracy: 78.200%\n",
      "[810] loss: 16.922, accuracy: 77.800%\n",
      "[811] loss: 16.912, accuracy: 78.200%\n",
      "[812] loss: 16.906, accuracy: 78.400%\n",
      "[813] loss: 16.899, accuracy: 78.200%\n",
      "[814] loss: 16.893, accuracy: 78.400%\n",
      "[815] loss: 16.890, accuracy: 77.900%\n",
      "[816] loss: 16.887, accuracy: 77.600%\n",
      "[817] loss: 16.883, accuracy: 77.400%\n",
      "[818] loss: 16.876, accuracy: 76.800%\n",
      "[819] loss: 16.870, accuracy: 77.300%\n",
      "[820] loss: 16.872, accuracy: 77.200%\n",
      "[821] loss: 16.872, accuracy: 77.400%\n",
      "[822] loss: 16.864, accuracy: 77.700%\n",
      "[823] loss: 16.862, accuracy: 76.700%\n",
      "[824] loss: 16.867, accuracy: 77.100%\n",
      "[825] loss: 16.863, accuracy: 77.300%\n",
      "[826] loss: 16.860, accuracy: 77.000%\n",
      "[827] loss: 16.858, accuracy: 76.800%\n",
      "[828] loss: 16.860, accuracy: 77.700%\n",
      "[829] loss: 16.856, accuracy: 77.200%\n",
      "[830] loss: 16.855, accuracy: 76.600%\n",
      "[831] loss: 16.857, accuracy: 77.400%\n",
      "[832] loss: 16.846, accuracy: 77.500%\n",
      "[833] loss: 16.850, accuracy: 79.000%\n",
      "[834] loss: 16.844, accuracy: 77.700%\n",
      "[835] loss: 16.847, accuracy: 77.500%\n",
      "[836] loss: 16.845, accuracy: 77.900%\n",
      "[837] loss: 16.841, accuracy: 78.500%\n",
      "[838] loss: 16.844, accuracy: 77.900%\n",
      "[839] loss: 16.841, accuracy: 78.100%\n",
      "[840] loss: 16.921, accuracy: 73.600%\n",
      "[841] loss: 17.970, accuracy: 73.800%\n",
      "[842] loss: 18.028, accuracy: 76.300%\n",
      "[843] loss: 17.675, accuracy: 75.700%\n",
      "[844] loss: 17.364, accuracy: 78.300%\n",
      "[845] loss: 17.201, accuracy: 76.700%\n",
      "[846] loss: 17.042, accuracy: 77.300%\n",
      "[847] loss: 16.964, accuracy: 78.900%\n",
      "[848] loss: 16.895, accuracy: 79.100%\n",
      "[849] loss: 16.866, accuracy: 79.200%\n",
      "[850] loss: 16.844, accuracy: 78.100%\n",
      "[851] loss: 16.836, accuracy: 79.300%\n",
      "[852] loss: 16.835, accuracy: 78.900%\n",
      "[853] loss: 16.834, accuracy: 79.000%\n",
      "[854] loss: 16.831, accuracy: 78.900%\n",
      "[855] loss: 16.837, accuracy: 79.100%\n",
      "[856] loss: 16.834, accuracy: 79.100%\n",
      "[857] loss: 16.830, accuracy: 79.100%\n",
      "[858] loss: 16.832, accuracy: 79.200%\n",
      "[859] loss: 16.829, accuracy: 79.100%\n",
      "[860] loss: 16.829, accuracy: 79.400%\n",
      "[861] loss: 16.833, accuracy: 78.800%\n",
      "[862] loss: 16.831, accuracy: 79.600%\n",
      "[863] loss: 16.828, accuracy: 79.300%\n",
      "[864] loss: 16.833, accuracy: 79.300%\n",
      "[865] loss: 16.830, accuracy: 79.100%\n",
      "[866] loss: 16.830, accuracy: 79.200%\n",
      "[867] loss: 16.830, accuracy: 78.500%\n",
      "[868] loss: 16.828, accuracy: 79.200%\n",
      "[869] loss: 16.830, accuracy: 78.700%\n",
      "[870] loss: 16.830, accuracy: 79.200%\n",
      "[871] loss: 16.832, accuracy: 78.700%\n",
      "[872] loss: 16.830, accuracy: 78.700%\n",
      "[873] loss: 16.827, accuracy: 78.500%\n",
      "[874] loss: 16.834, accuracy: 78.800%\n",
      "[875] loss: 16.831, accuracy: 78.800%\n",
      "[876] loss: 16.834, accuracy: 78.800%\n",
      "[877] loss: 16.839, accuracy: 78.800%\n",
      "[878] loss: 16.838, accuracy: 79.000%\n",
      "[879] loss: 16.837, accuracy: 78.600%\n",
      "[880] loss: 16.836, accuracy: 79.200%\n",
      "[881] loss: 16.836, accuracy: 78.900%\n",
      "[882] loss: 16.831, accuracy: 79.100%\n",
      "[883] loss: 16.836, accuracy: 78.900%\n",
      "[884] loss: 16.833, accuracy: 79.100%\n",
      "[885] loss: 16.831, accuracy: 79.200%\n",
      "[886] loss: 16.833, accuracy: 78.800%\n",
      "[887] loss: 16.831, accuracy: 78.500%\n",
      "[888] loss: 16.833, accuracy: 78.600%\n",
      "[889] loss: 16.831, accuracy: 78.300%\n",
      "[890] loss: 16.826, accuracy: 78.700%\n",
      "[891] loss: 16.828, accuracy: 78.600%\n",
      "[892] loss: 16.828, accuracy: 78.800%\n",
      "[893] loss: 16.830, accuracy: 78.700%\n",
      "[894] loss: 16.825, accuracy: 78.300%\n",
      "[895] loss: 16.828, accuracy: 78.800%\n",
      "[896] loss: 16.823, accuracy: 78.600%\n",
      "[897] loss: 16.825, accuracy: 78.900%\n",
      "[898] loss: 17.149, accuracy: 73.400%\n",
      "[899] loss: 17.902, accuracy: 74.000%\n",
      "[900] loss: 18.001, accuracy: 74.100%\n",
      "[901] loss: 17.964, accuracy: 77.400%\n",
      "[902] loss: 17.501, accuracy: 77.400%\n",
      "[903] loss: 17.249, accuracy: 77.600%\n",
      "[904] loss: 17.060, accuracy: 78.500%\n",
      "[905] loss: 16.953, accuracy: 77.600%\n",
      "[906] loss: 16.896, accuracy: 77.100%\n",
      "[907] loss: 16.859, accuracy: 78.300%\n",
      "[908] loss: 16.850, accuracy: 78.600%\n",
      "[909] loss: 16.842, accuracy: 78.400%\n",
      "[910] loss: 16.825, accuracy: 77.800%\n",
      "[911] loss: 16.822, accuracy: 78.700%\n",
      "[912] loss: 16.819, accuracy: 78.300%\n",
      "[913] loss: 16.817, accuracy: 78.500%\n",
      "[914] loss: 16.815, accuracy: 78.900%\n",
      "[915] loss: 16.812, accuracy: 78.500%\n",
      "[916] loss: 16.809, accuracy: 78.700%\n",
      "[917] loss: 16.811, accuracy: 78.700%\n",
      "[918] loss: 16.815, accuracy: 78.600%\n",
      "[919] loss: 16.810, accuracy: 78.600%\n",
      "[920] loss: 16.810, accuracy: 78.700%\n",
      "[921] loss: 16.810, accuracy: 78.700%\n",
      "[922] loss: 16.817, accuracy: 78.900%\n",
      "[923] loss: 16.815, accuracy: 78.800%\n",
      "[924] loss: 16.810, accuracy: 79.100%\n",
      "[925] loss: 16.810, accuracy: 78.800%\n",
      "[926] loss: 16.810, accuracy: 79.200%\n",
      "[927] loss: 16.810, accuracy: 79.200%\n",
      "[928] loss: 16.812, accuracy: 79.100%\n",
      "[929] loss: 16.809, accuracy: 79.000%\n",
      "[930] loss: 16.814, accuracy: 79.500%\n",
      "[931] loss: 16.807, accuracy: 78.800%\n",
      "[932] loss: 16.809, accuracy: 79.300%\n",
      "[933] loss: 16.812, accuracy: 79.100%\n",
      "[934] loss: 16.809, accuracy: 79.200%\n",
      "[935] loss: 16.809, accuracy: 79.100%\n",
      "[936] loss: 16.809, accuracy: 79.100%\n",
      "[937] loss: 16.809, accuracy: 79.200%\n",
      "[938] loss: 16.807, accuracy: 78.900%\n",
      "[939] loss: 16.812, accuracy: 79.000%\n",
      "[940] loss: 16.807, accuracy: 79.000%\n",
      "[941] loss: 16.811, accuracy: 79.100%\n",
      "[942] loss: 16.811, accuracy: 79.000%\n",
      "[943] loss: 16.809, accuracy: 78.900%\n",
      "[944] loss: 16.807, accuracy: 78.700%\n",
      "[945] loss: 16.812, accuracy: 78.600%\n",
      "[946] loss: 16.811, accuracy: 78.800%\n",
      "[947] loss: 16.807, accuracy: 78.900%\n",
      "[948] loss: 16.809, accuracy: 78.600%\n",
      "[949] loss: 16.807, accuracy: 78.900%\n",
      "[950] loss: 16.811, accuracy: 78.800%\n",
      "[951] loss: 16.809, accuracy: 78.800%\n",
      "[952] loss: 16.807, accuracy: 78.800%\n",
      "[953] loss: 16.807, accuracy: 78.600%\n",
      "[954] loss: 16.809, accuracy: 78.300%\n",
      "[955] loss: 16.814, accuracy: 78.700%\n",
      "[956] loss: 16.807, accuracy: 78.700%\n",
      "[957] loss: 16.807, accuracy: 78.600%\n",
      "[958] loss: 16.807, accuracy: 78.400%\n",
      "[959] loss: 16.807, accuracy: 78.300%\n",
      "[960] loss: 16.809, accuracy: 78.600%\n",
      "[961] loss: 16.811, accuracy: 78.400%\n",
      "[962] loss: 16.807, accuracy: 78.100%\n",
      "[963] loss: 16.809, accuracy: 78.100%\n",
      "[964] loss: 16.809, accuracy: 78.900%\n",
      "[965] loss: 16.812, accuracy: 78.400%\n",
      "[966] loss: 16.809, accuracy: 78.400%\n",
      "[967] loss: 16.807, accuracy: 78.700%\n",
      "[968] loss: 16.806, accuracy: 78.800%\n",
      "[969] loss: 16.806, accuracy: 77.800%\n",
      "[970] loss: 16.809, accuracy: 78.100%\n",
      "[971] loss: 16.807, accuracy: 78.000%\n",
      "[972] loss: 16.814, accuracy: 76.700%\n",
      "[973] loss: 27.130, accuracy: 14.000%\n",
      "[974] loss: 30.183, accuracy: 15.400%\n",
      "[975] loss: 29.846, accuracy: 15.500%\n",
      "[976] loss: 29.536, accuracy: 20.900%\n",
      "[977] loss: 28.412, accuracy: 28.700%\n",
      "[978] loss: 26.890, accuracy: 43.000%\n",
      "[979] loss: 24.750, accuracy: 55.700%\n",
      "[980] loss: 22.949, accuracy: 65.900%\n",
      "[981] loss: 21.617, accuracy: 70.400%\n",
      "[982] loss: 20.890, accuracy: 72.300%\n",
      "[983] loss: 20.344, accuracy: 73.200%\n",
      "[984] loss: 20.003, accuracy: 75.900%\n",
      "[985] loss: 19.462, accuracy: 74.800%\n",
      "[986] loss: 19.271, accuracy: 78.600%\n",
      "[987] loss: 18.965, accuracy: 78.000%\n",
      "[988] loss: 18.766, accuracy: 79.400%\n",
      "[989] loss: 18.542, accuracy: 78.900%\n",
      "[990] loss: 18.368, accuracy: 79.900%\n",
      "[991] loss: 18.194, accuracy: 80.500%\n",
      "[992] loss: 18.044, accuracy: 78.400%\n",
      "[993] loss: 17.965, accuracy: 80.400%\n",
      "[994] loss: 17.900, accuracy: 80.100%\n",
      "[995] loss: 17.739, accuracy: 81.100%\n",
      "[996] loss: 17.645, accuracy: 80.200%\n",
      "[997] loss: 17.583, accuracy: 80.700%\n",
      "[998] loss: 17.595, accuracy: 81.000%\n",
      "[999] loss: 17.513, accuracy: 80.800%\n",
      "[1000] loss: 17.502, accuracy: 79.000%\n",
      "[1001] loss: 17.547, accuracy: 80.100%\n",
      "[1002] loss: 17.443, accuracy: 78.800%\n",
      "[1003] loss: 17.347, accuracy: 80.800%\n",
      "[1004] loss: 17.308, accuracy: 81.400%\n",
      "[1005] loss: 17.270, accuracy: 80.800%\n",
      "[1006] loss: 17.240, accuracy: 81.400%\n",
      "[1007] loss: 17.269, accuracy: 80.800%\n",
      "[1008] loss: 17.235, accuracy: 80.100%\n",
      "[1009] loss: 17.219, accuracy: 81.100%\n",
      "[1010] loss: 17.218, accuracy: 80.700%\n",
      "[1011] loss: 17.184, accuracy: 80.200%\n",
      "[1012] loss: 17.151, accuracy: 82.000%\n",
      "[1013] loss: 17.147, accuracy: 79.300%\n",
      "[1014] loss: 17.133, accuracy: 80.900%\n",
      "[1015] loss: 17.101, accuracy: 80.800%\n",
      "[1016] loss: 17.096, accuracy: 79.100%\n",
      "[1017] loss: 17.123, accuracy: 80.100%\n",
      "[1018] loss: 17.103, accuracy: 78.200%\n",
      "[1019] loss: 17.159, accuracy: 78.200%\n",
      "[1020] loss: 17.151, accuracy: 80.400%\n",
      "[1021] loss: 17.138, accuracy: 80.600%\n",
      "[1022] loss: 17.110, accuracy: 80.200%\n",
      "[1023] loss: 17.059, accuracy: 81.300%\n",
      "[1024] loss: 17.026, accuracy: 80.700%\n",
      "[1025] loss: 17.004, accuracy: 80.100%\n",
      "[1026] loss: 16.992, accuracy: 80.900%\n",
      "[1027] loss: 16.979, accuracy: 80.700%\n",
      "[1028] loss: 16.976, accuracy: 79.600%\n",
      "[1029] loss: 16.977, accuracy: 80.100%\n",
      "[1030] loss: 16.969, accuracy: 80.300%\n",
      "[1031] loss: 16.971, accuracy: 80.200%\n",
      "[1032] loss: 16.969, accuracy: 79.800%\n",
      "[1033] loss: 16.967, accuracy: 80.100%\n",
      "[1034] loss: 16.970, accuracy: 79.600%\n",
      "[1035] loss: 16.967, accuracy: 80.000%\n",
      "[1036] loss: 16.968, accuracy: 79.900%\n",
      "[1037] loss: 16.971, accuracy: 79.800%\n",
      "[1038] loss: 16.967, accuracy: 79.500%\n",
      "[1039] loss: 16.965, accuracy: 79.800%\n",
      "[1040] loss: 16.969, accuracy: 80.400%\n",
      "[1041] loss: 16.968, accuracy: 79.800%\n",
      "[1042] loss: 16.964, accuracy: 79.500%\n",
      "[1043] loss: 16.962, accuracy: 79.700%\n",
      "[1044] loss: 16.960, accuracy: 80.000%\n",
      "[1045] loss: 16.964, accuracy: 79.700%\n",
      "[1046] loss: 16.965, accuracy: 79.200%\n",
      "[1047] loss: 16.957, accuracy: 79.000%\n",
      "[1048] loss: 16.957, accuracy: 79.100%\n",
      "[1049] loss: 16.962, accuracy: 79.400%\n",
      "[1050] loss: 16.957, accuracy: 79.400%\n",
      "[1051] loss: 16.962, accuracy: 79.500%\n",
      "[1052] loss: 17.663, accuracy: 75.700%\n",
      "[1053] loss: 18.449, accuracy: 76.700%\n",
      "[1054] loss: 18.280, accuracy: 79.000%\n",
      "[1055] loss: 17.898, accuracy: 80.700%\n",
      "[1056] loss: 17.526, accuracy: 79.600%\n",
      "[1057] loss: 17.329, accuracy: 79.600%\n",
      "[1058] loss: 17.210, accuracy: 79.700%\n",
      "[1059] loss: 17.125, accuracy: 80.600%\n",
      "[1060] loss: 17.063, accuracy: 79.400%\n",
      "[1061] loss: 17.019, accuracy: 81.200%\n",
      "[1062] loss: 16.990, accuracy: 80.800%\n",
      "[1063] loss: 16.975, accuracy: 81.300%\n",
      "[1064] loss: 16.969, accuracy: 81.300%\n",
      "[1065] loss: 16.966, accuracy: 81.700%\n",
      "[1066] loss: 16.960, accuracy: 81.400%\n",
      "[1067] loss: 16.954, accuracy: 80.300%\n",
      "[1068] loss: 16.946, accuracy: 80.800%\n",
      "[1069] loss: 16.956, accuracy: 81.100%\n",
      "[1070] loss: 16.949, accuracy: 80.800%\n",
      "[1071] loss: 16.952, accuracy: 80.800%\n",
      "[1072] loss: 16.945, accuracy: 80.800%\n",
      "[1073] loss: 16.944, accuracy: 80.600%\n",
      "[1074] loss: 16.949, accuracy: 80.600%\n",
      "[1075] loss: 16.942, accuracy: 80.600%\n",
      "[1076] loss: 16.941, accuracy: 80.100%\n",
      "[1077] loss: 16.940, accuracy: 80.800%\n",
      "[1078] loss: 16.944, accuracy: 80.600%\n",
      "[1079] loss: 16.937, accuracy: 80.300%\n",
      "[1080] loss: 16.941, accuracy: 79.600%\n",
      "[1081] loss: 16.967, accuracy: 79.100%\n",
      "[1082] loss: 17.024, accuracy: 77.800%\n",
      "[1083] loss: 17.042, accuracy: 78.300%\n",
      "[1084] loss: 17.020, accuracy: 79.200%\n",
      "[1085] loss: 17.027, accuracy: 80.200%\n",
      "[1086] loss: 17.031, accuracy: 79.600%\n",
      "[1087] loss: 17.097, accuracy: 80.100%\n",
      "[1088] loss: 17.067, accuracy: 79.200%\n",
      "[1089] loss: 17.109, accuracy: 79.000%\n",
      "[1090] loss: 17.065, accuracy: 78.800%\n",
      "[1091] loss: 17.076, accuracy: 80.200%\n",
      "[1092] loss: 17.063, accuracy: 78.700%\n",
      "[1093] loss: 17.064, accuracy: 79.500%\n",
      "[1094] loss: 17.065, accuracy: 78.400%\n",
      "[1095] loss: 17.011, accuracy: 78.400%\n",
      "[1096] loss: 16.966, accuracy: 79.100%\n",
      "[1097] loss: 16.959, accuracy: 80.100%\n",
      "[1098] loss: 16.945, accuracy: 78.300%\n",
      "[1099] loss: 16.925, accuracy: 78.400%\n",
      "[1100] loss: 16.913, accuracy: 79.900%\n",
      "[1101] loss: 16.905, accuracy: 78.600%\n",
      "[1102] loss: 16.905, accuracy: 79.100%\n",
      "[1103] loss: 16.906, accuracy: 78.700%\n",
      "[1104] loss: 16.907, accuracy: 78.900%\n",
      "[1105] loss: 16.900, accuracy: 78.400%\n",
      "[1106] loss: 16.904, accuracy: 78.700%\n",
      "[1107] loss: 16.899, accuracy: 78.700%\n",
      "[1108] loss: 16.897, accuracy: 78.800%\n",
      "[1109] loss: 16.899, accuracy: 78.600%\n",
      "[1110] loss: 16.899, accuracy: 78.800%\n",
      "[1111] loss: 16.899, accuracy: 78.500%\n",
      "[1112] loss: 16.899, accuracy: 78.700%\n",
      "[1113] loss: 16.901, accuracy: 78.700%\n",
      "[1114] loss: 16.902, accuracy: 78.800%\n",
      "[1115] loss: 16.902, accuracy: 79.000%\n",
      "[1116] loss: 16.899, accuracy: 78.700%\n",
      "[1117] loss: 16.904, accuracy: 78.300%\n",
      "[1118] loss: 16.906, accuracy: 78.500%\n",
      "[1119] loss: 16.897, accuracy: 78.400%\n",
      "[1120] loss: 16.902, accuracy: 78.200%\n",
      "[1121] loss: 16.899, accuracy: 78.300%\n",
      "[1122] loss: 16.900, accuracy: 78.400%\n",
      "[1123] loss: 16.897, accuracy: 77.900%\n",
      "[1124] loss: 16.899, accuracy: 78.000%\n",
      "[1125] loss: 16.908, accuracy: 77.500%\n",
      "[1126] loss: 16.956, accuracy: 76.100%\n",
      "[1127] loss: 17.143, accuracy: 74.000%\n",
      "[1128] loss: 17.453, accuracy: 78.400%\n",
      "[1129] loss: 17.411, accuracy: 78.300%\n",
      "[1130] loss: 17.366, accuracy: 78.900%\n",
      "[1131] loss: 17.138, accuracy: 78.600%\n",
      "[1132] loss: 17.080, accuracy: 79.000%\n",
      "[1133] loss: 16.974, accuracy: 78.500%\n",
      "[1134] loss: 16.941, accuracy: 79.400%\n",
      "[1135] loss: 16.914, accuracy: 79.300%\n",
      "[1136] loss: 16.890, accuracy: 80.200%\n",
      "[1137] loss: 16.876, accuracy: 80.600%\n",
      "[1138] loss: 16.883, accuracy: 80.300%\n",
      "[1139] loss: 16.874, accuracy: 80.000%\n",
      "[1140] loss: 16.873, accuracy: 79.600%\n",
      "[1141] loss: 16.878, accuracy: 79.900%\n",
      "[1142] loss: 16.873, accuracy: 79.700%\n",
      "[1143] loss: 16.871, accuracy: 79.400%\n",
      "[1144] loss: 16.877, accuracy: 79.100%\n",
      "[1145] loss: 16.872, accuracy: 79.500%\n",
      "[1146] loss: 16.873, accuracy: 79.500%\n",
      "[1147] loss: 16.877, accuracy: 79.000%\n",
      "[1148] loss: 16.880, accuracy: 78.700%\n",
      "[1149] loss: 16.875, accuracy: 79.100%\n",
      "[1150] loss: 16.871, accuracy: 79.000%\n",
      "[1151] loss: 16.874, accuracy: 78.800%\n",
      "[1152] loss: 16.872, accuracy: 78.400%\n",
      "[1153] loss: 16.863, accuracy: 78.900%\n",
      "[1154] loss: 16.865, accuracy: 78.700%\n",
      "[1155] loss: 16.875, accuracy: 78.800%\n",
      "[1156] loss: 16.867, accuracy: 78.400%\n",
      "[1157] loss: 16.865, accuracy: 78.900%\n",
      "[1158] loss: 16.871, accuracy: 78.600%\n",
      "[1159] loss: 16.862, accuracy: 79.100%\n",
      "[1160] loss: 16.868, accuracy: 78.100%\n",
      "[1161] loss: 16.863, accuracy: 78.800%\n",
      "[1162] loss: 16.861, accuracy: 78.900%\n",
      "[1163] loss: 16.866, accuracy: 78.100%\n",
      "[1164] loss: 16.862, accuracy: 78.700%\n",
      "[1165] loss: 16.864, accuracy: 78.800%\n",
      "[1166] loss: 16.864, accuracy: 78.800%\n",
      "[1167] loss: 16.859, accuracy: 78.500%\n",
      "[1168] loss: 16.862, accuracy: 78.600%\n",
      "[1169] loss: 16.864, accuracy: 78.500%\n",
      "[1170] loss: 16.866, accuracy: 78.300%\n",
      "[1171] loss: 16.864, accuracy: 78.400%\n",
      "[1172] loss: 16.867, accuracy: 78.700%\n",
      "[1173] loss: 16.859, accuracy: 78.300%\n",
      "[1174] loss: 16.859, accuracy: 78.700%\n",
      "[1175] loss: 16.864, accuracy: 78.600%\n",
      "[1176] loss: 16.864, accuracy: 78.300%\n",
      "[1177] loss: 16.871, accuracy: 77.600%\n",
      "[1178] loss: 16.860, accuracy: 78.400%\n",
      "[1179] loss: 16.860, accuracy: 77.800%\n",
      "[1180] loss: 16.867, accuracy: 78.000%\n",
      "[1181] loss: 16.863, accuracy: 78.300%\n",
      "[1182] loss: 16.870, accuracy: 77.200%\n",
      "[1183] loss: 16.868, accuracy: 77.700%\n",
      "[1184] loss: 16.866, accuracy: 78.700%\n",
      "[1185] loss: 16.865, accuracy: 75.300%\n",
      "[1186] loss: 21.126, accuracy: 69.700%\n",
      "[1187] loss: 19.584, accuracy: 76.000%\n",
      "[1188] loss: 18.608, accuracy: 77.500%\n",
      "[1189] loss: 18.003, accuracy: 79.500%\n",
      "[1190] loss: 17.645, accuracy: 79.100%\n",
      "[1191] loss: 17.425, accuracy: 79.800%\n",
      "[1192] loss: 17.267, accuracy: 81.100%\n",
      "[1193] loss: 17.148, accuracy: 79.000%\n",
      "[1194] loss: 17.077, accuracy: 79.600%\n",
      "[1195] loss: 17.004, accuracy: 79.000%\n",
      "[1196] loss: 16.969, accuracy: 79.800%\n",
      "[1197] loss: 16.948, accuracy: 79.700%\n",
      "[1198] loss: 16.917, accuracy: 79.600%\n",
      "[1199] loss: 16.913, accuracy: 79.100%\n",
      "[1200] loss: 16.905, accuracy: 79.400%\n",
      "[1201] loss: 16.907, accuracy: 79.300%\n",
      "[1202] loss: 16.903, accuracy: 78.700%\n",
      "[1203] loss: 16.899, accuracy: 79.300%\n",
      "[1204] loss: 16.896, accuracy: 78.400%\n",
      "[1205] loss: 16.892, accuracy: 78.300%\n",
      "[1206] loss: 16.887, accuracy: 79.100%\n",
      "[1207] loss: 16.891, accuracy: 78.600%\n",
      "[1208] loss: 16.886, accuracy: 78.400%\n",
      "[1209] loss: 16.895, accuracy: 78.600%\n",
      "[1210] loss: 16.882, accuracy: 77.800%\n",
      "[1211] loss: 16.879, accuracy: 78.800%\n",
      "[1212] loss: 16.889, accuracy: 77.900%\n",
      "[1213] loss: 16.880, accuracy: 78.200%\n",
      "[1214] loss: 16.880, accuracy: 78.200%\n",
      "[1215] loss: 16.884, accuracy: 77.700%\n",
      "[1216] loss: 16.884, accuracy: 78.000%\n",
      "[1217] loss: 16.879, accuracy: 77.800%\n",
      "[1218] loss: 16.887, accuracy: 77.600%\n",
      "[1219] loss: 16.887, accuracy: 78.500%\n",
      "[1220] loss: 16.887, accuracy: 78.500%\n",
      "[1221] loss: 16.882, accuracy: 77.600%\n",
      "[1222] loss: 16.886, accuracy: 78.100%\n",
      "[1223] loss: 16.880, accuracy: 78.200%\n",
      "[1224] loss: 16.884, accuracy: 77.500%\n",
      "[1225] loss: 16.882, accuracy: 77.500%\n",
      "[1226] loss: 16.882, accuracy: 78.300%\n",
      "[1227] loss: 16.881, accuracy: 77.600%\n",
      "[1228] loss: 16.885, accuracy: 78.500%\n",
      "[1229] loss: 16.883, accuracy: 78.200%\n",
      "[1230] loss: 16.876, accuracy: 77.900%\n",
      "[1231] loss: 16.876, accuracy: 78.500%\n",
      "[1232] loss: 16.878, accuracy: 78.300%\n",
      "[1233] loss: 16.877, accuracy: 77.500%\n",
      "[1234] loss: 16.876, accuracy: 78.300%\n",
      "[1235] loss: 16.875, accuracy: 78.300%\n",
      "[1236] loss: 16.882, accuracy: 77.900%\n",
      "[1237] loss: 16.878, accuracy: 78.000%\n",
      "[1238] loss: 16.880, accuracy: 78.100%\n",
      "[1239] loss: 16.883, accuracy: 78.000%\n",
      "[1240] loss: 16.875, accuracy: 77.300%\n",
      "[1241] loss: 16.875, accuracy: 77.500%\n",
      "[1242] loss: 16.874, accuracy: 78.100%\n",
      "[1243] loss: 16.874, accuracy: 77.700%\n",
      "[1244] loss: 16.871, accuracy: 78.000%\n",
      "[1245] loss: 16.875, accuracy: 77.700%\n",
      "[1246] loss: 16.868, accuracy: 78.100%\n",
      "[1247] loss: 16.872, accuracy: 77.800%\n",
      "[1248] loss: 16.872, accuracy: 77.600%\n",
      "[1249] loss: 16.873, accuracy: 77.900%\n",
      "[1250] loss: 16.877, accuracy: 77.100%\n",
      "[1251] loss: 16.868, accuracy: 77.200%\n",
      "[1252] loss: 16.877, accuracy: 77.400%\n",
      "[1253] loss: 16.873, accuracy: 78.000%\n",
      "[1254] loss: 16.871, accuracy: 77.300%\n",
      "[1255] loss: 16.879, accuracy: 77.800%\n",
      "[1256] loss: 16.869, accuracy: 78.100%\n",
      "[1257] loss: 16.878, accuracy: 77.600%\n",
      "[1258] loss: 17.417, accuracy: 76.700%\n",
      "[1259] loss: 17.908, accuracy: 76.700%\n",
      "[1260] loss: 17.868, accuracy: 78.400%\n",
      "[1261] loss: 17.635, accuracy: 77.800%\n",
      "[1262] loss: 17.377, accuracy: 78.000%\n",
      "[1263] loss: 17.263, accuracy: 79.100%\n",
      "[1264] loss: 17.069, accuracy: 78.600%\n",
      "[1265] loss: 16.975, accuracy: 78.700%\n",
      "[1266] loss: 16.929, accuracy: 78.600%\n",
      "[1267] loss: 16.910, accuracy: 79.200%\n",
      "[1268] loss: 16.906, accuracy: 79.400%\n",
      "[1269] loss: 16.880, accuracy: 78.500%\n",
      "[1270] loss: 16.869, accuracy: 79.200%\n",
      "[1271] loss: 16.873, accuracy: 78.600%\n",
      "[1272] loss: 16.860, accuracy: 78.500%\n",
      "[1273] loss: 16.858, accuracy: 78.900%\n",
      "[1274] loss: 16.858, accuracy: 78.800%\n",
      "[1275] loss: 16.856, accuracy: 78.600%\n",
      "[1276] loss: 16.865, accuracy: 78.500%\n",
      "[1277] loss: 16.853, accuracy: 78.400%\n",
      "[1278] loss: 16.853, accuracy: 79.100%\n",
      "[1279] loss: 16.853, accuracy: 78.600%\n",
      "[1280] loss: 16.853, accuracy: 78.500%\n",
      "[1281] loss: 16.858, accuracy: 78.700%\n",
      "[1282] loss: 16.857, accuracy: 78.700%\n",
      "[1283] loss: 16.855, accuracy: 78.800%\n",
      "[1284] loss: 16.858, accuracy: 78.700%\n",
      "[1285] loss: 16.855, accuracy: 78.400%\n",
      "[1286] loss: 16.862, accuracy: 78.400%\n",
      "[1287] loss: 16.855, accuracy: 78.400%\n",
      "[1288] loss: 16.853, accuracy: 78.600%\n",
      "[1289] loss: 16.850, accuracy: 78.700%\n",
      "[1290] loss: 16.855, accuracy: 78.600%\n",
      "[1291] loss: 16.860, accuracy: 78.900%\n",
      "[1292] loss: 16.855, accuracy: 78.600%\n",
      "[1293] loss: 16.855, accuracy: 78.500%\n",
      "[1294] loss: 16.853, accuracy: 78.700%\n",
      "[1295] loss: 16.857, accuracy: 78.600%\n",
      "[1296] loss: 16.858, accuracy: 78.500%\n",
      "[1297] loss: 16.851, accuracy: 78.700%\n",
      "[1298] loss: 16.853, accuracy: 79.000%\n",
      "[1299] loss: 16.851, accuracy: 78.400%\n",
      "[1300] loss: 16.850, accuracy: 78.700%\n",
      "[1301] loss: 16.856, accuracy: 78.700%\n",
      "[1302] loss: 16.851, accuracy: 78.100%\n",
      "[1303] loss: 16.857, accuracy: 78.500%\n",
      "[1304] loss: 16.853, accuracy: 78.900%\n",
      "[1305] loss: 16.853, accuracy: 78.200%\n",
      "[1306] loss: 16.856, accuracy: 78.200%\n",
      "[1307] loss: 16.858, accuracy: 78.500%\n",
      "[1308] loss: 16.851, accuracy: 78.300%\n",
      "[1309] loss: 16.858, accuracy: 78.500%\n",
      "[1310] loss: 16.856, accuracy: 78.200%\n",
      "[1311] loss: 16.857, accuracy: 78.400%\n",
      "[1312] loss: 16.853, accuracy: 78.500%\n",
      "[1313] loss: 16.851, accuracy: 78.800%\n",
      "[1314] loss: 16.858, accuracy: 78.100%\n",
      "[1315] loss: 16.859, accuracy: 78.000%\n",
      "[1316] loss: 16.852, accuracy: 78.600%\n",
      "[1317] loss: 16.854, accuracy: 77.900%\n",
      "[1318] loss: 16.853, accuracy: 78.300%\n",
      "[1319] loss: 16.850, accuracy: 78.300%\n",
      "[1320] loss: 16.855, accuracy: 78.400%\n",
      "[1321] loss: 16.863, accuracy: 76.700%\n",
      "[1322] loss: 17.078, accuracy: 75.700%\n",
      "[1323] loss: 17.489, accuracy: 77.400%\n",
      "[1324] loss: 17.598, accuracy: 78.100%\n",
      "[1325] loss: 17.359, accuracy: 79.200%\n",
      "[1326] loss: 17.203, accuracy: 78.800%\n",
      "[1327] loss: 17.144, accuracy: 79.100%\n",
      "[1328] loss: 17.120, accuracy: 79.100%\n",
      "[1329] loss: 17.049, accuracy: 79.800%\n",
      "[1330] loss: 16.952, accuracy: 79.500%\n",
      "[1331] loss: 16.892, accuracy: 79.700%\n",
      "[1332] loss: 16.874, accuracy: 79.400%\n",
      "[1333] loss: 16.857, accuracy: 79.000%\n",
      "[1334] loss: 16.846, accuracy: 79.300%\n",
      "[1335] loss: 16.847, accuracy: 79.500%\n",
      "[1336] loss: 16.843, accuracy: 79.200%\n",
      "[1337] loss: 16.842, accuracy: 79.300%\n",
      "[1338] loss: 16.847, accuracy: 79.300%\n",
      "[1339] loss: 16.842, accuracy: 79.300%\n",
      "[1340] loss: 16.839, accuracy: 79.200%\n",
      "[1341] loss: 16.842, accuracy: 79.300%\n",
      "[1342] loss: 16.841, accuracy: 79.200%\n",
      "[1343] loss: 16.839, accuracy: 79.400%\n",
      "[1344] loss: 16.839, accuracy: 79.200%\n",
      "[1345] loss: 16.844, accuracy: 79.100%\n",
      "[1346] loss: 16.848, accuracy: 79.200%\n",
      "[1347] loss: 16.841, accuracy: 79.300%\n",
      "[1348] loss: 16.844, accuracy: 79.200%\n",
      "[1349] loss: 16.842, accuracy: 78.900%\n",
      "[1350] loss: 16.844, accuracy: 79.000%\n",
      "[1351] loss: 16.846, accuracy: 79.400%\n",
      "[1352] loss: 16.848, accuracy: 78.900%\n",
      "[1353] loss: 16.843, accuracy: 79.100%\n",
      "[1354] loss: 16.839, accuracy: 79.100%\n",
      "[1355] loss: 16.844, accuracy: 78.900%\n",
      "[1356] loss: 16.839, accuracy: 78.800%\n",
      "[1357] loss: 16.841, accuracy: 78.800%\n",
      "[1358] loss: 16.841, accuracy: 78.700%\n",
      "[1359] loss: 16.839, accuracy: 79.000%\n",
      "[1360] loss: 16.846, accuracy: 78.800%\n",
      "[1361] loss: 16.844, accuracy: 78.900%\n",
      "[1362] loss: 16.842, accuracy: 78.900%\n",
      "[1363] loss: 16.842, accuracy: 78.800%\n",
      "[1364] loss: 16.842, accuracy: 78.800%\n",
      "[1365] loss: 16.841, accuracy: 78.800%\n",
      "[1366] loss: 16.844, accuracy: 78.700%\n",
      "[1367] loss: 16.846, accuracy: 78.800%\n",
      "[1368] loss: 16.842, accuracy: 78.600%\n",
      "[1369] loss: 16.844, accuracy: 78.800%\n",
      "[1370] loss: 16.845, accuracy: 78.900%\n",
      "[1371] loss: 16.840, accuracy: 78.400%\n",
      "[1372] loss: 16.844, accuracy: 78.700%\n",
      "[1373] loss: 16.845, accuracy: 79.000%\n",
      "[1374] loss: 16.849, accuracy: 78.600%\n",
      "[1375] loss: 16.843, accuracy: 78.300%\n",
      "[1376] loss: 16.845, accuracy: 78.600%\n",
      "[1377] loss: 16.840, accuracy: 78.500%\n",
      "[1378] loss: 16.842, accuracy: 79.100%\n",
      "[1379] loss: 16.839, accuracy: 79.100%\n",
      "[1380] loss: 16.840, accuracy: 78.800%\n",
      "[1381] loss: 16.842, accuracy: 78.700%\n",
      "[1382] loss: 16.840, accuracy: 78.600%\n",
      "[1383] loss: 16.842, accuracy: 78.700%\n",
      "[1384] loss: 16.849, accuracy: 79.200%\n",
      "[1385] loss: 16.840, accuracy: 79.200%\n",
      "[1386] loss: 16.842, accuracy: 78.500%\n",
      "[1387] loss: 16.841, accuracy: 79.000%\n",
      "[1388] loss: 16.843, accuracy: 78.300%\n",
      "[1389] loss: 16.858, accuracy: 77.800%\n",
      "[1390] loss: 17.558, accuracy: 75.500%\n",
      "[1391] loss: 17.800, accuracy: 78.700%\n",
      "[1392] loss: 17.561, accuracy: 78.500%\n",
      "[1393] loss: 17.395, accuracy: 76.200%\n",
      "[1394] loss: 17.387, accuracy: 79.000%\n",
      "[1395] loss: 17.130, accuracy: 78.600%\n",
      "[1396] loss: 17.069, accuracy: 79.000%\n",
      "[1397] loss: 16.978, accuracy: 78.400%\n",
      "[1398] loss: 16.944, accuracy: 78.800%\n",
      "[1399] loss: 16.888, accuracy: 80.200%\n",
      "[1400] loss: 16.869, accuracy: 78.700%\n",
      "[1401] loss: 16.854, accuracy: 79.300%\n",
      "[1402] loss: 16.847, accuracy: 79.800%\n",
      "[1403] loss: 16.848, accuracy: 79.700%\n",
      "[1404] loss: 16.840, accuracy: 79.600%\n",
      "[1405] loss: 16.842, accuracy: 79.600%\n",
      "[1406] loss: 16.842, accuracy: 79.500%\n",
      "[1407] loss: 16.842, accuracy: 79.400%\n",
      "[1408] loss: 16.841, accuracy: 79.500%\n",
      "[1409] loss: 16.839, accuracy: 78.900%\n",
      "[1410] loss: 16.843, accuracy: 79.400%\n",
      "[1411] loss: 16.841, accuracy: 79.000%\n",
      "[1412] loss: 16.839, accuracy: 79.100%\n",
      "[1413] loss: 16.841, accuracy: 79.200%\n",
      "[1414] loss: 16.845, accuracy: 79.000%\n",
      "[1415] loss: 16.841, accuracy: 79.200%\n",
      "[1416] loss: 16.841, accuracy: 79.200%\n",
      "[1417] loss: 16.844, accuracy: 79.100%\n",
      "[1418] loss: 16.839, accuracy: 79.100%\n",
      "[1419] loss: 16.839, accuracy: 79.200%\n",
      "[1420] loss: 16.843, accuracy: 79.200%\n",
      "[1421] loss: 16.839, accuracy: 78.900%\n",
      "[1422] loss: 16.841, accuracy: 79.000%\n",
      "[1423] loss: 16.843, accuracy: 79.100%\n",
      "[1424] loss: 16.841, accuracy: 79.300%\n",
      "[1425] loss: 16.841, accuracy: 78.900%\n",
      "[1426] loss: 16.843, accuracy: 78.800%\n",
      "[1427] loss: 16.846, accuracy: 78.700%\n",
      "[1428] loss: 16.838, accuracy: 79.200%\n",
      "[1429] loss: 16.839, accuracy: 78.900%\n",
      "[1430] loss: 16.843, accuracy: 79.200%\n",
      "[1431] loss: 16.843, accuracy: 78.800%\n",
      "[1432] loss: 16.846, accuracy: 79.100%\n",
      "[1433] loss: 16.841, accuracy: 78.600%\n",
      "[1434] loss: 16.844, accuracy: 78.500%\n",
      "[1435] loss: 16.839, accuracy: 78.700%\n",
      "[1436] loss: 16.844, accuracy: 78.900%\n",
      "[1437] loss: 16.839, accuracy: 79.000%\n",
      "[1438] loss: 16.846, accuracy: 78.700%\n",
      "[1439] loss: 16.847, accuracy: 78.800%\n",
      "[1440] loss: 16.840, accuracy: 78.800%\n",
      "[1441] loss: 16.844, accuracy: 79.200%\n",
      "[1442] loss: 16.841, accuracy: 79.100%\n",
      "[1443] loss: 16.844, accuracy: 78.600%\n",
      "[1444] loss: 16.848, accuracy: 78.300%\n",
      "[1445] loss: 16.847, accuracy: 79.100%\n",
      "[1446] loss: 16.840, accuracy: 78.500%\n",
      "[1447] loss: 16.845, accuracy: 79.200%\n",
      "[1448] loss: 16.849, accuracy: 78.600%\n",
      "[1449] loss: 16.842, accuracy: 78.600%\n",
      "[1450] loss: 16.843, accuracy: 78.400%\n",
      "[1451] loss: 16.849, accuracy: 78.500%\n",
      "[1452] loss: 16.842, accuracy: 78.200%\n",
      "[1453] loss: 16.844, accuracy: 79.000%\n",
      "[1454] loss: 16.846, accuracy: 78.400%\n",
      "[1455] loss: 16.840, accuracy: 78.900%\n",
      "[1456] loss: 16.845, accuracy: 79.300%\n",
      "[1457] loss: 16.848, accuracy: 78.600%\n",
      "[1458] loss: 16.852, accuracy: 78.400%\n",
      "[1459] loss: 16.846, accuracy: 79.000%\n",
      "[1460] loss: 16.846, accuracy: 78.300%\n",
      "[1461] loss: 16.844, accuracy: 78.800%\n",
      "[1462] loss: 16.841, accuracy: 78.900%\n",
      "[1463] loss: 16.845, accuracy: 78.300%\n",
      "[1464] loss: 16.843, accuracy: 78.700%\n",
      "[1465] loss: 16.843, accuracy: 78.500%\n",
      "[1466] loss: 16.851, accuracy: 77.900%\n",
      "[1467] loss: 17.434, accuracy: 76.400%\n",
      "[1468] loss: 17.561, accuracy: 76.300%\n",
      "[1469] loss: 17.610, accuracy: 76.300%\n",
      "[1470] loss: 17.428, accuracy: 77.300%\n",
      "[1471] loss: 17.297, accuracy: 77.700%\n",
      "[1472] loss: 17.140, accuracy: 79.400%\n",
      "[1473] loss: 17.057, accuracy: 80.000%\n",
      "[1474] loss: 16.958, accuracy: 80.000%\n",
      "[1475] loss: 16.916, accuracy: 79.300%\n",
      "[1476] loss: 16.874, accuracy: 78.200%\n",
      "[1477] loss: 16.867, accuracy: 79.100%\n",
      "[1478] loss: 16.857, accuracy: 79.400%\n",
      "[1479] loss: 16.849, accuracy: 79.400%\n",
      "[1480] loss: 16.846, accuracy: 79.600%\n",
      "[1481] loss: 16.843, accuracy: 79.200%\n",
      "[1482] loss: 16.845, accuracy: 79.000%\n",
      "[1483] loss: 16.842, accuracy: 79.100%\n",
      "[1484] loss: 16.847, accuracy: 79.300%\n",
      "[1485] loss: 16.842, accuracy: 79.500%\n",
      "[1486] loss: 16.840, accuracy: 79.400%\n",
      "[1487] loss: 16.839, accuracy: 78.900%\n",
      "[1488] loss: 16.844, accuracy: 78.700%\n",
      "[1489] loss: 16.844, accuracy: 78.900%\n",
      "[1490] loss: 16.841, accuracy: 79.000%\n",
      "[1491] loss: 16.846, accuracy: 79.300%\n",
      "[1492] loss: 16.841, accuracy: 78.900%\n",
      "[1493] loss: 16.841, accuracy: 79.000%\n",
      "[1494] loss: 16.843, accuracy: 79.100%\n",
      "[1495] loss: 16.839, accuracy: 79.500%\n",
      "[1496] loss: 16.841, accuracy: 79.200%\n",
      "[1497] loss: 16.841, accuracy: 78.700%\n",
      "[1498] loss: 16.846, accuracy: 79.300%\n",
      "[1499] loss: 16.842, accuracy: 79.100%\n",
      "[1500] loss: 16.846, accuracy: 79.100%\n",
      "[1501] loss: 16.839, accuracy: 78.700%\n",
      "[1502] loss: 16.844, accuracy: 79.100%\n",
      "[1503] loss: 16.842, accuracy: 78.700%\n",
      "[1504] loss: 16.855, accuracy: 78.700%\n",
      "[1505] loss: 16.853, accuracy: 79.700%\n",
      "[1506] loss: 16.843, accuracy: 79.100%\n",
      "[1507] loss: 16.841, accuracy: 79.500%\n",
      "[1508] loss: 16.839, accuracy: 79.000%\n",
      "[1509] loss: 16.843, accuracy: 79.000%\n",
      "[1510] loss: 16.845, accuracy: 78.900%\n",
      "[1511] loss: 16.840, accuracy: 79.100%\n",
      "[1512] loss: 16.840, accuracy: 78.700%\n",
      "[1513] loss: 16.838, accuracy: 78.600%\n",
      "[1514] loss: 16.838, accuracy: 78.100%\n",
      "[1515] loss: 16.835, accuracy: 78.700%\n",
      "[1516] loss: 16.847, accuracy: 78.300%\n",
      "[1517] loss: 16.842, accuracy: 78.500%\n",
      "[1518] loss: 16.836, accuracy: 78.500%\n",
      "[1519] loss: 16.835, accuracy: 78.600%\n",
      "[1520] loss: 16.838, accuracy: 78.000%\n",
      "[1521] loss: 16.839, accuracy: 78.700%\n",
      "[1522] loss: 16.841, accuracy: 78.500%\n",
      "[1523] loss: 16.839, accuracy: 78.500%\n",
      "[1524] loss: 16.836, accuracy: 78.500%\n",
      "[1525] loss: 16.840, accuracy: 78.400%\n",
      "[1526] loss: 16.840, accuracy: 78.700%\n",
      "[1527] loss: 16.838, accuracy: 78.300%\n",
      "[1528] loss: 16.840, accuracy: 78.100%\n",
      "[1529] loss: 16.836, accuracy: 78.300%\n",
      "[1530] loss: 16.838, accuracy: 78.500%\n",
      "[1531] loss: 16.836, accuracy: 78.700%\n",
      "[1532] loss: 16.843, accuracy: 78.100%\n",
      "[1533] loss: 16.837, accuracy: 78.900%\n",
      "[1534] loss: 16.845, accuracy: 78.300%\n",
      "[1535] loss: 16.836, accuracy: 78.500%\n",
      "[1536] loss: 16.836, accuracy: 78.300%\n",
      "[1537] loss: 16.840, accuracy: 78.200%\n",
      "[1538] loss: 16.841, accuracy: 78.400%\n",
      "[1539] loss: 16.837, accuracy: 78.800%\n",
      "[1540] loss: 16.848, accuracy: 77.800%\n",
      "[1541] loss: 16.839, accuracy: 78.500%\n",
      "[1542] loss: 16.842, accuracy: 78.200%\n",
      "[1543] loss: 16.840, accuracy: 78.500%\n",
      "[1544] loss: 16.837, accuracy: 78.100%\n",
      "[1545] loss: 16.840, accuracy: 78.500%\n",
      "[1546] loss: 16.872, accuracy: 76.100%\n",
      "[1547] loss: 17.264, accuracy: 77.000%\n",
      "[1548] loss: 17.762, accuracy: 73.700%\n",
      "[1549] loss: 17.606, accuracy: 78.300%\n",
      "[1550] loss: 17.502, accuracy: 76.800%\n",
      "[1551] loss: 17.213, accuracy: 79.300%\n",
      "[1552] loss: 17.091, accuracy: 79.000%\n",
      "[1553] loss: 16.975, accuracy: 79.400%\n",
      "[1554] loss: 16.907, accuracy: 79.200%\n",
      "[1555] loss: 16.900, accuracy: 79.400%\n",
      "[1556] loss: 16.880, accuracy: 79.800%\n",
      "[1557] loss: 16.857, accuracy: 79.500%\n",
      "[1558] loss: 16.848, accuracy: 79.700%\n",
      "[1559] loss: 16.839, accuracy: 79.800%\n",
      "[1560] loss: 16.842, accuracy: 79.700%\n",
      "[1561] loss: 16.837, accuracy: 79.600%\n",
      "[1562] loss: 16.841, accuracy: 79.500%\n",
      "[1563] loss: 16.850, accuracy: 79.200%\n",
      "[1564] loss: 16.838, accuracy: 79.400%\n",
      "[1565] loss: 16.838, accuracy: 79.300%\n",
      "[1566] loss: 16.835, accuracy: 79.400%\n",
      "[1567] loss: 16.839, accuracy: 79.300%\n",
      "[1568] loss: 16.837, accuracy: 79.200%\n",
      "[1569] loss: 16.835, accuracy: 79.100%\n",
      "[1570] loss: 16.832, accuracy: 79.600%\n",
      "[1571] loss: 16.841, accuracy: 79.200%\n",
      "[1572] loss: 16.836, accuracy: 79.000%\n",
      "[1573] loss: 16.832, accuracy: 79.100%\n",
      "[1574] loss: 16.834, accuracy: 78.800%\n",
      "[1575] loss: 16.832, accuracy: 78.800%\n",
      "[1576] loss: 16.832, accuracy: 78.600%\n",
      "[1577] loss: 16.834, accuracy: 78.800%\n",
      "[1578] loss: 16.838, accuracy: 79.200%\n",
      "[1579] loss: 16.843, accuracy: 78.900%\n",
      "[1580] loss: 16.832, accuracy: 78.900%\n",
      "[1581] loss: 16.836, accuracy: 79.100%\n",
      "[1582] loss: 16.832, accuracy: 79.000%\n",
      "[1583] loss: 16.831, accuracy: 78.800%\n",
      "[1584] loss: 16.834, accuracy: 78.700%\n",
      "[1585] loss: 16.834, accuracy: 79.000%\n",
      "[1586] loss: 16.832, accuracy: 78.700%\n",
      "[1587] loss: 16.836, accuracy: 78.500%\n",
      "[1588] loss: 16.841, accuracy: 78.500%\n",
      "[1589] loss: 16.832, accuracy: 78.500%\n",
      "[1590] loss: 16.841, accuracy: 78.900%\n",
      "[1591] loss: 16.832, accuracy: 78.700%\n",
      "[1592] loss: 16.832, accuracy: 78.900%\n",
      "[1593] loss: 16.832, accuracy: 78.500%\n",
      "[1594] loss: 16.839, accuracy: 78.500%\n",
      "[1595] loss: 16.834, accuracy: 78.600%\n",
      "[1596] loss: 16.843, accuracy: 78.700%\n",
      "[1597] loss: 16.831, accuracy: 78.300%\n",
      "[1598] loss: 16.836, accuracy: 78.100%\n",
      "[1599] loss: 16.839, accuracy: 78.500%\n",
      "[1600] loss: 16.837, accuracy: 78.400%\n",
      "[1601] loss: 16.836, accuracy: 78.800%\n",
      "[1602] loss: 16.834, accuracy: 78.500%\n",
      "[1603] loss: 16.832, accuracy: 77.700%\n",
      "[1604] loss: 16.834, accuracy: 78.700%\n",
      "[1605] loss: 16.834, accuracy: 78.600%\n",
      "[1606] loss: 16.835, accuracy: 78.400%\n",
      "[1607] loss: 16.837, accuracy: 78.400%\n",
      "[1608] loss: 16.832, accuracy: 78.600%\n",
      "[1609] loss: 16.835, accuracy: 78.100%\n",
      "[1610] loss: 16.835, accuracy: 77.900%\n",
      "[1611] loss: 16.835, accuracy: 78.600%\n",
      "[1612] loss: 16.833, accuracy: 78.700%\n",
      "[1613] loss: 16.835, accuracy: 78.600%\n",
      "[1614] loss: 16.834, accuracy: 79.200%\n",
      "[1615] loss: 16.835, accuracy: 77.800%\n",
      "[1616] loss: 16.840, accuracy: 79.100%\n",
      "[1617] loss: 16.838, accuracy: 78.300%\n",
      "[1618] loss: 16.838, accuracy: 78.900%\n",
      "[1619] loss: 16.833, accuracy: 78.400%\n",
      "[1620] loss: 16.836, accuracy: 78.100%\n",
      "[1621] loss: 16.862, accuracy: 77.500%\n",
      "[1622] loss: 17.200, accuracy: 76.700%\n",
      "[1623] loss: 17.546, accuracy: 78.100%\n",
      "[1624] loss: 17.642, accuracy: 76.700%\n",
      "[1625] loss: 17.646, accuracy: 78.600%\n",
      "[1626] loss: 17.466, accuracy: 77.800%\n",
      "[1627] loss: 17.318, accuracy: 79.100%\n",
      "[1628] loss: 17.115, accuracy: 80.000%\n",
      "[1629] loss: 17.014, accuracy: 80.500%\n",
      "[1630] loss: 16.926, accuracy: 81.000%\n",
      "[1631] loss: 16.876, accuracy: 79.600%\n",
      "[1632] loss: 16.853, accuracy: 79.600%\n",
      "[1633] loss: 16.867, accuracy: 79.400%\n",
      "[1634] loss: 16.845, accuracy: 79.500%\n",
      "[1635] loss: 16.837, accuracy: 79.300%\n",
      "[1636] loss: 16.830, accuracy: 79.400%\n",
      "[1637] loss: 16.833, accuracy: 79.500%\n",
      "[1638] loss: 16.835, accuracy: 79.600%\n",
      "[1639] loss: 16.830, accuracy: 79.500%\n",
      "[1640] loss: 16.830, accuracy: 79.300%\n",
      "[1641] loss: 16.828, accuracy: 79.500%\n",
      "[1642] loss: 16.832, accuracy: 79.500%\n",
      "[1643] loss: 16.830, accuracy: 79.500%\n",
      "[1644] loss: 16.832, accuracy: 79.700%\n",
      "[1645] loss: 16.828, accuracy: 79.900%\n",
      "[1646] loss: 16.832, accuracy: 79.700%\n",
      "[1647] loss: 16.832, accuracy: 79.600%\n",
      "[1648] loss: 16.830, accuracy: 79.500%\n",
      "[1649] loss: 16.834, accuracy: 79.600%\n",
      "[1650] loss: 16.832, accuracy: 79.400%\n",
      "[1651] loss: 16.834, accuracy: 79.900%\n",
      "[1652] loss: 16.830, accuracy: 79.600%\n",
      "[1653] loss: 16.830, accuracy: 79.700%\n",
      "[1654] loss: 16.830, accuracy: 79.600%\n",
      "[1655] loss: 16.827, accuracy: 79.800%\n",
      "[1656] loss: 16.830, accuracy: 79.700%\n",
      "[1657] loss: 16.827, accuracy: 79.300%\n",
      "[1658] loss: 16.830, accuracy: 79.300%\n",
      "[1659] loss: 16.830, accuracy: 79.500%\n",
      "[1660] loss: 16.827, accuracy: 79.700%\n",
      "[1661] loss: 16.829, accuracy: 79.400%\n",
      "[1662] loss: 16.828, accuracy: 79.700%\n",
      "[1663] loss: 16.830, accuracy: 79.700%\n",
      "[1664] loss: 16.832, accuracy: 79.500%\n",
      "[1665] loss: 16.830, accuracy: 79.300%\n",
      "[1666] loss: 16.828, accuracy: 79.200%\n",
      "[1667] loss: 16.827, accuracy: 79.600%\n",
      "[1668] loss: 16.832, accuracy: 79.500%\n",
      "[1669] loss: 16.837, accuracy: 79.400%\n",
      "[1670] loss: 16.828, accuracy: 79.200%\n",
      "[1671] loss: 16.835, accuracy: 79.000%\n",
      "[1672] loss: 16.828, accuracy: 79.900%\n",
      "[1673] loss: 16.830, accuracy: 79.500%\n",
      "[1674] loss: 16.833, accuracy: 78.800%\n",
      "[1675] loss: 16.833, accuracy: 79.400%\n",
      "[1676] loss: 16.833, accuracy: 78.900%\n",
      "[1677] loss: 16.828, accuracy: 79.500%\n",
      "[1678] loss: 16.828, accuracy: 79.400%\n",
      "[1679] loss: 16.833, accuracy: 78.900%\n",
      "[1680] loss: 16.828, accuracy: 78.900%\n",
      "[1681] loss: 16.830, accuracy: 79.500%\n",
      "[1682] loss: 16.837, accuracy: 79.500%\n",
      "[1683] loss: 16.834, accuracy: 78.200%\n",
      "[1684] loss: 16.833, accuracy: 79.100%\n",
      "[1685] loss: 16.831, accuracy: 78.700%\n",
      "[1686] loss: 16.829, accuracy: 79.000%\n",
      "[1687] loss: 16.833, accuracy: 78.600%\n",
      "[1688] loss: 16.838, accuracy: 78.400%\n",
      "[1689] loss: 16.834, accuracy: 78.600%\n",
      "[1690] loss: 16.829, accuracy: 78.300%\n",
      "[1691] loss: 16.835, accuracy: 78.800%\n",
      "[1692] loss: 16.833, accuracy: 78.700%\n",
      "[1693] loss: 16.836, accuracy: 78.000%\n",
      "[1694] loss: 16.829, accuracy: 78.400%\n",
      "[1695] loss: 16.838, accuracy: 78.200%\n",
      "[1696] loss: 16.836, accuracy: 78.800%\n",
      "[1697] loss: 16.834, accuracy: 78.700%\n",
      "[1698] loss: 16.831, accuracy: 78.900%\n",
      "[1699] loss: 16.833, accuracy: 78.800%\n",
      "[1700] loss: 16.831, accuracy: 78.700%\n",
      "[1701] loss: 16.833, accuracy: 78.700%\n",
      "[1702] loss: 16.835, accuracy: 78.400%\n",
      "[1703] loss: 16.831, accuracy: 78.600%\n",
      "[1704] loss: 16.836, accuracy: 78.300%\n",
      "[1705] loss: 16.832, accuracy: 78.000%\n",
      "[1706] loss: 16.839, accuracy: 79.600%\n",
      "[1707] loss: 16.831, accuracy: 79.200%\n",
      "[1708] loss: 16.836, accuracy: 79.000%\n",
      "[1709] loss: 17.170, accuracy: 75.900%\n",
      "[1710] loss: 17.698, accuracy: 74.200%\n",
      "[1711] loss: 17.669, accuracy: 77.100%\n",
      "[1712] loss: 17.552, accuracy: 77.900%\n",
      "[1713] loss: 17.378, accuracy: 76.700%\n",
      "[1714] loss: 17.255, accuracy: 78.700%\n",
      "[1715] loss: 17.073, accuracy: 79.500%\n",
      "[1716] loss: 16.975, accuracy: 78.300%\n",
      "[1717] loss: 16.929, accuracy: 80.300%\n",
      "[1718] loss: 16.886, accuracy: 80.400%\n",
      "[1719] loss: 16.856, accuracy: 79.700%\n",
      "[1720] loss: 16.834, accuracy: 80.200%\n",
      "[1721] loss: 16.834, accuracy: 80.300%\n",
      "[1722] loss: 16.834, accuracy: 80.200%\n",
      "[1723] loss: 16.828, accuracy: 80.400%\n",
      "[1724] loss: 16.828, accuracy: 80.100%\n",
      "[1725] loss: 16.835, accuracy: 80.000%\n",
      "[1726] loss: 16.830, accuracy: 80.200%\n",
      "[1727] loss: 16.832, accuracy: 79.700%\n",
      "[1728] loss: 16.830, accuracy: 80.200%\n",
      "[1729] loss: 16.832, accuracy: 80.000%\n",
      "[1730] loss: 16.830, accuracy: 79.700%\n",
      "[1731] loss: 16.827, accuracy: 80.300%\n",
      "[1732] loss: 16.832, accuracy: 80.100%\n",
      "[1733] loss: 16.832, accuracy: 80.000%\n",
      "[1734] loss: 16.832, accuracy: 80.000%\n",
      "[1735] loss: 16.832, accuracy: 79.700%\n",
      "[1736] loss: 16.827, accuracy: 80.300%\n",
      "[1737] loss: 16.829, accuracy: 80.100%\n",
      "[1738] loss: 16.832, accuracy: 80.200%\n",
      "[1739] loss: 16.827, accuracy: 80.200%\n",
      "[1740] loss: 16.827, accuracy: 79.600%\n",
      "[1741] loss: 16.832, accuracy: 80.000%\n",
      "[1742] loss: 16.832, accuracy: 80.100%\n",
      "[1743] loss: 16.831, accuracy: 79.900%\n",
      "[1744] loss: 16.832, accuracy: 80.000%\n",
      "[1745] loss: 16.837, accuracy: 79.600%\n",
      "[1746] loss: 16.832, accuracy: 79.800%\n",
      "[1747] loss: 16.827, accuracy: 79.400%\n",
      "[1748] loss: 16.830, accuracy: 79.600%\n",
      "[1749] loss: 16.827, accuracy: 79.700%\n",
      "[1750] loss: 16.829, accuracy: 79.400%\n",
      "[1751] loss: 16.829, accuracy: 79.300%\n",
      "[1752] loss: 16.832, accuracy: 79.700%\n",
      "[1753] loss: 16.829, accuracy: 79.300%\n",
      "[1754] loss: 16.827, accuracy: 79.400%\n",
      "[1755] loss: 16.829, accuracy: 79.200%\n",
      "[1756] loss: 16.827, accuracy: 79.100%\n",
      "[1757] loss: 16.830, accuracy: 78.900%\n",
      "[1758] loss: 16.830, accuracy: 79.000%\n",
      "[1759] loss: 16.830, accuracy: 79.100%\n",
      "[1760] loss: 16.832, accuracy: 79.100%\n",
      "[1761] loss: 16.830, accuracy: 79.100%\n",
      "[1762] loss: 16.828, accuracy: 79.300%\n",
      "[1763] loss: 16.830, accuracy: 78.800%\n",
      "[1764] loss: 16.831, accuracy: 79.300%\n",
      "[1765] loss: 16.835, accuracy: 79.200%\n",
      "[1766] loss: 16.832, accuracy: 78.700%\n",
      "[1767] loss: 16.828, accuracy: 78.900%\n",
      "[1768] loss: 16.828, accuracy: 79.200%\n",
      "[1769] loss: 16.831, accuracy: 79.100%\n",
      "[1770] loss: 16.832, accuracy: 78.900%\n",
      "[1771] loss: 16.839, accuracy: 79.400%\n",
      "[1772] loss: 16.848, accuracy: 79.300%\n",
      "[1773] loss: 16.847, accuracy: 79.300%\n",
      "[1774] loss: 16.884, accuracy: 77.700%\n",
      "[1775] loss: 16.916, accuracy: 78.500%\n",
      "[1776] loss: 17.048, accuracy: 78.100%\n",
      "[1777] loss: 17.086, accuracy: 79.100%\n",
      "[1778] loss: 17.101, accuracy: 78.800%\n",
      "[1779] loss: 17.070, accuracy: 77.900%\n",
      "[1780] loss: 17.111, accuracy: 79.200%\n",
      "[1781] loss: 17.034, accuracy: 78.700%\n",
      "[1782] loss: 16.974, accuracy: 78.900%\n",
      "[1783] loss: 16.971, accuracy: 78.700%\n",
      "[1784] loss: 16.918, accuracy: 80.300%\n",
      "[1785] loss: 16.893, accuracy: 79.200%\n",
      "[1786] loss: 16.868, accuracy: 80.200%\n",
      "[1787] loss: 16.860, accuracy: 79.500%\n",
      "[1788] loss: 16.843, accuracy: 79.100%\n",
      "[1789] loss: 16.839, accuracy: 79.300%\n",
      "[1790] loss: 16.836, accuracy: 79.300%\n",
      "[1791] loss: 16.830, accuracy: 79.300%\n",
      "[1792] loss: 16.830, accuracy: 79.300%\n",
      "[1793] loss: 16.831, accuracy: 79.400%\n",
      "[1794] loss: 16.829, accuracy: 79.500%\n",
      "[1795] loss: 16.833, accuracy: 79.500%\n",
      "[1796] loss: 16.827, accuracy: 79.300%\n",
      "[1797] loss: 16.826, accuracy: 79.700%\n",
      "[1798] loss: 16.827, accuracy: 80.100%\n",
      "[1799] loss: 16.833, accuracy: 79.900%\n",
      "[1800] loss: 16.829, accuracy: 79.700%\n",
      "[1801] loss: 16.826, accuracy: 80.300%\n",
      "[1802] loss: 16.833, accuracy: 79.500%\n",
      "[1803] loss: 16.827, accuracy: 79.600%\n",
      "[1804] loss: 16.829, accuracy: 79.600%\n",
      "[1805] loss: 16.832, accuracy: 79.700%\n",
      "[1806] loss: 16.829, accuracy: 79.600%\n",
      "[1807] loss: 16.827, accuracy: 80.100%\n",
      "[1808] loss: 16.831, accuracy: 79.200%\n",
      "[1809] loss: 16.832, accuracy: 80.000%\n",
      "[1810] loss: 16.827, accuracy: 79.300%\n",
      "[1811] loss: 16.832, accuracy: 79.700%\n",
      "[1812] loss: 16.828, accuracy: 79.700%\n",
      "[1813] loss: 16.827, accuracy: 79.300%\n",
      "[1814] loss: 16.827, accuracy: 79.700%\n",
      "[1815] loss: 16.834, accuracy: 79.900%\n",
      "[1816] loss: 16.828, accuracy: 78.900%\n",
      "[1817] loss: 16.832, accuracy: 79.300%\n",
      "[1818] loss: 16.829, accuracy: 79.000%\n",
      "[1819] loss: 16.832, accuracy: 79.000%\n",
      "[1820] loss: 16.827, accuracy: 79.300%\n",
      "[1821] loss: 16.827, accuracy: 79.100%\n",
      "[1822] loss: 16.829, accuracy: 78.900%\n",
      "[1823] loss: 16.834, accuracy: 78.700%\n",
      "[1824] loss: 16.830, accuracy: 79.100%\n",
      "[1825] loss: 16.832, accuracy: 78.400%\n",
      "[1826] loss: 16.828, accuracy: 78.400%\n",
      "[1827] loss: 16.828, accuracy: 79.100%\n",
      "[1828] loss: 16.830, accuracy: 79.100%\n",
      "[1829] loss: 16.833, accuracy: 79.000%\n",
      "[1830] loss: 16.828, accuracy: 78.500%\n",
      "[1831] loss: 16.830, accuracy: 78.700%\n",
      "[1832] loss: 16.830, accuracy: 79.000%\n",
      "[1833] loss: 16.832, accuracy: 79.300%\n",
      "[1834] loss: 16.830, accuracy: 78.500%\n",
      "[1835] loss: 16.828, accuracy: 78.700%\n",
      "[1836] loss: 16.828, accuracy: 79.100%\n",
      "[1837] loss: 16.832, accuracy: 78.700%\n",
      "[1838] loss: 16.831, accuracy: 78.900%\n",
      "[1839] loss: 16.833, accuracy: 78.600%\n",
      "[1840] loss: 16.837, accuracy: 78.000%\n",
      "[1841] loss: 16.836, accuracy: 78.100%\n",
      "[1842] loss: 17.081, accuracy: 76.000%\n",
      "[1843] loss: 17.377, accuracy: 75.000%\n",
      "[1844] loss: 17.378, accuracy: 78.700%\n",
      "[1845] loss: 17.338, accuracy: 78.800%\n",
      "[1846] loss: 17.134, accuracy: 80.300%\n",
      "[1847] loss: 17.068, accuracy: 78.500%\n",
      "[1848] loss: 17.075, accuracy: 79.500%\n",
      "[1849] loss: 16.981, accuracy: 78.400%\n",
      "[1850] loss: 16.931, accuracy: 78.100%\n",
      "[1851] loss: 16.910, accuracy: 79.000%\n",
      "[1852] loss: 16.872, accuracy: 79.400%\n",
      "[1853] loss: 16.846, accuracy: 79.600%\n",
      "[1854] loss: 16.840, accuracy: 79.400%\n",
      "[1855] loss: 16.830, accuracy: 79.200%\n",
      "[1856] loss: 16.829, accuracy: 79.400%\n",
      "[1857] loss: 16.828, accuracy: 79.300%\n",
      "[1858] loss: 16.826, accuracy: 79.500%\n",
      "[1859] loss: 16.826, accuracy: 79.100%\n",
      "[1860] loss: 16.828, accuracy: 79.100%\n",
      "[1861] loss: 16.823, accuracy: 79.300%\n",
      "[1862] loss: 16.823, accuracy: 79.200%\n",
      "[1863] loss: 16.823, accuracy: 79.400%\n",
      "[1864] loss: 16.822, accuracy: 79.300%\n",
      "[1865] loss: 16.827, accuracy: 79.200%\n",
      "[1866] loss: 16.827, accuracy: 79.600%\n",
      "[1867] loss: 16.823, accuracy: 79.800%\n",
      "[1868] loss: 16.828, accuracy: 79.800%\n",
      "[1869] loss: 16.825, accuracy: 79.800%\n",
      "[1870] loss: 16.827, accuracy: 79.600%\n",
      "[1871] loss: 16.823, accuracy: 79.800%\n",
      "[1872] loss: 16.825, accuracy: 79.800%\n",
      "[1873] loss: 16.827, accuracy: 79.800%\n",
      "[1874] loss: 16.825, accuracy: 79.900%\n",
      "[1875] loss: 16.823, accuracy: 79.500%\n",
      "[1876] loss: 16.825, accuracy: 79.700%\n",
      "[1877] loss: 16.828, accuracy: 79.900%\n",
      "[1878] loss: 16.825, accuracy: 79.600%\n",
      "[1879] loss: 16.836, accuracy: 78.900%\n",
      "[1880] loss: 16.825, accuracy: 79.400%\n",
      "[1881] loss: 16.829, accuracy: 79.200%\n",
      "[1882] loss: 16.825, accuracy: 79.300%\n",
      "[1883] loss: 16.823, accuracy: 79.200%\n",
      "[1884] loss: 16.825, accuracy: 79.300%\n",
      "[1885] loss: 16.823, accuracy: 78.900%\n",
      "[1886] loss: 16.823, accuracy: 79.200%\n",
      "[1887] loss: 16.825, accuracy: 79.000%\n",
      "[1888] loss: 16.823, accuracy: 79.400%\n",
      "[1889] loss: 16.826, accuracy: 78.800%\n",
      "[1890] loss: 16.823, accuracy: 79.200%\n",
      "[1891] loss: 16.823, accuracy: 78.700%\n",
      "[1892] loss: 16.823, accuracy: 79.200%\n",
      "[1893] loss: 16.828, accuracy: 79.300%\n",
      "[1894] loss: 16.828, accuracy: 78.700%\n",
      "[1895] loss: 16.832, accuracy: 78.000%\n",
      "[1896] loss: 16.831, accuracy: 78.400%\n",
      "[1897] loss: 16.829, accuracy: 78.700%\n",
      "[1898] loss: 16.823, accuracy: 79.500%\n",
      "[1899] loss: 16.828, accuracy: 78.700%\n",
      "[1900] loss: 16.822, accuracy: 79.200%\n",
      "[1901] loss: 16.822, accuracy: 79.100%\n",
      "[1902] loss: 16.819, accuracy: 79.100%\n",
      "[1903] loss: 16.822, accuracy: 79.100%\n",
      "[1904] loss: 16.826, accuracy: 79.200%\n",
      "[1905] loss: 16.824, accuracy: 78.900%\n",
      "[1906] loss: 16.824, accuracy: 79.100%\n",
      "[1907] loss: 16.820, accuracy: 78.600%\n",
      "[1908] loss: 16.820, accuracy: 79.300%\n",
      "[1909] loss: 16.822, accuracy: 78.500%\n",
      "[1910] loss: 16.822, accuracy: 78.500%\n",
      "[1911] loss: 16.827, accuracy: 78.600%\n",
      "[1912] loss: 16.822, accuracy: 78.800%\n",
      "[1913] loss: 16.820, accuracy: 78.300%\n",
      "[1914] loss: 16.823, accuracy: 78.800%\n",
      "[1915] loss: 16.828, accuracy: 78.500%\n",
      "[1916] loss: 16.830, accuracy: 78.300%\n",
      "[1917] loss: 16.821, accuracy: 78.500%\n",
      "[1918] loss: 16.827, accuracy: 78.800%\n",
      "[1919] loss: 16.823, accuracy: 79.100%\n",
      "[1920] loss: 16.821, accuracy: 78.600%\n",
      "[1921] loss: 16.821, accuracy: 77.800%\n",
      "[1922] loss: 16.827, accuracy: 78.300%\n",
      "[1923] loss: 16.823, accuracy: 78.700%\n",
      "[1924] loss: 16.964, accuracy: 75.100%\n",
      "[1925] loss: 17.412, accuracy: 77.700%\n",
      "[1926] loss: 17.479, accuracy: 76.800%\n",
      "[1927] loss: 17.464, accuracy: 76.700%\n",
      "[1928] loss: 17.268, accuracy: 78.200%\n",
      "[1929] loss: 17.121, accuracy: 78.100%\n",
      "[1930] loss: 17.033, accuracy: 77.900%\n",
      "[1931] loss: 16.983, accuracy: 77.800%\n",
      "[1932] loss: 16.903, accuracy: 79.300%\n",
      "[1933] loss: 16.866, accuracy: 79.800%\n",
      "[1934] loss: 16.870, accuracy: 79.300%\n",
      "[1935] loss: 16.851, accuracy: 79.900%\n",
      "[1936] loss: 16.835, accuracy: 79.200%\n",
      "[1937] loss: 16.834, accuracy: 79.100%\n",
      "[1938] loss: 16.831, accuracy: 79.200%\n",
      "[1939] loss: 16.828, accuracy: 79.000%\n",
      "[1940] loss: 16.829, accuracy: 78.900%\n",
      "[1941] loss: 16.828, accuracy: 78.900%\n",
      "[1942] loss: 16.830, accuracy: 79.300%\n",
      "[1943] loss: 16.826, accuracy: 79.000%\n",
      "[1944] loss: 16.826, accuracy: 79.200%\n",
      "[1945] loss: 16.828, accuracy: 78.800%\n",
      "[1946] loss: 16.823, accuracy: 79.200%\n",
      "[1947] loss: 16.827, accuracy: 79.000%\n",
      "[1948] loss: 16.823, accuracy: 79.100%\n",
      "[1949] loss: 16.828, accuracy: 78.900%\n",
      "[1950] loss: 16.827, accuracy: 78.800%\n",
      "[1951] loss: 16.825, accuracy: 79.000%\n",
      "[1952] loss: 16.832, accuracy: 78.900%\n",
      "[1953] loss: 16.830, accuracy: 78.900%\n",
      "[1954] loss: 16.823, accuracy: 78.700%\n",
      "[1955] loss: 16.825, accuracy: 79.000%\n",
      "[1956] loss: 16.828, accuracy: 79.000%\n",
      "[1957] loss: 16.825, accuracy: 78.800%\n",
      "[1958] loss: 16.825, accuracy: 78.600%\n",
      "[1959] loss: 16.828, accuracy: 78.700%\n",
      "[1960] loss: 16.832, accuracy: 78.600%\n",
      "[1961] loss: 16.825, accuracy: 79.100%\n",
      "[1962] loss: 16.825, accuracy: 78.800%\n",
      "[1963] loss: 16.823, accuracy: 78.700%\n",
      "[1964] loss: 16.825, accuracy: 78.600%\n",
      "[1965] loss: 16.825, accuracy: 78.900%\n",
      "[1966] loss: 16.822, accuracy: 78.600%\n",
      "[1967] loss: 16.821, accuracy: 77.900%\n",
      "[1968] loss: 16.830, accuracy: 79.100%\n",
      "[1969] loss: 16.825, accuracy: 79.000%\n",
      "[1970] loss: 16.825, accuracy: 79.000%\n",
      "[1971] loss: 16.820, accuracy: 78.700%\n",
      "[1972] loss: 16.819, accuracy: 78.400%\n",
      "[1973] loss: 16.822, accuracy: 78.300%\n",
      "[1974] loss: 16.826, accuracy: 78.200%\n",
      "[1975] loss: 16.822, accuracy: 78.500%\n",
      "[1976] loss: 16.821, accuracy: 78.400%\n",
      "[1977] loss: 16.820, accuracy: 77.800%\n",
      "[1978] loss: 16.824, accuracy: 78.100%\n",
      "[1979] loss: 16.827, accuracy: 78.200%\n",
      "[1980] loss: 16.819, accuracy: 78.400%\n",
      "[1981] loss: 16.820, accuracy: 78.100%\n",
      "[1982] loss: 16.824, accuracy: 78.000%\n",
      "[1983] loss: 16.820, accuracy: 78.100%\n",
      "[1984] loss: 16.822, accuracy: 78.300%\n",
      "[1985] loss: 16.822, accuracy: 78.000%\n",
      "[1986] loss: 16.822, accuracy: 78.200%\n",
      "[1987] loss: 16.820, accuracy: 78.500%\n",
      "[1988] loss: 16.824, accuracy: 78.400%\n",
      "[1989] loss: 16.820, accuracy: 77.600%\n",
      "[1990] loss: 16.820, accuracy: 78.000%\n",
      "[1991] loss: 16.822, accuracy: 78.400%\n",
      "[1992] loss: 16.820, accuracy: 78.800%\n",
      "[1993] loss: 16.820, accuracy: 77.600%\n",
      "[1994] loss: 16.821, accuracy: 77.900%\n",
      "[1995] loss: 16.829, accuracy: 77.600%\n",
      "[1996] loss: 16.823, accuracy: 78.000%\n",
      "[1997] loss: 16.820, accuracy: 78.200%\n",
      "[1998] loss: 16.820, accuracy: 77.800%\n",
      "[1999] loss: 16.819, accuracy: 77.500%\n",
      "[2000] loss: 16.825, accuracy: 78.000%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #print(\"inputs.shape : \", inputs.shape)   # (batch size, embedding_size) \n",
    "        #print(\"labels.shape : \", labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).to(gpudevice)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate accuracy on validation set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #print(\"val-inputs.shape : \", inputs.shape)   # (batch size, embedding_size) \n",
    "            #print(\"val-labels.shape : \", labels.shape)\n",
    "            \n",
    "            outputs = model(inputs).to(gpudevice)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            \n",
    "            #print(predicted)\n",
    "            #print(predicted.shape)\n",
    "            \n",
    "            \n",
    "    \n",
    "            #print(\"predicted.shape : \", predicted.shape)\n",
    "            #print(\"labels.shape : \", labels.shape)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print('[%d] loss: %.3f, accuracy: %.3f%%' % (epoch + 1, running_loss, accuracy))\n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name, param.data)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21bd6233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:10:59.803797Z",
     "iopub.status.busy": "2023-04-30T12:10:59.802916Z",
     "iopub.status.idle": "2023-04-30T12:10:59.826107Z",
     "shell.execute_reply": "2023-04-30T12:10:59.825016Z"
    },
    "papermill": {
     "duration": 0.142982,
     "end_time": "2023-04-30T12:10:59.828240",
     "exception": false,
     "start_time": "2023-04-30T12:10:59.685258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 3, 4, 1, 3, 5, 1, 5, 1, 1, 1, 4, 3, 0, 0, 1, 4, 3, 0, 0, 1, 1, 1, 0,\n",
       "        5, 0, 3, 5, 0, 5, 3, 1, 3, 5, 3, 5, 1, 5, 5, 1, 5, 1, 0, 3, 3, 5, 1, 1,\n",
       "        1, 3, 4, 3, 1, 4, 3, 0, 1, 1, 5, 5, 4, 1, 0, 0, 1, 3, 3, 2, 4, 5, 3, 4,\n",
       "        4, 3, 0, 5, 5, 4, 1, 3, 0, 0, 1, 1, 3, 3, 1, 1, 0, 2, 0, 1, 5, 0, 1, 0,\n",
       "        1, 0, 3, 3, 1, 4, 0, 5, 3, 0, 2, 0, 0, 5, 4, 3, 3, 3, 3, 1, 1, 1, 3, 4,\n",
       "        0, 3, 1, 3, 5, 1, 0, 0, 3, 4, 3, 4, 1, 1, 0, 5, 0, 1, 0, 5, 1, 5, 5, 0,\n",
       "        3, 4, 0, 4, 2, 1, 1, 1, 4, 5, 4, 5, 1, 1, 5, 1, 5, 3, 0, 5, 5, 1, 3, 5,\n",
       "        1, 3, 1, 3, 0, 4, 4, 1, 0, 4, 5, 0, 1, 4, 0, 1, 3, 1, 1, 0, 0, 5, 1, 0,\n",
       "        4, 3, 0, 5, 3, 5, 0, 1, 3, 0, 0, 3, 0, 0, 1, 3, 4, 0, 1, 0, 3, 1, 3, 5,\n",
       "        4, 5, 4, 5, 0, 3, 1, 1, 4, 1, 1, 5, 4, 0, 1, 1, 0, 1, 4, 3, 1, 3, 3, 3,\n",
       "        5, 1, 3, 3, 0, 2, 4, 3, 5, 3, 5, 3, 1, 4, 1, 1, 1, 0, 4, 1, 4, 0, 3, 3,\n",
       "        5, 4, 1, 0, 0, 3, 0, 1, 3, 1, 4, 4, 0, 3, 4, 0, 3, 3, 1, 1, 0, 3, 5, 5,\n",
       "        0, 4, 1, 0, 0, 1, 5, 1, 2, 5, 0, 1, 1, 5, 1, 3, 0, 1, 4, 0, 3, 3, 4, 4,\n",
       "        1, 5, 4, 5, 1, 0, 0, 0, 5, 0, 5, 1, 0, 0, 0, 3, 3, 0, 0, 0, 5, 4, 0, 1,\n",
       "        1, 3, 3, 5, 3, 1, 1, 1, 3, 3, 4, 0, 0, 3, 0, 5, 0, 5, 1, 3, 3, 1, 0, 5,\n",
       "        4, 1, 5, 3, 0, 5, 3, 1, 1, 5, 3, 1, 3, 1, 1, 1, 0, 0, 4, 4, 4, 1, 5, 5,\n",
       "        1, 5, 0, 1, 3, 0, 5, 3, 0, 0, 4, 5, 1, 4, 0, 1, 5, 0, 4, 5, 4, 3, 3, 4,\n",
       "        0, 1, 3, 4, 3, 3, 5, 3, 5, 0, 3, 5, 4, 3, 0, 5, 0, 5, 2, 4, 1, 4, 3, 5,\n",
       "        1, 0, 3, 3, 3, 0, 0, 0, 5, 0, 0, 3, 0, 3, 0, 0, 0, 3, 1, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(gpudevice)\n",
    "with torch.no_grad():\n",
    "    output = model(X_test_tensor)\n",
    "    _, predicted_labels = torch.max(output, 1)\n",
    "    \n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ec106a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:11:00.054666Z",
     "iopub.status.busy": "2023-04-30T12:11:00.053596Z",
     "iopub.status.idle": "2023-04-30T12:11:00.076502Z",
     "shell.execute_reply": "2023-04-30T12:11:00.075350Z"
    },
    "papermill": {
     "duration": 0.138262,
     "end_time": "2023-04-30T12:11:00.079034",
     "exception": false,
     "start_time": "2023-04-30T12:10:59.940772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>S447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>S448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>S449</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>S450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>S451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  label\n",
       "0      S0      5\n",
       "1      S1      3\n",
       "2      S2      4\n",
       "3      S3      1\n",
       "4      S4      3\n",
       "..    ...    ...\n",
       "447  S447      0\n",
       "448  S448      0\n",
       "449  S449      3\n",
       "450  S450      1\n",
       "451  S451      0\n",
       "\n",
       "[452 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "index_list = []\n",
    "for i in range(0,len(predicted_labels)):\n",
    "    index_list.append(f\"S{i}\")\n",
    "    \n",
    "prediction = pd.DataFrame(columns=['ID', 'label'])\n",
    "\n",
    "prediction[\"ID\"] = index_list\n",
    "prediction[\"label\"] = predicted_labels\n",
    "\n",
    "prediction = prediction.reset_index(drop=True)\n",
    "\n",
    "prediction.to_csv('20221119_하준서_sent_class.pred.csv', index = False)\n",
    "\n",
    "#index_list\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a1c24",
   "metadata": {
    "papermill": {
     "duration": 0.114002,
     "end_time": "2023-04-30T12:11:00.307641",
     "exception": false,
     "start_time": "2023-04-30T12:11:00.193639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9c431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T11:56:11.223817Z",
     "iopub.status.busy": "2023-04-20T11:56:11.222734Z",
     "iopub.status.idle": "2023-04-20T11:56:11.306870Z",
     "shell.execute_reply": "2023-04-20T11:56:11.305273Z",
     "shell.execute_reply.started": "2023-04-20T11:56:11.223775Z"
    },
    "papermill": {
     "duration": 0.111161,
     "end_time": "2023-04-30T12:11:00.530927",
     "exception": false,
     "start_time": "2023-04-30T12:11:00.419766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# 데이터를 Tensor로 변환\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# 데이터를 train set과 validation set으로 나눔\n",
    "n_samples = X_train.shape[0]\n",
    "val_size = int(0.1 * n_samples)  # 20%를 validation set으로 사용\n",
    "train_size = n_samples - val_size\n",
    "\n",
    "train_indices, val_indices = np.split(np.arange(n_samples), [train_size])\n",
    "\n",
    "train_data = data_utils.TensorDataset(X_tensor[train_indices], y_tensor[train_indices])\n",
    "val_data = data_utils.TensorDataset(X_tensor[val_indices], y_tensor[val_indices])\n",
    "\n",
    "trainloader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 모델 구현\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.rnn1 = nn.RNN(embedding_dim*20, hidden_dim, batch_first = True)\n",
    "        self.rnn2 = nn.RNN(hidden_dim, hidden_dim, batch_first = True)\n",
    "        self.rnn3 = nn.RNN(hidden_dim, hidden_dim, batch_first = True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # zero vector로 초기 상태 설정\n",
    "        #self.zero_state = torch.zeros(1, hidden_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        flatten = self.flatten(embedded)\n",
    "        \n",
    "        rnn_output1, hidden = self.rnn1(flatten)#, self.zero_state)\n",
    "        rnn_output2, hidden = self.rnn2(rnn_output1)#, self.zero_state)\n",
    "        rnn_output3, hidden = self.rnn3(rnn_output2)#, self.zero_state)\n",
    "        \n",
    "        norm = self.layer_norm(rnn_output3)\n",
    "\n",
    "\n",
    "        #hidden = torch.tanh(output)\n",
    "        fc_output = self.fc(norm)\n",
    "        softmax_output = self.log_softmax(fc_output)\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        \n",
    "\n",
    "        \n",
    "        return fc_output\n",
    "\n",
    "model = RNN(num_embeddings=len(dic),embedding_dim = 300, hidden_dim=512, output_dim=y_train.shape[1])\n",
    "#print(model.state_dict())     ### 초기 가중치 확인\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0001, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b149eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T11:56:11.309118Z",
     "iopub.status.busy": "2023-04-20T11:56:11.308702Z",
     "iopub.status.idle": "2023-04-20T11:56:11.372218Z",
     "shell.execute_reply": "2023-04-20T11:56:11.370575Z",
     "shell.execute_reply.started": "2023-04-20T11:56:11.309068Z"
    },
    "papermill": {
     "duration": 0.117137,
     "end_time": "2023-04-30T12:11:00.771048",
     "exception": false,
     "start_time": "2023-04-30T12:11:00.653911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        #print(inputs.shape)\n",
    "        #print(labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate accuracy on validation set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)          # convert word to id\n",
    "            #print(predicted)\n",
    "            #print(labels)\n",
    "            #print(torch.max(labels, 1))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print('[%d] loss: %.3f, accuracy: %.3f%%' % (epoch + 1, running_loss, accuracy))\n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name, param.data)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406db73",
   "metadata": {
    "papermill": {
     "duration": 0.110838,
     "end_time": "2023-04-30T12:11:00.992936",
     "exception": false,
     "start_time": "2023-04-30T12:11:00.882098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NEW_algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 617.596705,
   "end_time": "2023-04-30T12:11:02.730905",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-30T12:00:45.134200",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
